{"title":"Expanding Geospatial Information","markdown":{"yaml":{"title":"Expanding Geospatial Information","editor_options":{"chunk_output_type":"console"},"execute":{"message":false,"warning":false}},"headingText":"Setting up the environment","containsRefs":false,"markdown":"\n\nAs we saw in the **initial exploration**, `take_current_trip` varies across zones, but we know that only providing IDs offers limited context for the models to differentiate between zones. Thus, we enrich the data with **geospatial features** derived from **OpenStreetMap (OSM)** to provide deeper insights into each zone.\n\nHere are the steps to follow:\n\n1. **Import data**:  \n\n   - Load NYC taxi zone shapefiles (from TLC, EPSG:2263) and training data.  \n\n2. **Extract OSM data**:  \n\n   - Fetch OSM data for Queens, Brooklyn, and Manhattan using key tags.\n   \n     - `amenity`\n     - `shop`\n     - `office`\n     - `tourism`\n     - `public_transport`\n     - `aeroway`\n     - `emergency`\n     - `sport`\n     - `highway`\n     - `landuse`\n     \n   - Reproject OSM data to EPSG:2263 to match taxi zone CRS.  \n\n3. **Spatial joins**:  \n\n   - Link OSM features to taxi zones:  \n   \n     - Points: Include if within zone.  \n     - Lines: Include if features are fully contained in zones and for partially intersecting lines, clip the feature to the zone boundary and retain only the overlapping portion.\n     - Polygons: Include if features are fully contained in zones and for partially intersecting polygons, clip the feature to the zone boundary and retain only the overlapping portion.\n\n4. **Extract new features**:  \n\n  - For each zone and tag:  \n   \n     - Number of OSM points per $mi^2$.  \n     - Total length ($mi$) of OSM lines per $mi^2$.  \n     - Total area ($mi^2$) of OSM polygons per $mi^2$.  \n\n5. **Validation**:  \n\n   - Integrate new features and training data\n   - Define the correlation between new predictors and `take_current_trip`\n    \n\n### Define colors to use\n\n```{r}\nBoroughColors <- c(\n  'Manhattan' = '#e41a1c',\n  'Queens' = '#377eb8',\n  'Brooklyn'= '#4daf4a'\n)\n\nBoroughSelected <- names(BoroughColors)\n\nColorHighlight <- \"lightslateblue\"\nColorGray <- \"gray80\"\n```\n\n### Define features to import\n\n```{r}\nFeaturesToGet <- c(\n   \"amenity\",\n   \"shop\",\n   \"office\",\n   \"tourism\",\n   \"public_transport\",\n   \"aeroway\",\n   \"emergency\",\n   \"sport\",\n   \"highway\",\n   \"landuse\"\n)\n```\n\n### Loading packages\n\n```{r}\n## To manage relative paths\nlibrary(here)\n\n## To transform data that fits in RAM\nlibrary(data.table)\n\n## To work with spatial data\nlibrary(sf)\n\n## To create interactive maps\nlibrary(tmap)\ntmap_mode(\"view\")\n\n## To download OpenStreetMap data\nlibrary(osmdata)\n\n## Transforming distance units\nlibrary(units)\n\n## To create general plots\nlibrary(ggplot2)\n\n## Custom functions\ndevtools::load_all()\n```\n\n:::{.callout-note title=\"Computational Performance\"}\nThe geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.\n:::\n\n## Import data\n\nDetails on how this data was obtained can be found in the **Data Collection Process**.\n\n```{r}\nZonesShapes <-\n  read_sf(here(\"raw-data/taxi_zones/taxi_zones.shp\")) |>\n  subset(borough %in% BoroughSelected) |>\n  transform(Shape_Area = st_area(geometry))\n\nTrainingSample <-\n  here(\"output/take-trip-fst\") |>\n  list.files(full.names = TRUE) |>\n  (\\(x) data.table(full_path = x,\n                   n_char = nchar(basename(x)),\n                   name = basename(x)))() |>\n  (\\(dt) dt[order(n_char, name), full_path])() |>\n  head(12L) |>\n  lapply(FUN = fst::read_fst,\n         columns = c(\"trip_id\",\"PULocationID\", \"DOLocationID\", \"take_current_trip\"),\n         as.data.table = TRUE) |>\n  rbindlist()\n```\n\n## Extract OSM data\n\n### Defining bounding box\n\nNow we can create a bounding box that encompasses the selected boroughs to efficiently query OSM data for the region we really need.\n\n```{r}\n#| echo: false\n#| output: false\n\nbb_boxFilePath <- here(\"output/cache-data/07-expanding-geospatial-data/bb_box.qs\")\n\nif(file.exists(bb_boxFilePath)) {\n  bb_box <- qs2::qs_read(bb_boxFilePath)\n}\n```\n\n```{r}\n#| eval: false\n\nbb_box <-\n  lapply(X = paste0(BoroughSelected, \", NY\"),\n         FUN = osmdata::getbb) |>\n  do.call(what = \"cbind\") |>\n  (\\(x) cbind(min = apply(x, 1, min),\n              max = apply(x, 1, max)))()\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nqs2::qs_save(bb_box, bb_boxFilePath)\n```\n\nThis plot shows the selected boroughs and the taxi zones within them, giving us a visual overview of our geographical units of analysis. The blue rectangle represents the bounding box we defined earlier, encompassing Manhattan (red), Queens (blue), and Brooklyn (green). Each of these boroughs is further divided into smaller taxi zones, which are the **primary geographical units for our analysis**. The different colors assigned to each borough (defined in the `BoroughColors` vector) help to visually distinguish them. This initial map confirms that we have correctly loaded the taxi zone shapefile and subsetted it to include only the boroughs of interest for our study.\n\n\n```{r}\nplot_box(bb_box) +\n  tm_shape(ZonesShapes) +\n  tm_polygons(fill = \"borough\",\n              fill.scale = tm_scale_categorical(values = BoroughColors),\n              fill_alpha = 0.6)\n```\n\n### Getting data from API\n\nThis section retrieves geospatial data from OpenStreetMap (OSM) for the features listed in `FeaturesToGet` and save it in RDS files in the `output/cache-data/ directory` to **speed up the development process** of this portfolio.\n\n:::{.callout-note}\nBy caching the results of computationally intensive operations, we can avoid re-running the entire workflow every time the page is updated or changes are made.\n:::\n\n\n```r\nkeep_feature_value <- function(osm_list, key) {\n  \n  shape_list =\n    c(\"osm_points\",\n      \"osm_lines\",\n      \"osm_polygons\",\n      \"osm_multilines\",\n      \"osm_multipolygons\")\n  \n  for(shape_i in shape_list){\n    \n    if(is.null(osm_list[[shape_i]])){\n      next\n    }\n    \n    # Removing cases without key value\n    osm_list[[shape_i]] =\n      subset(osm_list[[shape_i]],\n             !is.na(get(key, inherits = FALSE)),\n             select = c(\"name\", key))\n    \n    # Fixing not valid geometries\n    not_valid_cases = !sf::st_is_valid(osm_list[[shape_i]])\n    if(sum(not_valid_cases) > 0) {\n      osm_list[[shape_i]] = sf::st_make_valid(osm_list[[shape_i]])\n    }\n    \n  }\n  \n  return(osm_list)\n}\n\n\nfor(feature_i in FeaturesToGet){\n  \n  OsmData <-\n    opq(bb_box) |>\n    add_osm_feature(feature_i) |>\n    osmdata_sf() |>\n    keep_feature_value(feature_i)\n  \n  saveRDS(OsmData, \n          here(sprintf(\"output/cache-data/feature_%s.rds\", feature_i)), \n          compress = \"xz\")\n}\n```\n\n### Consolidating data\n\nThis function takes the list of OSM features, selects relevant columns, pivots the data to a longer format, and transforms it to the correct coordinate reference system.\n\n```r\ntidy_osm_data <- function(osm_sf, crs = NULL) {\n  \n  shape_list =\n    c(\"osm_points\",\n      \"osm_lines\",\n      \"osm_polygons\",\n      \"osm_multilines\",\n      \"osm_multipolygons\")\n  \n  for(shape_i in shape_list) {\n    \n    osm_sf[[shape_i]] =\n      osm_sf[[shape_i]] |>\n      dplyr::select(-osm_id) |>\n      tidyr::pivot_longer(!c(\"name\", \"geometry\"),\n                          names_to = \"feature\",\n                          values_drop_na = TRUE)\n    \n    if(!is.null(crs)){\n      osm_sf[[shape_i]] = st_transform(osm_sf[[shape_i]], crs = crs)\n    }\n    \n  }\n  \n  return(osm_sf)\n  \n}\n\nOsmDataComplete <-\n  FeaturesToGet |>\n  sapply(FUN = \\(x) here(sprintf(\"output/cache-data/feature_%s.rds\", x))) |>\n  lapply(FUN = readRDS) |>\n  do.call(what = \"c\") |>\n  tidy_osm_data(crs = st_crs(ZonesShapes)$epsg)\n```\n\n### Saving the data\n\nI opted to use the `qs2` package for saving and reading R objects throughout this project instead of the base R functions like `saveRDS` and `readRDS`. This decision was based on benchmarks I encountered while learning about the `targets` R package, which indicated that `qs2` offers **significantly faster serialization and de-serialization times**, particularly for larger datasets like the geospatial data we are working with. This speed advantage helps to reduce the overall execution time of the workflow, which is beneficial for the development and presentation of this portfolio.\n\n```r\nqs2::qs_save(OsmDataComplete, \n             here(\"output/cache-data/OsmDataComplete.qs\"))\n```\n\n\n```{r}\n#| echo: FALSE\n\nOsmDataComplete <- qs2::qs_read(here(\"output/cache-data/OsmDataComplete.qs\"))\n\nOsmDataComplete\n```\n\n## Spatial joins\n\nWe perform two main types of spatial joins to associate the OpenStreetMap features with our taxi zones: first, to identify features entirely within each zone, and second, to identify features that intersect or overlap with the zone boundaries.\n\n### Joining elements **contained** by each zone\n\nUsing the `st_within` function, this section identifies OpenStreetMap features—such as points, lines, polygons, multilinestrings, and multipolygons—that are **fully contained** within each taxi zone's boundaries. `st_within` considers multilinestrings and multipolygons to be within a zone only if all their constituent parts are inside.\n\n```r\nPointsWithinZones <-\n  st_join(OsmDataComplete$osm_points,\n          ZonesShapes,\n          join = st_within,\n          left = FALSE)\n\nLinesWithinZones <-\n  OsmDataComplete$osm_lines |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n\nMultiLinesWithinZones <-\n  OsmDataComplete$osm_multilines |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n          \nPolygonWithinZones <-\n  OsmDataComplete$osm_polygons |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n\nMultiPolygonWithinZones <-\n  st_make_valid(OsmDataComplete$osm_multipolygons) |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n```\n\n```{r}\n#| echo: false\n\nPointsWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/PointsWithinZones.qs\"))\n\nLinesWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/LinesWithinZones.qs\"))\n\nMultiLinesWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiLinesWithinZones.qs\"))\n\nPolygonWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/PolygonWithinZones.qs\"))\n\nMultiPolygonWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiPolygonWithinZones.qs\"))\n```\n\nTo ensure the accuracy of our spatial joins, we first want to visually inspect the features that were identified as being *fully contained* within a specific taxi zone. We'll start by examining zone with `LocationID` 191.\n\nBy examining this map, we can visually confirm that the **lines and polygons displayed are indeed located entirely within the boundaries** of the chosen taxi zone. This helps us to verify the correctness of our `st_within` spatial join operation before proceeding with further analysis on these fully contained features.\n\n```{r}\nZoneToCheck <- 191L\n\nZonesShapesToCheck <- subset(ZonesShapes, LocationID == ZoneToCheck)\nLinesWithinZonesToCheck  <- subset(LinesWithinZones, LocationID == ZoneToCheck)\nPolygonWithinZonesToCheck <- subset(PolygonWithinZones, LocationID == ZoneToCheck)\n\ntm_shape(ZonesShapesToCheck)+\n  tm_polygons(fill = \"grey\") +\n  tm_shape(LinesWithinZonesToCheck) +\n  tm_lines(col = \"feature\") +\n  tm_shape(PolygonWithinZonesToCheck) +\n  tm_polygons(fill = \"feature\")\n```\n\n<br>\n\nNow we can save the save the results.\n\n```r\nqs2::qs_save(PointsWithinZones, \n             here(\"output/cache-data/PointsWithinZones.qs\"))\n\nqs2::qs_save(LinesWithinZones, \n             here(\"output/cache-data/LinesWithinZones.qs\"))\n\nqs2::qs_save(MultiLinesWithinZones, \n             here(\"output/cache-data/MultiLinesWithinZones.qs\"))\n\nqs2::qs_save(PolygonWithinZones, \n             here(\"output/cache-data/PolygonWithinZones.qs\"))\n\nqs2::qs_save(MultiPolygonWithinZones, \n             here(\"output/cache-data/MultiPolygonWithinZones.qs\"))\n```\n\n\n\n### Joining lines that are **partially contained** for each zone. \n\nIn this section, we identify OSM features that are not entirely within a taxi zone but do intersect or overlap with its boundary. For lines and multilinestrings, we use `st_crosses`, which identifies features that intersect the interior of a zone but are not completely contained within it. For polygons and multipolygons, we use `st_overlaps`, which identifies features that have some area in common with a zone but are not identical to or completely contained within it.\n\n```r\nLinesInSeveralZones <-\n  OsmDataComplete$osm_lines |>\n  st_join(y = ZonesShapes,\n          join = st_crosses,\n          left = FALSE)\n\nMultiLinesInSeveralZones <-\n  OsmDataComplete$osm_multilines |>\n  st_join(y = ZonesShapes,\n          join = st_crosses,\n          left = FALSE)\n\nPolygonInSeveralZones <-\n  OsmDataComplete$osm_polygons |>\n  st_join(y = ZonesShapes,\n          join = st_overlaps,\n          left = FALSE)\n\nMultiPolygonInSeveralZones <-\n  st_make_valid(OsmDataComplete$osm_multipolygons) |>\n  st_join(y = ZonesShapes,\n          join = st_overlaps,\n          left = FALSE)\n```\n\n```{r}\n#| echo: false\n\nLinesInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/LinesInSeveralZones.qs\"))\n\nMultiLinesInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiLinesInSeveralZones.qs\"))\n\nPolygonInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/PolygonInSeveralZones.qs\"))\n\nMultiPolygonInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiPolygonInSeveralZones.qs\"))\n```\n\nBy visualizing these features, we can confirm that our `st_crosses` and `st_overlaps` operations **correctly identified the lines and polygons that only partially interact** with the selected taxi zone. This is a crucial step before we proceed with the cropping process in the subsequent section, ensuring we only apply the cropping to the features that actually extend beyond the zone boundaries.\n\n```{r}\nLinesInSeveralZonesToCheck  <- subset(LinesInSeveralZones, LocationID == ZoneToCheck)\nPolygonInSeveralZonesToCheck <- subset(PolygonInSeveralZones, LocationID == ZoneToCheck)\n\ntm_shape(ZonesShapesToCheck)+\n  tm_polygons(fill = \"grey\") +\n  tm_shape(LinesInSeveralZonesToCheck) +\n  tm_lines(col = \"blue\") +\n  tm_shape(PolygonInSeveralZonesToCheck) +\n  tm_polygons(fill = \"feature\")\n```\n\n<br>\n\nLet's save the data for future use.\n\n```r\nqs2::qs_save(LinesInSeveralZones, \n             here(\"output/cache-data/LinesInSeveralZones.qs\"))\n\nqs2::qs_save(MultiLinesInSeveralZones, \n             here(\"output/cache-data/MultiLinesInSeveralZones.qs\"))\n             \nqs2::qs_save(PolygonInSeveralZones, \n             here(\"output/cache-data/PolygonInSeveralZones.qs\"))\n\nqs2::qs_save(MultiPolygonInSeveralZones, \n             here(\"output/cache-data/MultiPolygonInSeveralZones.qs\"))\n```\n\n#### Cropping features overlapping multiple zones\n\nThis section focuses on refining the geospatial data of lines and polygons that were identified as overlapping multiple taxi zones. The goal is to accurately attribute these features to the specific zone they intersect with.\n\nThe `apply_intersection_by_id` function iterates through each unique taxi zone ID. For each zone, it subsets the lines and polygons that overlap with it and then performs a spatial intersection. This operation effectively \"crops\" the line or polygon to the exact boundaries of the taxi zone.\n\nThis step is crucial for accurately calculating metrics like the length of roads or the area of land use within each zone, as it ensures that only the portion of a feature that lies within a specific zone is considered for that zone.\n\n\n```r\napply_intersection_by_id <- function(features,\n                                     shapes){\n  \n  intersection_by_id = function(id, features, shapes) {\n    \n    features_sub = subset(features, LocationID == id)\n    shapes_sub_geo = subset(shapes, LocationID == id) |> st_geometry()\n    \n    new_df = st_intersection(features_sub, shapes_sub_geo)\n    \n    return(new_df)\n  }\n  \n  new_df =\n    lapply(unique(features$LocationID),\n           FUN = intersection_by_id,\n           features = features,\n           shapes = shapes) |>\n    do.call(what = \"rbind\")\n  \n  return(new_df)\n  \n}\n\nLinesInSeveralZonesCropped <- \n  apply_intersection_by_id(LinesInSeveralZones, ZonesShapes)\n  \nMultiLinesInSeveralZonesCropped <-\n  apply_intersection_by_id(MultiLinesInSeveralZones, ZonesShapes)\n  \nPolygonInSeveralZonesCropped <-\n  apply_intersection_by_id(PolygonInSeveralZones, ZonesShapes)\n  \nMultiPolygonInSeveralZonesCropped <-\n  apply_intersection_by_id(MultiPolygonInSeveralZones, ZonesShapes)\n```\n\n```{r}\n#| echo: false\n\nLinesInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/LinesInSeveralZonesCropped.qs\"))\n\nMultiLinesInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/MultiLinesInSeveralZonesCropped.qs\"))\n\nPolygonInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/PolygonInSeveralZonesCropped.qs\"))\n\nMultiPolygonInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/MultiPolygonInSeveralZonesCropped.qs\"))\n```\n\n\nThe resulting map shows the blue lines and colored polygons neatly contained within the grey boundary of zone `r ZoneToCheck`. This ensures that when we calculate features like road length or land use area for zone `r ZoneToCheck`, we only consider the portions of these features that actually lie within its boundaries, leading to more accurate analysis.\n\n```{r}\nLinesInSeveralZonesCroppedToCheck  <- \n  subset(LinesInSeveralZonesCropped, LocationID == ZoneToCheck)\n\nPolygonInSeveralZonesCroppedToCheck <-\n  subset(PolygonInSeveralZonesCropped, LocationID == ZoneToCheck)\n\n\ntm_shape(ZonesShapesToCheck)+\n  tm_polygons(fill = \"grey\") +\n  tm_shape(LinesInSeveralZonesCroppedToCheck) +\n  tm_lines(col = \"blue\") +\n  tm_shape(PolygonInSeveralZonesCroppedToCheck) +\n  tm_polygons(fill = \"feature\")\n```\n\n<br>\n\nAfter performing those operations, we can save the data for future use.\n\n```r\nqs2::qs_save(LinesInSeveralZonesCropped, \n             here(\"output/cache-data/LinesInSeveralZonesCropped.qs\"))\n\nqs2::qs_save(MultiLinesInSeveralZonesCropped, \n             here(\"output/cache-data/MultiLinesInSeveralZonesCropped.qs\"))\n\nqs2::qs_save(PolygonInSeveralZonesCropped, \n             here(\"output/cache-data/PolygonInSeveralZonesCropped.qs\"))\n\nqs2::qs_save(MultiPolygonInSeveralZonesCropped, \n             here(\"output/cache-data/MultiPolygonInSeveralZonesCropped.qs\"))\n```\n\n\n### Joining objects based on geometry types\n\nAfter making those changes, we can consolidate all the data to use into 3 **geometry types**:\n\n- `POINT`\n- `MULTILINESTRING`\n- `MULTIPOLYGON`\n\n```{r}\n#| echo: false\n\nAllPointsZones <-\n  qs2::qs_read(here(\"output/cache-data/AllPointsZones.qs\"))\n\nAllLinesZones <-\n  qs2::qs_read(here(\"output/cache-data/AllLinesZones.qs\"))\n\nAllPolygonZones <-\n  qs2::qs_read(here(\"output/cache-data/AllPolygonZones.qs\"))\n```\n\n\n```r\nAllPointsZones <- as.data.table(PointsWithinZones)\n\nAllLinesZones <-\n  list(LinesWithinZones,\n       MultiLinesInSeveralZones,\n       MultiLinesWithinZones,\n       MultiLinesInSeveralZonesCropped) |>\n  lapply(FUN = \\(df) as.data.table(st_cast(df, to = \"MULTILINESTRING\"))) |>\n  rbindlist(use.names = TRUE)\n\nAllPolygonZones <-\n  list(PolygonWithinZones,\n       PolygonInSeveralZonesCropped,\n       MultiPolygonWithinZones,\n       MultiPolygonInSeveralZonesCropped) |>\n  lapply(FUN = \\(df) as.data.table(st_cast(df, to = \"MULTIPOLYGON\"))) |>\n  rbindlist(use.names = TRUE)\n```\n\n```{r}\n#| eval: false\n#| echo: false\n\nqs2::qs_save(AllPointsZones, \n             here(\"output/cache-data/AllPointsZones.qs\"))\n\nqs2::qs_save(AllLinesZones, \n             here(\"output/cache-data/AllLinesZones.qs\"))\n\nqs2::qs_save(AllPolygonZones, \n             here(\"output/cache-data/AllPolygonZones.qs\"))\n```\n\n## Extract new features\n\n```{r}\n#| echo: false\n\nPointDensityFeatures <-\n  qs2::qs_read(here(\"output/cache-data/PointDensityFeatures.qs\"))\n\nLineDensityFeatures <-\n  qs2::qs_read(here(\"output/cache-data/LineDensityFeatures.qs\"))\n\nPolygonDensityFeatures <-\n  qs2::qs_read(here(\"output/cache-data/PolygonDensityFeatures.qs\"))\n```\n\n\n- **Number of OSM points per $mi^2$**\n\n```{r}\n#| eval: false\n\nPointDensityFeatures <-\n  AllPointsZones[, .N, \n                 .(LocationID = OBJECTID,\n                   Shape_Area = set_units(Shape_Area, mi^2),\n                   variable = paste0(toupper(feature),\"-\", value, \"_per_mi2\"))\n  ][, n_per_mi2 := N / as.double(Shape_Area)\n  ][, dcast(.SD, LocationID ~ variable, value.var = \"n_per_mi2\", fill = 0L)]\n```\n\n\n- **Total length ($mi$) of OSM lines per $mi^2$**\n\n```{r}\n#| eval: false\n\nLineDensityFeatures <-\n  AllLinesZones[, .(total_length = sum(st_length(geometry)) |> set_units(value = mi)) , \n                 .(LocationID = OBJECTID,\n                   Shape_Area = set_units(Shape_Area, mi^2),\n                   variable = paste0(toupper(feature),\"-\", value, \"_per_mi\"))\n  ][, distance_per_mi2 := as.double(total_length) / as.double(Shape_Area)\n  ][, dcast(.SD, LocationID ~ variable, value.var = \"distance_per_mi2\", fill = 0L)]\n```\n\n- **Total area ($mi^2$) of OSM polygons per $mi^2$**\n\n```{r}\n#| eval: false\n\nPolygonDensityFeatures <-\n  AllPolygonZones[, .(total_area = sum(st_area(geometry)) |> set_units(value = mi^2)) , \n                 .(LocationID = OBJECTID,\n                   Shape_Area = set_units(Shape_Area, mi^2),\n                   variable = paste0(toupper(feature),\"-\", value))\n  ][, area_prop := as.double(total_area) / as.double(Shape_Area)\n  ][, dcast(.SD, LocationID ~ variable, value.var = \"area_prop\", fill = 0L)]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n\nqs2::qs_save(PointDensityFeatures, \n             here(\"output/cache-data/PointDensityFeatures.qs\"))\n\nqs2::qs_save(LineDensityFeatures, \n             here(\"output/cache-data/LineDensityFeatures.qs\"))\n\nqs2::qs_save(PolygonDensityFeatures, \n             here(\"output/cache-data/PolygonDensityFeatures.qs\"))\n```\n\n\n## Validation\n\nNow we just need to consolidate and save all features in a single table and join to the training data.\n\n```{r}\nOmsDensityFeatures <-\n  list(PointDensityFeatures,\n       LineDensityFeatures,\n       PolygonDensityFeatures) |>\n  Reduce(f = \\(x,y) x[y, on = \"LocationID\"])\n\nPuOmsDensityFeatures <-\n  setnames(copy(OmsDensityFeatures), \n           c(\"PULocationID\", paste0(\"PU_\", names(OmsDensityFeatures)[-1L])))\n\nDoOmsDensityFeatures <-\n  setnames(copy(OmsDensityFeatures), \n           c(\"DOLocationID\", paste0(\"DO_\", names(OmsDensityFeatures)[-1L])))\n\nTrainingSampleOmsDensityFeatures <-\n  PuOmsDensityFeatures[TrainingSample, on = \"PULocationID\"\n  ][, DoOmsDensityFeatures[.SD, on = \"DOLocationID\"]]\n\ndim(TrainingSampleOmsDensityFeatures) |> scales::comma()\n```\n\n```{r}\n#| echo: false\n#| output: false\n\nrm(list = grep(\"OmsDensityFeatures|TrainingSample|Color\", ls(), invert = TRUE, value = TRUE))\ngc()\n```\n\nLet's explore the correlated features.\n\n```{r}\n#| echo: false\n\nCorMatrix = qs2::qs_read(here(\"output/cache-data/CorMatrix.qs\"))\n```\n\n\n```{r}\n#| eval: false\n\nCorMatrix <- cor(TrainingSampleOmsDensityFeatures[, !c(\"trip_id\")])\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(CorMatrix, \n             here(\"output/cache-data/CorMatrix.qs\"))\n```\n\nAs we can see, the variables **don't show a strong correlation** (<11% for the top cases) with the variable to predict, so we will need to go deeper on feature engineering before using these variables for modeling.\n\n```{r}\n#| echo: false\n\nTopOsmCorrelations = qs2::qs_read(here(\"output/cache-data/TopOsmCorrelations.qs\"))\n```\n\n```r\nTopOsmCorrelations <-\n  as.data.table(CorMatrix, \n                keep.rownames = \"variable\"\n  )[order(-abs(take_current_trip)), \n    .(variable, \n      abs_cor = abs(take_current_trip),\n      greater_than_0 = take_current_trip > 0)\n  ][2:16\n  ][,variable := reorder(variable, abs_cor, sum)]\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(TopOsmCorrelations, \n             here(\"output/cache-data/TopOsmCorrelations.qs\"))\n```\n\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nggplot(TopOsmCorrelations)+\n  geom_col(aes(abs_cor, variable, fill = greater_than_0),\n           color = \"black\",\n           linewidth = 0.4)+\n  scale_fill_manual(values = c(\"TRUE\" = ColorHighlight,\n                               \"FALSE\" = ColorGray))+\n  scale_x_continuous(breaks = scales::breaks_width(0.01),\n                     labels = scales::label_percent(accuracy = 1))+\n  labs(y = \"\",\n       x = \"Absolute Correlation\",\n       fill = \"Positive Correlation\") +\n  theme_minimal()+\n  theme(legend.position = \"top\")\n```\n\nOn the other hand, we can also explore in the target variable is correlated with a different median of the new features. As all the variable present different dimensions we are going to scale the variables before calculating the mean and then taking the difference of means.\n\n```{r}\n#| echo: false\n#| output: false\n\nrm(list = setdiff(ls(), c(\"OmsDensityFeatures\",\"TrainingSampleOmsDensityFeatures\", \"ColorHighlight\", \"ColorGray\")))\ngc()\n\nOsmMedianDiff = qs2::qs_read(here(\"output/cache-data/OsmMedianDiff.qs\"))\n```\n\n```r\nOsmMedianDiff <-\n  TrainingSampleOmsDensityFeatures[, lapply(.SD, \\(x) if(uniqueN(x) == 1L) x[1L] else median(scale(x, center = FALSE))),\n                                   .SDcols = !c(\"trip_id\", \"PULocationID\", \"DOLocationID\"),\n                                   by = \"take_current_trip\"\n  ][, melt(.SD,\n           id.vars = \"take_current_trip\")\n  ][, .(not_take_vs_take = diff(value)),\n    by = .(variable = as.character(variable))\n  ][order(-abs(not_take_vs_take))]\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(OsmMedianDiff, \n             here(\"output/cache-data/OsmMedianDiff.qs\"))\n```\n\nIn the below chart we can see that for majority of the variables the difference of medians is 0, probably because the feature are **near zero variance** as doesn't show very often in many zones, but on the other hand we can see that some features present some $|\\text{med}(x)| > 0.1$ that would be important to see in more detail.\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nggplot(OsmMedianDiff) +\n  geom_histogram(aes(not_take_vs_take)) +\n  scale_y_continuous(transform = scales::new_transform(\"signed_log10\",\n                                                       transform = function(x) sign(x) * log10(1 + abs(x)),\n                                                       inverse = function(x) sign(x) * (10^(abs(x)) - 1),\n                                                       breaks = scales::pretty_breaks()),\n                     breaks = c(0, 10^(1:4))) +\n  labs(y = \"count on log10 scale\")+\n  theme_minimal()\n```\n\n```{r}\n#| echo: false\n#| output: false\n\nrm(list = setdiff(ls(), c(\"OmsDensityFeatures\",\"OsmMedianDiff\", \"TrainingSampleOmsDensityFeatures\", \"ColorHighlight\", \"ColorGray\")))\ngc()\n\nTopOsmMedianDiff = qs2::qs_read(here(\"output/cache-data/TopOsmMedianDiff.qs\"))\nTopOsmMedianDiffFeatures = OsmMedianDiff[abs(not_take_vs_take) > 0.1, variable]\n```\n\nAfter selecting the to variables to explore, we just need to see\n\n```r\nTopOsmMedianDiffFeatures <- OsmMedianDiff[abs(not_take_vs_take) > 0.1, variable]\n\nTopOsmMedianDiff <-\n  TrainingSampleOmsDensityFeatures[, melt(.SD,\n                                          id.vars = c(\"trip_id\", \"take_current_trip\")),\n                                   .SDcols = c(\"trip_id\",\n                                               \"take_current_trip\",\n                                               TopOsmMedianDiffFeatures)\n  ][, `:=`(variable = factor(variable, levels = rev(TopOsmMedianDiffFeatures)),\n           take_current_trip = take_current_trip == 1L)\n  ][, value := scale(value, center = FALSE),\n    by = \"variable\"]\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(TopOsmMedianDiff, \n             here(\"output/cache-data/TopOsmMedianDiff.qs\"))\n```\n\nBased on the next chart, we can see that the median change based on different factors like:\n\n| Variable                            | Influence of `take_current_trip` (TRUE vs FALSE) on Median        |\n|-------------------------------------|-------------------------------------------------------------------|\n| PU_SPORT-american_handball          | Median **higher** when `take_current_trip` is `TRUE`. |\n| DO_AMENITY-bicycle_rental_per_mi2   | Median  **lower**  when `take_current_trip` is `TRUE`.|\n| PU_AMENITY-school                   | Median **higher**  when `take_current_trip` is `TRUE`. |\n| DO_AMENITY-bicycle_parking_per_mi2  | Median **lower** when `take_current_trip` is `TRUE`.|\n| PU_AMENITY-restaurant_per_mi2       | Median **lower** when `take_current_trip` is `TRUE`.|\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| fig-height: 7.5\n\n\nggplot(TopOsmMedianDiff) +\n  geom_boxplot(aes(x = value,\n                   y  = variable,\n                   fill = take_current_trip)) +\n  scale_fill_manual(values = c(\"TRUE\" = ColorHighlight,\n                               \"FALSE\" = ColorGray))+\n  scale_x_continuous(transform = scales::new_transform(\"signed_log10\",\n                                                       transform = function(x) sign(x) * log10(1 + abs(x)),\n                                                       inverse = function(x) sign(x) * (10^(abs(x)) - 1),\n                                                       breaks = scales::pretty_breaks()),\n                     labels = scales::label_comma(accuracy = 0.1),\n                     breaks = c(0, 10^seq(-1,4, by = 0.25))) +\n  labs(x = \"\",\n       y = \"\") +\n  theme_minimal()\n```\n\nAfter confirming these promising results we can save the most import features for future use.\n\n```r\nFeaturesToKeep <- \n  c(\"LocationID\", sub(\"(PU|DO)_\",\"\", TopOsmMedianDiffFeatures)) |>\n  unique()\n\nfst::write_fst(OmsDensityFeatures[, ..FeaturesToKeep], \n               path = here(\"output/OmsDensityFeatures.fst\"))\n```\n\n\n## Conclusion\n\nWe know that these features will be useful for modeling, especially for finding correlations with weekdays and hours.\n\nIt's important to take in consideration that the data is **right skewed** and present some **high-leverage points** that will need to be solved depending on the model to select.\n","srcMarkdownNoYaml":"\n\nAs we saw in the **initial exploration**, `take_current_trip` varies across zones, but we know that only providing IDs offers limited context for the models to differentiate between zones. Thus, we enrich the data with **geospatial features** derived from **OpenStreetMap (OSM)** to provide deeper insights into each zone.\n\nHere are the steps to follow:\n\n1. **Import data**:  \n\n   - Load NYC taxi zone shapefiles (from TLC, EPSG:2263) and training data.  \n\n2. **Extract OSM data**:  \n\n   - Fetch OSM data for Queens, Brooklyn, and Manhattan using key tags.\n   \n     - `amenity`\n     - `shop`\n     - `office`\n     - `tourism`\n     - `public_transport`\n     - `aeroway`\n     - `emergency`\n     - `sport`\n     - `highway`\n     - `landuse`\n     \n   - Reproject OSM data to EPSG:2263 to match taxi zone CRS.  \n\n3. **Spatial joins**:  \n\n   - Link OSM features to taxi zones:  \n   \n     - Points: Include if within zone.  \n     - Lines: Include if features are fully contained in zones and for partially intersecting lines, clip the feature to the zone boundary and retain only the overlapping portion.\n     - Polygons: Include if features are fully contained in zones and for partially intersecting polygons, clip the feature to the zone boundary and retain only the overlapping portion.\n\n4. **Extract new features**:  \n\n  - For each zone and tag:  \n   \n     - Number of OSM points per $mi^2$.  \n     - Total length ($mi$) of OSM lines per $mi^2$.  \n     - Total area ($mi^2$) of OSM polygons per $mi^2$.  \n\n5. **Validation**:  \n\n   - Integrate new features and training data\n   - Define the correlation between new predictors and `take_current_trip`\n    \n## Setting up the environment\n\n### Define colors to use\n\n```{r}\nBoroughColors <- c(\n  'Manhattan' = '#e41a1c',\n  'Queens' = '#377eb8',\n  'Brooklyn'= '#4daf4a'\n)\n\nBoroughSelected <- names(BoroughColors)\n\nColorHighlight <- \"lightslateblue\"\nColorGray <- \"gray80\"\n```\n\n### Define features to import\n\n```{r}\nFeaturesToGet <- c(\n   \"amenity\",\n   \"shop\",\n   \"office\",\n   \"tourism\",\n   \"public_transport\",\n   \"aeroway\",\n   \"emergency\",\n   \"sport\",\n   \"highway\",\n   \"landuse\"\n)\n```\n\n### Loading packages\n\n```{r}\n## To manage relative paths\nlibrary(here)\n\n## To transform data that fits in RAM\nlibrary(data.table)\n\n## To work with spatial data\nlibrary(sf)\n\n## To create interactive maps\nlibrary(tmap)\ntmap_mode(\"view\")\n\n## To download OpenStreetMap data\nlibrary(osmdata)\n\n## Transforming distance units\nlibrary(units)\n\n## To create general plots\nlibrary(ggplot2)\n\n## Custom functions\ndevtools::load_all()\n```\n\n:::{.callout-note title=\"Computational Performance\"}\nThe geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.\n:::\n\n## Import data\n\nDetails on how this data was obtained can be found in the **Data Collection Process**.\n\n```{r}\nZonesShapes <-\n  read_sf(here(\"raw-data/taxi_zones/taxi_zones.shp\")) |>\n  subset(borough %in% BoroughSelected) |>\n  transform(Shape_Area = st_area(geometry))\n\nTrainingSample <-\n  here(\"output/take-trip-fst\") |>\n  list.files(full.names = TRUE) |>\n  (\\(x) data.table(full_path = x,\n                   n_char = nchar(basename(x)),\n                   name = basename(x)))() |>\n  (\\(dt) dt[order(n_char, name), full_path])() |>\n  head(12L) |>\n  lapply(FUN = fst::read_fst,\n         columns = c(\"trip_id\",\"PULocationID\", \"DOLocationID\", \"take_current_trip\"),\n         as.data.table = TRUE) |>\n  rbindlist()\n```\n\n## Extract OSM data\n\n### Defining bounding box\n\nNow we can create a bounding box that encompasses the selected boroughs to efficiently query OSM data for the region we really need.\n\n```{r}\n#| echo: false\n#| output: false\n\nbb_boxFilePath <- here(\"output/cache-data/07-expanding-geospatial-data/bb_box.qs\")\n\nif(file.exists(bb_boxFilePath)) {\n  bb_box <- qs2::qs_read(bb_boxFilePath)\n}\n```\n\n```{r}\n#| eval: false\n\nbb_box <-\n  lapply(X = paste0(BoroughSelected, \", NY\"),\n         FUN = osmdata::getbb) |>\n  do.call(what = \"cbind\") |>\n  (\\(x) cbind(min = apply(x, 1, min),\n              max = apply(x, 1, max)))()\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nqs2::qs_save(bb_box, bb_boxFilePath)\n```\n\nThis plot shows the selected boroughs and the taxi zones within them, giving us a visual overview of our geographical units of analysis. The blue rectangle represents the bounding box we defined earlier, encompassing Manhattan (red), Queens (blue), and Brooklyn (green). Each of these boroughs is further divided into smaller taxi zones, which are the **primary geographical units for our analysis**. The different colors assigned to each borough (defined in the `BoroughColors` vector) help to visually distinguish them. This initial map confirms that we have correctly loaded the taxi zone shapefile and subsetted it to include only the boroughs of interest for our study.\n\n\n```{r}\nplot_box(bb_box) +\n  tm_shape(ZonesShapes) +\n  tm_polygons(fill = \"borough\",\n              fill.scale = tm_scale_categorical(values = BoroughColors),\n              fill_alpha = 0.6)\n```\n\n### Getting data from API\n\nThis section retrieves geospatial data from OpenStreetMap (OSM) for the features listed in `FeaturesToGet` and save it in RDS files in the `output/cache-data/ directory` to **speed up the development process** of this portfolio.\n\n:::{.callout-note}\nBy caching the results of computationally intensive operations, we can avoid re-running the entire workflow every time the page is updated or changes are made.\n:::\n\n\n```r\nkeep_feature_value <- function(osm_list, key) {\n  \n  shape_list =\n    c(\"osm_points\",\n      \"osm_lines\",\n      \"osm_polygons\",\n      \"osm_multilines\",\n      \"osm_multipolygons\")\n  \n  for(shape_i in shape_list){\n    \n    if(is.null(osm_list[[shape_i]])){\n      next\n    }\n    \n    # Removing cases without key value\n    osm_list[[shape_i]] =\n      subset(osm_list[[shape_i]],\n             !is.na(get(key, inherits = FALSE)),\n             select = c(\"name\", key))\n    \n    # Fixing not valid geometries\n    not_valid_cases = !sf::st_is_valid(osm_list[[shape_i]])\n    if(sum(not_valid_cases) > 0) {\n      osm_list[[shape_i]] = sf::st_make_valid(osm_list[[shape_i]])\n    }\n    \n  }\n  \n  return(osm_list)\n}\n\n\nfor(feature_i in FeaturesToGet){\n  \n  OsmData <-\n    opq(bb_box) |>\n    add_osm_feature(feature_i) |>\n    osmdata_sf() |>\n    keep_feature_value(feature_i)\n  \n  saveRDS(OsmData, \n          here(sprintf(\"output/cache-data/feature_%s.rds\", feature_i)), \n          compress = \"xz\")\n}\n```\n\n### Consolidating data\n\nThis function takes the list of OSM features, selects relevant columns, pivots the data to a longer format, and transforms it to the correct coordinate reference system.\n\n```r\ntidy_osm_data <- function(osm_sf, crs = NULL) {\n  \n  shape_list =\n    c(\"osm_points\",\n      \"osm_lines\",\n      \"osm_polygons\",\n      \"osm_multilines\",\n      \"osm_multipolygons\")\n  \n  for(shape_i in shape_list) {\n    \n    osm_sf[[shape_i]] =\n      osm_sf[[shape_i]] |>\n      dplyr::select(-osm_id) |>\n      tidyr::pivot_longer(!c(\"name\", \"geometry\"),\n                          names_to = \"feature\",\n                          values_drop_na = TRUE)\n    \n    if(!is.null(crs)){\n      osm_sf[[shape_i]] = st_transform(osm_sf[[shape_i]], crs = crs)\n    }\n    \n  }\n  \n  return(osm_sf)\n  \n}\n\nOsmDataComplete <-\n  FeaturesToGet |>\n  sapply(FUN = \\(x) here(sprintf(\"output/cache-data/feature_%s.rds\", x))) |>\n  lapply(FUN = readRDS) |>\n  do.call(what = \"c\") |>\n  tidy_osm_data(crs = st_crs(ZonesShapes)$epsg)\n```\n\n### Saving the data\n\nI opted to use the `qs2` package for saving and reading R objects throughout this project instead of the base R functions like `saveRDS` and `readRDS`. This decision was based on benchmarks I encountered while learning about the `targets` R package, which indicated that `qs2` offers **significantly faster serialization and de-serialization times**, particularly for larger datasets like the geospatial data we are working with. This speed advantage helps to reduce the overall execution time of the workflow, which is beneficial for the development and presentation of this portfolio.\n\n```r\nqs2::qs_save(OsmDataComplete, \n             here(\"output/cache-data/OsmDataComplete.qs\"))\n```\n\n\n```{r}\n#| echo: FALSE\n\nOsmDataComplete <- qs2::qs_read(here(\"output/cache-data/OsmDataComplete.qs\"))\n\nOsmDataComplete\n```\n\n## Spatial joins\n\nWe perform two main types of spatial joins to associate the OpenStreetMap features with our taxi zones: first, to identify features entirely within each zone, and second, to identify features that intersect or overlap with the zone boundaries.\n\n### Joining elements **contained** by each zone\n\nUsing the `st_within` function, this section identifies OpenStreetMap features—such as points, lines, polygons, multilinestrings, and multipolygons—that are **fully contained** within each taxi zone's boundaries. `st_within` considers multilinestrings and multipolygons to be within a zone only if all their constituent parts are inside.\n\n```r\nPointsWithinZones <-\n  st_join(OsmDataComplete$osm_points,\n          ZonesShapes,\n          join = st_within,\n          left = FALSE)\n\nLinesWithinZones <-\n  OsmDataComplete$osm_lines |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n\nMultiLinesWithinZones <-\n  OsmDataComplete$osm_multilines |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n          \nPolygonWithinZones <-\n  OsmDataComplete$osm_polygons |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n\nMultiPolygonWithinZones <-\n  st_make_valid(OsmDataComplete$osm_multipolygons) |>\n  st_join(y = ZonesShapes,\n          join = st_within,\n          left = FALSE)\n```\n\n```{r}\n#| echo: false\n\nPointsWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/PointsWithinZones.qs\"))\n\nLinesWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/LinesWithinZones.qs\"))\n\nMultiLinesWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiLinesWithinZones.qs\"))\n\nPolygonWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/PolygonWithinZones.qs\"))\n\nMultiPolygonWithinZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiPolygonWithinZones.qs\"))\n```\n\nTo ensure the accuracy of our spatial joins, we first want to visually inspect the features that were identified as being *fully contained* within a specific taxi zone. We'll start by examining zone with `LocationID` 191.\n\nBy examining this map, we can visually confirm that the **lines and polygons displayed are indeed located entirely within the boundaries** of the chosen taxi zone. This helps us to verify the correctness of our `st_within` spatial join operation before proceeding with further analysis on these fully contained features.\n\n```{r}\nZoneToCheck <- 191L\n\nZonesShapesToCheck <- subset(ZonesShapes, LocationID == ZoneToCheck)\nLinesWithinZonesToCheck  <- subset(LinesWithinZones, LocationID == ZoneToCheck)\nPolygonWithinZonesToCheck <- subset(PolygonWithinZones, LocationID == ZoneToCheck)\n\ntm_shape(ZonesShapesToCheck)+\n  tm_polygons(fill = \"grey\") +\n  tm_shape(LinesWithinZonesToCheck) +\n  tm_lines(col = \"feature\") +\n  tm_shape(PolygonWithinZonesToCheck) +\n  tm_polygons(fill = \"feature\")\n```\n\n<br>\n\nNow we can save the save the results.\n\n```r\nqs2::qs_save(PointsWithinZones, \n             here(\"output/cache-data/PointsWithinZones.qs\"))\n\nqs2::qs_save(LinesWithinZones, \n             here(\"output/cache-data/LinesWithinZones.qs\"))\n\nqs2::qs_save(MultiLinesWithinZones, \n             here(\"output/cache-data/MultiLinesWithinZones.qs\"))\n\nqs2::qs_save(PolygonWithinZones, \n             here(\"output/cache-data/PolygonWithinZones.qs\"))\n\nqs2::qs_save(MultiPolygonWithinZones, \n             here(\"output/cache-data/MultiPolygonWithinZones.qs\"))\n```\n\n\n\n### Joining lines that are **partially contained** for each zone. \n\nIn this section, we identify OSM features that are not entirely within a taxi zone but do intersect or overlap with its boundary. For lines and multilinestrings, we use `st_crosses`, which identifies features that intersect the interior of a zone but are not completely contained within it. For polygons and multipolygons, we use `st_overlaps`, which identifies features that have some area in common with a zone but are not identical to or completely contained within it.\n\n```r\nLinesInSeveralZones <-\n  OsmDataComplete$osm_lines |>\n  st_join(y = ZonesShapes,\n          join = st_crosses,\n          left = FALSE)\n\nMultiLinesInSeveralZones <-\n  OsmDataComplete$osm_multilines |>\n  st_join(y = ZonesShapes,\n          join = st_crosses,\n          left = FALSE)\n\nPolygonInSeveralZones <-\n  OsmDataComplete$osm_polygons |>\n  st_join(y = ZonesShapes,\n          join = st_overlaps,\n          left = FALSE)\n\nMultiPolygonInSeveralZones <-\n  st_make_valid(OsmDataComplete$osm_multipolygons) |>\n  st_join(y = ZonesShapes,\n          join = st_overlaps,\n          left = FALSE)\n```\n\n```{r}\n#| echo: false\n\nLinesInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/LinesInSeveralZones.qs\"))\n\nMultiLinesInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiLinesInSeveralZones.qs\"))\n\nPolygonInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/PolygonInSeveralZones.qs\"))\n\nMultiPolygonInSeveralZones <-\n  qs2::qs_read(here(\"output/cache-data/MultiPolygonInSeveralZones.qs\"))\n```\n\nBy visualizing these features, we can confirm that our `st_crosses` and `st_overlaps` operations **correctly identified the lines and polygons that only partially interact** with the selected taxi zone. This is a crucial step before we proceed with the cropping process in the subsequent section, ensuring we only apply the cropping to the features that actually extend beyond the zone boundaries.\n\n```{r}\nLinesInSeveralZonesToCheck  <- subset(LinesInSeveralZones, LocationID == ZoneToCheck)\nPolygonInSeveralZonesToCheck <- subset(PolygonInSeveralZones, LocationID == ZoneToCheck)\n\ntm_shape(ZonesShapesToCheck)+\n  tm_polygons(fill = \"grey\") +\n  tm_shape(LinesInSeveralZonesToCheck) +\n  tm_lines(col = \"blue\") +\n  tm_shape(PolygonInSeveralZonesToCheck) +\n  tm_polygons(fill = \"feature\")\n```\n\n<br>\n\nLet's save the data for future use.\n\n```r\nqs2::qs_save(LinesInSeveralZones, \n             here(\"output/cache-data/LinesInSeveralZones.qs\"))\n\nqs2::qs_save(MultiLinesInSeveralZones, \n             here(\"output/cache-data/MultiLinesInSeveralZones.qs\"))\n             \nqs2::qs_save(PolygonInSeveralZones, \n             here(\"output/cache-data/PolygonInSeveralZones.qs\"))\n\nqs2::qs_save(MultiPolygonInSeveralZones, \n             here(\"output/cache-data/MultiPolygonInSeveralZones.qs\"))\n```\n\n#### Cropping features overlapping multiple zones\n\nThis section focuses on refining the geospatial data of lines and polygons that were identified as overlapping multiple taxi zones. The goal is to accurately attribute these features to the specific zone they intersect with.\n\nThe `apply_intersection_by_id` function iterates through each unique taxi zone ID. For each zone, it subsets the lines and polygons that overlap with it and then performs a spatial intersection. This operation effectively \"crops\" the line or polygon to the exact boundaries of the taxi zone.\n\nThis step is crucial for accurately calculating metrics like the length of roads or the area of land use within each zone, as it ensures that only the portion of a feature that lies within a specific zone is considered for that zone.\n\n\n```r\napply_intersection_by_id <- function(features,\n                                     shapes){\n  \n  intersection_by_id = function(id, features, shapes) {\n    \n    features_sub = subset(features, LocationID == id)\n    shapes_sub_geo = subset(shapes, LocationID == id) |> st_geometry()\n    \n    new_df = st_intersection(features_sub, shapes_sub_geo)\n    \n    return(new_df)\n  }\n  \n  new_df =\n    lapply(unique(features$LocationID),\n           FUN = intersection_by_id,\n           features = features,\n           shapes = shapes) |>\n    do.call(what = \"rbind\")\n  \n  return(new_df)\n  \n}\n\nLinesInSeveralZonesCropped <- \n  apply_intersection_by_id(LinesInSeveralZones, ZonesShapes)\n  \nMultiLinesInSeveralZonesCropped <-\n  apply_intersection_by_id(MultiLinesInSeveralZones, ZonesShapes)\n  \nPolygonInSeveralZonesCropped <-\n  apply_intersection_by_id(PolygonInSeveralZones, ZonesShapes)\n  \nMultiPolygonInSeveralZonesCropped <-\n  apply_intersection_by_id(MultiPolygonInSeveralZones, ZonesShapes)\n```\n\n```{r}\n#| echo: false\n\nLinesInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/LinesInSeveralZonesCropped.qs\"))\n\nMultiLinesInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/MultiLinesInSeveralZonesCropped.qs\"))\n\nPolygonInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/PolygonInSeveralZonesCropped.qs\"))\n\nMultiPolygonInSeveralZonesCropped <-\n  qs2::qs_read(here(\"output/cache-data/MultiPolygonInSeveralZonesCropped.qs\"))\n```\n\n\nThe resulting map shows the blue lines and colored polygons neatly contained within the grey boundary of zone `r ZoneToCheck`. This ensures that when we calculate features like road length or land use area for zone `r ZoneToCheck`, we only consider the portions of these features that actually lie within its boundaries, leading to more accurate analysis.\n\n```{r}\nLinesInSeveralZonesCroppedToCheck  <- \n  subset(LinesInSeveralZonesCropped, LocationID == ZoneToCheck)\n\nPolygonInSeveralZonesCroppedToCheck <-\n  subset(PolygonInSeveralZonesCropped, LocationID == ZoneToCheck)\n\n\ntm_shape(ZonesShapesToCheck)+\n  tm_polygons(fill = \"grey\") +\n  tm_shape(LinesInSeveralZonesCroppedToCheck) +\n  tm_lines(col = \"blue\") +\n  tm_shape(PolygonInSeveralZonesCroppedToCheck) +\n  tm_polygons(fill = \"feature\")\n```\n\n<br>\n\nAfter performing those operations, we can save the data for future use.\n\n```r\nqs2::qs_save(LinesInSeveralZonesCropped, \n             here(\"output/cache-data/LinesInSeveralZonesCropped.qs\"))\n\nqs2::qs_save(MultiLinesInSeveralZonesCropped, \n             here(\"output/cache-data/MultiLinesInSeveralZonesCropped.qs\"))\n\nqs2::qs_save(PolygonInSeveralZonesCropped, \n             here(\"output/cache-data/PolygonInSeveralZonesCropped.qs\"))\n\nqs2::qs_save(MultiPolygonInSeveralZonesCropped, \n             here(\"output/cache-data/MultiPolygonInSeveralZonesCropped.qs\"))\n```\n\n\n### Joining objects based on geometry types\n\nAfter making those changes, we can consolidate all the data to use into 3 **geometry types**:\n\n- `POINT`\n- `MULTILINESTRING`\n- `MULTIPOLYGON`\n\n```{r}\n#| echo: false\n\nAllPointsZones <-\n  qs2::qs_read(here(\"output/cache-data/AllPointsZones.qs\"))\n\nAllLinesZones <-\n  qs2::qs_read(here(\"output/cache-data/AllLinesZones.qs\"))\n\nAllPolygonZones <-\n  qs2::qs_read(here(\"output/cache-data/AllPolygonZones.qs\"))\n```\n\n\n```r\nAllPointsZones <- as.data.table(PointsWithinZones)\n\nAllLinesZones <-\n  list(LinesWithinZones,\n       MultiLinesInSeveralZones,\n       MultiLinesWithinZones,\n       MultiLinesInSeveralZonesCropped) |>\n  lapply(FUN = \\(df) as.data.table(st_cast(df, to = \"MULTILINESTRING\"))) |>\n  rbindlist(use.names = TRUE)\n\nAllPolygonZones <-\n  list(PolygonWithinZones,\n       PolygonInSeveralZonesCropped,\n       MultiPolygonWithinZones,\n       MultiPolygonInSeveralZonesCropped) |>\n  lapply(FUN = \\(df) as.data.table(st_cast(df, to = \"MULTIPOLYGON\"))) |>\n  rbindlist(use.names = TRUE)\n```\n\n```{r}\n#| eval: false\n#| echo: false\n\nqs2::qs_save(AllPointsZones, \n             here(\"output/cache-data/AllPointsZones.qs\"))\n\nqs2::qs_save(AllLinesZones, \n             here(\"output/cache-data/AllLinesZones.qs\"))\n\nqs2::qs_save(AllPolygonZones, \n             here(\"output/cache-data/AllPolygonZones.qs\"))\n```\n\n## Extract new features\n\n```{r}\n#| echo: false\n\nPointDensityFeatures <-\n  qs2::qs_read(here(\"output/cache-data/PointDensityFeatures.qs\"))\n\nLineDensityFeatures <-\n  qs2::qs_read(here(\"output/cache-data/LineDensityFeatures.qs\"))\n\nPolygonDensityFeatures <-\n  qs2::qs_read(here(\"output/cache-data/PolygonDensityFeatures.qs\"))\n```\n\n\n- **Number of OSM points per $mi^2$**\n\n```{r}\n#| eval: false\n\nPointDensityFeatures <-\n  AllPointsZones[, .N, \n                 .(LocationID = OBJECTID,\n                   Shape_Area = set_units(Shape_Area, mi^2),\n                   variable = paste0(toupper(feature),\"-\", value, \"_per_mi2\"))\n  ][, n_per_mi2 := N / as.double(Shape_Area)\n  ][, dcast(.SD, LocationID ~ variable, value.var = \"n_per_mi2\", fill = 0L)]\n```\n\n\n- **Total length ($mi$) of OSM lines per $mi^2$**\n\n```{r}\n#| eval: false\n\nLineDensityFeatures <-\n  AllLinesZones[, .(total_length = sum(st_length(geometry)) |> set_units(value = mi)) , \n                 .(LocationID = OBJECTID,\n                   Shape_Area = set_units(Shape_Area, mi^2),\n                   variable = paste0(toupper(feature),\"-\", value, \"_per_mi\"))\n  ][, distance_per_mi2 := as.double(total_length) / as.double(Shape_Area)\n  ][, dcast(.SD, LocationID ~ variable, value.var = \"distance_per_mi2\", fill = 0L)]\n```\n\n- **Total area ($mi^2$) of OSM polygons per $mi^2$**\n\n```{r}\n#| eval: false\n\nPolygonDensityFeatures <-\n  AllPolygonZones[, .(total_area = sum(st_area(geometry)) |> set_units(value = mi^2)) , \n                 .(LocationID = OBJECTID,\n                   Shape_Area = set_units(Shape_Area, mi^2),\n                   variable = paste0(toupper(feature),\"-\", value))\n  ][, area_prop := as.double(total_area) / as.double(Shape_Area)\n  ][, dcast(.SD, LocationID ~ variable, value.var = \"area_prop\", fill = 0L)]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n\nqs2::qs_save(PointDensityFeatures, \n             here(\"output/cache-data/PointDensityFeatures.qs\"))\n\nqs2::qs_save(LineDensityFeatures, \n             here(\"output/cache-data/LineDensityFeatures.qs\"))\n\nqs2::qs_save(PolygonDensityFeatures, \n             here(\"output/cache-data/PolygonDensityFeatures.qs\"))\n```\n\n\n## Validation\n\nNow we just need to consolidate and save all features in a single table and join to the training data.\n\n```{r}\nOmsDensityFeatures <-\n  list(PointDensityFeatures,\n       LineDensityFeatures,\n       PolygonDensityFeatures) |>\n  Reduce(f = \\(x,y) x[y, on = \"LocationID\"])\n\nPuOmsDensityFeatures <-\n  setnames(copy(OmsDensityFeatures), \n           c(\"PULocationID\", paste0(\"PU_\", names(OmsDensityFeatures)[-1L])))\n\nDoOmsDensityFeatures <-\n  setnames(copy(OmsDensityFeatures), \n           c(\"DOLocationID\", paste0(\"DO_\", names(OmsDensityFeatures)[-1L])))\n\nTrainingSampleOmsDensityFeatures <-\n  PuOmsDensityFeatures[TrainingSample, on = \"PULocationID\"\n  ][, DoOmsDensityFeatures[.SD, on = \"DOLocationID\"]]\n\ndim(TrainingSampleOmsDensityFeatures) |> scales::comma()\n```\n\n```{r}\n#| echo: false\n#| output: false\n\nrm(list = grep(\"OmsDensityFeatures|TrainingSample|Color\", ls(), invert = TRUE, value = TRUE))\ngc()\n```\n\nLet's explore the correlated features.\n\n```{r}\n#| echo: false\n\nCorMatrix = qs2::qs_read(here(\"output/cache-data/CorMatrix.qs\"))\n```\n\n\n```{r}\n#| eval: false\n\nCorMatrix <- cor(TrainingSampleOmsDensityFeatures[, !c(\"trip_id\")])\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(CorMatrix, \n             here(\"output/cache-data/CorMatrix.qs\"))\n```\n\nAs we can see, the variables **don't show a strong correlation** (<11% for the top cases) with the variable to predict, so we will need to go deeper on feature engineering before using these variables for modeling.\n\n```{r}\n#| echo: false\n\nTopOsmCorrelations = qs2::qs_read(here(\"output/cache-data/TopOsmCorrelations.qs\"))\n```\n\n```r\nTopOsmCorrelations <-\n  as.data.table(CorMatrix, \n                keep.rownames = \"variable\"\n  )[order(-abs(take_current_trip)), \n    .(variable, \n      abs_cor = abs(take_current_trip),\n      greater_than_0 = take_current_trip > 0)\n  ][2:16\n  ][,variable := reorder(variable, abs_cor, sum)]\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(TopOsmCorrelations, \n             here(\"output/cache-data/TopOsmCorrelations.qs\"))\n```\n\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nggplot(TopOsmCorrelations)+\n  geom_col(aes(abs_cor, variable, fill = greater_than_0),\n           color = \"black\",\n           linewidth = 0.4)+\n  scale_fill_manual(values = c(\"TRUE\" = ColorHighlight,\n                               \"FALSE\" = ColorGray))+\n  scale_x_continuous(breaks = scales::breaks_width(0.01),\n                     labels = scales::label_percent(accuracy = 1))+\n  labs(y = \"\",\n       x = \"Absolute Correlation\",\n       fill = \"Positive Correlation\") +\n  theme_minimal()+\n  theme(legend.position = \"top\")\n```\n\nOn the other hand, we can also explore in the target variable is correlated with a different median of the new features. As all the variable present different dimensions we are going to scale the variables before calculating the mean and then taking the difference of means.\n\n```{r}\n#| echo: false\n#| output: false\n\nrm(list = setdiff(ls(), c(\"OmsDensityFeatures\",\"TrainingSampleOmsDensityFeatures\", \"ColorHighlight\", \"ColorGray\")))\ngc()\n\nOsmMedianDiff = qs2::qs_read(here(\"output/cache-data/OsmMedianDiff.qs\"))\n```\n\n```r\nOsmMedianDiff <-\n  TrainingSampleOmsDensityFeatures[, lapply(.SD, \\(x) if(uniqueN(x) == 1L) x[1L] else median(scale(x, center = FALSE))),\n                                   .SDcols = !c(\"trip_id\", \"PULocationID\", \"DOLocationID\"),\n                                   by = \"take_current_trip\"\n  ][, melt(.SD,\n           id.vars = \"take_current_trip\")\n  ][, .(not_take_vs_take = diff(value)),\n    by = .(variable = as.character(variable))\n  ][order(-abs(not_take_vs_take))]\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(OsmMedianDiff, \n             here(\"output/cache-data/OsmMedianDiff.qs\"))\n```\n\nIn the below chart we can see that for majority of the variables the difference of medians is 0, probably because the feature are **near zero variance** as doesn't show very often in many zones, but on the other hand we can see that some features present some $|\\text{med}(x)| > 0.1$ that would be important to see in more detail.\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nggplot(OsmMedianDiff) +\n  geom_histogram(aes(not_take_vs_take)) +\n  scale_y_continuous(transform = scales::new_transform(\"signed_log10\",\n                                                       transform = function(x) sign(x) * log10(1 + abs(x)),\n                                                       inverse = function(x) sign(x) * (10^(abs(x)) - 1),\n                                                       breaks = scales::pretty_breaks()),\n                     breaks = c(0, 10^(1:4))) +\n  labs(y = \"count on log10 scale\")+\n  theme_minimal()\n```\n\n```{r}\n#| echo: false\n#| output: false\n\nrm(list = setdiff(ls(), c(\"OmsDensityFeatures\",\"OsmMedianDiff\", \"TrainingSampleOmsDensityFeatures\", \"ColorHighlight\", \"ColorGray\")))\ngc()\n\nTopOsmMedianDiff = qs2::qs_read(here(\"output/cache-data/TopOsmMedianDiff.qs\"))\nTopOsmMedianDiffFeatures = OsmMedianDiff[abs(not_take_vs_take) > 0.1, variable]\n```\n\nAfter selecting the to variables to explore, we just need to see\n\n```r\nTopOsmMedianDiffFeatures <- OsmMedianDiff[abs(not_take_vs_take) > 0.1, variable]\n\nTopOsmMedianDiff <-\n  TrainingSampleOmsDensityFeatures[, melt(.SD,\n                                          id.vars = c(\"trip_id\", \"take_current_trip\")),\n                                   .SDcols = c(\"trip_id\",\n                                               \"take_current_trip\",\n                                               TopOsmMedianDiffFeatures)\n  ][, `:=`(variable = factor(variable, levels = rev(TopOsmMedianDiffFeatures)),\n           take_current_trip = take_current_trip == 1L)\n  ][, value := scale(value, center = FALSE),\n    by = \"variable\"]\n```\n\n```{r}\n#| eval: false  \n#| echo: false\n\nqs2::qs_save(TopOsmMedianDiff, \n             here(\"output/cache-data/TopOsmMedianDiff.qs\"))\n```\n\nBased on the next chart, we can see that the median change based on different factors like:\n\n| Variable                            | Influence of `take_current_trip` (TRUE vs FALSE) on Median        |\n|-------------------------------------|-------------------------------------------------------------------|\n| PU_SPORT-american_handball          | Median **higher** when `take_current_trip` is `TRUE`. |\n| DO_AMENITY-bicycle_rental_per_mi2   | Median  **lower**  when `take_current_trip` is `TRUE`.|\n| PU_AMENITY-school                   | Median **higher**  when `take_current_trip` is `TRUE`. |\n| DO_AMENITY-bicycle_parking_per_mi2  | Median **lower** when `take_current_trip` is `TRUE`.|\n| PU_AMENITY-restaurant_per_mi2       | Median **lower** when `take_current_trip` is `TRUE`.|\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| fig-height: 7.5\n\n\nggplot(TopOsmMedianDiff) +\n  geom_boxplot(aes(x = value,\n                   y  = variable,\n                   fill = take_current_trip)) +\n  scale_fill_manual(values = c(\"TRUE\" = ColorHighlight,\n                               \"FALSE\" = ColorGray))+\n  scale_x_continuous(transform = scales::new_transform(\"signed_log10\",\n                                                       transform = function(x) sign(x) * log10(1 + abs(x)),\n                                                       inverse = function(x) sign(x) * (10^(abs(x)) - 1),\n                                                       breaks = scales::pretty_breaks()),\n                     labels = scales::label_comma(accuracy = 0.1),\n                     breaks = c(0, 10^seq(-1,4, by = 0.25))) +\n  labs(x = \"\",\n       y = \"\") +\n  theme_minimal()\n```\n\nAfter confirming these promising results we can save the most import features for future use.\n\n```r\nFeaturesToKeep <- \n  c(\"LocationID\", sub(\"(PU|DO)_\",\"\", TopOsmMedianDiffFeatures)) |>\n  unique()\n\nfst::write_fst(OmsDensityFeatures[, ..FeaturesToKeep], \n               path = here(\"output/OmsDensityFeatures.fst\"))\n```\n\n\n## Conclusion\n\nWe know that these features will be useful for modeling, especially for finding correlations with weekdays and hours.\n\nIt's important to take in consideration that the data is **right skewed** and present some **high-leverage points** that will need to be solved depending on the model to select.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"07-expanding-geospatial-data.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"source","theme":"cosmo","title":"Expanding Geospatial Information","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}