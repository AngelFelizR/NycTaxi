{"title":"Exploring Transportation and Socioeconomic Patterns","markdown":{"yaml":{"title":"Exploring Transportation and Socioeconomic Patterns","editor_options":{"chunk_output_type":"console"},"execute":{"message":false,"warning":false}},"headingText":"Setting up the environment","containsRefs":false,"markdown":"\n\nExpanding on the previous post's geospatial feature extraction, this article will delve into enriching our understanding of NYC taxi zones with socioeconomic and transportation-related data from the **US Census Bureau**. This new layer of information will provide crucial context for analyzing `take_current_trip` by revealing underlying community characteristics and commuting behaviors within each zone.\n\nHere are the steps to follow:\n\n1. **Importing training data with zone shapes**.\n\n1.  **Import Census data**:\n    * Identify relevant variables.\n    * Download data for Manhattan, Queens and Brooklyn with geometry.\n\n2.  **Spatially join ACS data to taxi zones**:\n    * Perform spatial joins to aggregate census tract data to the taxi zone level.\n    * Address overlapping and partially contained tracts.\n\n3.  **Extract new features**:\n    * Calculate proportions, averages, and densities of the ACS variables for each taxi zone.\n\n4.  **Validation**:\n    * Integrate new features with existing data and `take_current_trip`.\n    * Define the correlation between new predictors and `take_current_trip`.\n    \n    \n\n### Define colors to use\n\n```{r}\nBoroughColors <- c(\n  'Manhattan' = '#e41a1c',\n  'Queens' = '#377eb8',\n  'Brooklyn'= '#4daf4a'\n)\n\nBoroughSelected <- names(BoroughColors)\n\nColorHighlight <- \"lightslateblue\"\nColorHighlightLow <- \"#D3CDF7\"\nColorGray <- \"gray80\"\n```\n\n### Loading packages\n\n```{r}\n## To manage relative paths\nlibrary(here)\n\n## To transform data that fits in RAM\nlibrary(data.table)\n\n## To work with spatial data\nlibrary(sf)\n\n# To work and plot graphs\nlibrary(tidytext)\nlibrary(stringdist)\nlibrary(hashr)\nlibrary(igraph)\n\n# Access to US Census Bureau datasets\nlibrary(tidycensus)\n\n## To create general plots\nlibrary(ggplot2)\nlibrary(scales)\n\n## To create table\nlibrary(gt)\n\n## Custom functions\ndevtools::load_all()\n```\n\n:::{.callout-note title=\"Computational Performance\"}\nThe geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.\n:::\n\n## Importing training data with zone shapes\n\nDetails on how this data was obtained can be found in the **Data Collection Process**.\n\n```{r}\nZonesShapes <-\n  read_sf(here(\"raw-data/taxi_zones/taxi_zones.shp\")) |>\n  (\\(df) df[df[[\"borough\"]] %chin% BoroughSelected,\n            c(\"OBJECTID\", \"LocationID\")])() |>\n  st_cast(to = \"MULTIPOLYGON\")\n\nTrainingSample <-\n  here(\"output/take-trip-fst\") |>\n  list.files(full.names = TRUE) |>\n  (\\(x) data.table(full_path = x,\n                   n_char = nchar(basename(x)),\n                   name = basename(x)))() |>\n  (\\(dt) dt[order(n_char, name), full_path])() |>\n  head(12L) |>\n  lapply(FUN = fst::read_fst,\n         columns = c(\"trip_id\",\"PULocationID\", \"DOLocationID\", \"take_current_trip\"),\n         as.data.table = TRUE) |>\n  rbindlist()\n```\n\n## Import Census data\n\n### Identify relevant variables\n\nBased on the `tidycensus` documentation we have the below tables available for exploration.\n\n```{r}\nValidTables = c(\n  \"sf1\", \"sf2\", \"sf3\", \"sf4\", \"pl\", \"dhc\", \"dp\", \n  \"ddhca\", \"ddhcb\", \"sdhc\", \"as\", \"gu\", \"mp\", \n  \"vi\", \"acsse\", \"dpas\", \"dpgu\", \"dpmp\", \"dpvi\",\n  \"dhcvi\", \"dhcgu\", \"dhcvi\", \"dhcas\", \"acs1\", \n  \"acs3\", \"acs5\", \"acs1/profile\", \"acs3/profile\",\n  \"acs5/profile\", \"acs1/subject\", \"acs3/subject\",\n  \"acs5/subject\", \"acs1/cprofile\", \"acs5/cprofile\",\n  \"sf2profile\", \"sf3profile\", \"sf4profile\", \"aian\",\n  \"aianprofile\", \"cd110h\", \"cd110s\", \"cd110hprofile\",\n  \"cd110sprofile\", \"sldh\", \"slds\", \"sldhprofile\",\n  \"sldsprofile\", \"cqr\", \"cd113\", \"cd113profile\",\n  \"cd115\", \"cd115profile\", \"cd116\", \"plnat\", \"cd118\"\n)\n\nlength(ValidTables)\n```\n\nThat means that we have a lot a data to use, but to be able to get the list of variables available for each table we need to know the different year when each table was updated. In our particular case, we only know  the **last year when each table was updated from 2020 to 2020**.\n\nTo solve that question, we only need to scrap the table available in [https://api.census.gov/data.html](https://api.census.gov/data.html) and apply our particular selection to that huge table. \n\n```{r}\n#| echo: false\n#| output: false\n\nValidTablesYearsFilePath = here(\"output/cache-data/08-expanding-transportation-socioeconomic/ValidTablesYears.fst\")\n\n\nif(file.exists(ValidTablesYearsFilePath)) {\n  ValidTablesYears <- \n    fst::read_fst(ValidTablesYearsFilePath, as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nCensusTables <-\n  rvest::read_html(\"https://api.census.gov/data.html\") |>\n  rvest::html_table() |>\n  (\\(x) x[[1L]])() |>\n  as.data.table() |>\n  subset(!is.na(`API Base URL`)\n         & Vintage != \"N/A\",\n         select = c(\"Title\",\n                    \"Description\",\n                    \"Vintage\",\n                    \"Dataset Type\",\n                    \"Dataset Name\",\n                    \"API Base URL\"))\n\nCensusTables[,`:=`(Vintage = as.integer(Vintage),\n                   `Dataset Name` = gsub(\"â€º \", \"/\", `Dataset Name`))]\n\nCensusTables[, `:=`(survey = fcase(`Dataset Name` %like% \"^dec\",\n                                   \"dec\",\n                                   `Dataset Name` %like% \"^acs\",\n                                   \"acs\"),\n                    `Dataset Name` = gsub(\"(dec|acs)/\", \"\", `Dataset Name`))]\n\nsetorder(CensusTables, -Vintage)\n\nValidTablesYears <-\n  CensusTables[\n    Vintage %between% c(2020, 2022)\n  ][ValidTables,\n    on = \"Dataset Name\",\n    nomatch = NULL,\n    unique(.SD, by = \"Dataset Name\"),\n    .SDcols = c(\"Vintage\", \"Dataset Name\", \"survey\")]\n\nValidTablesYears[, head(.SD, 5),\n                 by = \"survey\"]\n\n```\n\n```{r}\n#| echo: false\n\nValidTablesYears[, head(.SD, 5),\n                 by = \"survey\"]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ValidTablesYears, ValidTablesYearsFilePath)\n```\n\nNow that we have the years related to each table, let's apply `tidycensus::load_variables` for each table and year to see the variables to explore. \n\n```{r}\n#| echo: false\n#| output: false\n\nValidTablesLastYearFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/ValidTablesLastYear.fst\")\n\n\nif(file.exists(ValidTablesLastYearFilePath)) {\n  ValidTablesLastYear <- \n    fst::read_fst(ValidTablesLastYearFilePath, as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nget_all_data = function(x, ref_df){\n  \n  imported_data =\n    load_variables(ref_df$Vintage[x],\n                   ref_df$`Dataset Name`[x])\n  \n  setDT(imported_data)\n  \n  imported_data[, `:=`(year = ref_df$Vintage[x],\n                       dataset = ref_df$`Dataset Name`[x],\n                       survey = ref_df$survey[x])]\n  \n  return(imported_data)\n  \n} \n  \nValidTablesLastYear <-\n  lapply(seq_along(ValidTablesYears$`Dataset Name`), \n         get_all_data,\n         ref_df = ValidTablesYears)|>\n  rbindlist(use.names = TRUE, fill = TRUE)\n\nnrow(ValidTablesLastYear) |> comma()\n```\n\n```{r}\n#| echo: false\n\nnrow(ValidTablesLastYear) |> comma()\n```\n\nAfter making that exploration we can see that we have almost **160K variables** to explore.\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ValidTablesLastYear, ValidTablesLastYearFilePath)\n```\n\nTo understand what have we have let's plot the number of variables by dataset and survey for each year.\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| fig-height: 7.5\n\nVariablesPerDataset <-\n  ValidTablesLastYear[, .N, \n                      .(year = factor(year), \n                        dataset,\n                        survey)\n  ][, dataset := reorder(dataset, N, FUN = sum)] \n\nggplot(VariablesPerDataset) +\n  geom_col(aes(N, dataset, fill = year),\n           color = \"black\",\n           linewidth = 0.2) +\n  scale_fill_manual(values = c(\"2022\" = ColorHighlight,\n                               \"2020\" = ColorGray)) +\n  scale_x_continuous(labels = scales::label_comma(scale = 1/1000, suffix = \"k\"))+\n  expand_limits(x = 1e4) +\n  labs(x = \"Number of variables\",\n       fill = \"Last update year\")+\n  facet_wrap(vars(survey), scale = \"free\") +\n  theme_minimal()+\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.y = element_blank())\n```\n\nIt's clear that **there is one way we can read the variables one by one** and hypothesize the variables to select. Instead, we are going to flow some steps to reduce the number of variable and the classify them in to groups to select.\n\nJust by looking to the raw data, I found that many variables measure the same thing for particular groups based on:\n\n- Race\n- Place of Birth\n- Sex\n\n```{r}\nVariablesToIgnore <- c(\n  \"Hispanic\",\n  \"Latino\",\n  \"American Indian\",\n  \"Alaska Native\",\n  \"White\",\n  \"Black\",\n  \"African American\",\n  \"Asian\",\n  \"Native Hawaiian\",\n  \"Pacific Islander\",\n  \"Races?\",\n  \n  \"Ancestry\",\n  \"Nativity\",\n  \"Citizenship Status\",\n  \"Place of Birth\",\n  \"Naturalization\",\n  \"Current Residence\",\n  \n  \"Male\",\n  \"Female\",\n  \"Women\",\n  \"Men\"\n)\n```\n\nAs we will join the data based on geography it doesn't make sense to have predictors based on the situations of those particular groups as we won't have does details of passengers of the trips.\n\n\n```{r}\nConceptsToExplore <-\n  ValidTablesLastYear[!concept %ilike% paste(VariablesToIgnore, collapse = \"|\")\n                      & !label %ilike% \"Male|Female\",\n                      .(concept = unique(concept))]\n\nnrow(ConceptsToExplore)\n```\n\nAnother aspect we found out by exploring the data it's that some variables names are really long and the long the concept definition the more specific the variable and less useful for our model. For example, below we can see the variable with the longer number of characters:\n\n```{r}\n#| echo: false\n#| output: asis\n\nConceptsToExplore[nchar(concept) == max(nchar(concept)), paste0(\"> \", concept)] |> cat()\n```\n\n- Population Measured: \n  - Grandparents living with own grandchildren under 18 years\n  - 30 years and over\n  - In households (excluding military housing units)\n\n- Data Categories:\n  - By responsibility for own grandchildren\n  - By length of time responsible for grandchildren\n\nBy exploring the cumulative distribution we can see that near 50% of the variable present less **70 characters** per variable which seems the better group to focus the exploration\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nConceptsToExplore[, concept_length := nchar(concept)]\n\nggplot(ConceptsToExplore)+\n  stat_ecdf(aes(concept_length)) +\n  scale_x_continuous(breaks = breaks_width(10))+\n  scale_y_continuous(labels = label_percent(accuracy = 1))+\n  labs(title = \"Empirical Cumulative Distribution\",\n       subtitle = \"Number of Character per Variable\",\n       x = \"Number of Characters\",\n       y = \"Cumulative Proportion\") +\n  theme_minimal()+\n  theme(panel.grid.minor = element_blank())\n```\n\nAfter applying that change we only have 675 pending variables to explore the classify.\n\n```{r}\nConceptsToExploreMaxConceptLength <- 70\n\nConceptsToExploreValidConceptLength <- \n  ConceptsToExplore[concept_length <= ConceptsToExploreMaxConceptLength]\n\nConceptsToExploreValidConceptLength[, .N]\n```\n\nTo confirm if that action make sense we can print the longer variables and confirm that aren't too specific.\n\n```{r}\n#| output: asis\n#| echo: false\n\nConceptsToExploreValidConceptLength[concept_length == max(concept_length), \n                                    paste0(\"- `\", concept, \"`\")] |>\n  head() |>\n  cat(sep = \"\\n\")\n```\n\nNow we can calculate the **pairwise Jaccard string distances** of each of the variables to identify conceptually similar terms based on similar words to then identify variable clusters to select.\n\n1. Create word-level hash tokens (not character-level) after case normalization.\n\n```{r}\nConceptsToExploreHash <- \n  tolower(ConceptsToExploreValidConceptLength$concept) |>\n  strsplit(\"\\\\s+\") |> \n  hash()\n```\n\n2. Compute pairwise Jaccard distances between hashed word representations.\n\n```{r}\nConceptsToExploreWordDistance <-\n  seq_distmatrix(ConceptsToExploreHash, \n                 ConceptsToExploreHash, \n                 method = \"jaccard\", \n                 q = 1)\n```\n\n3. Assign concept names to matrix dimensions for interpretability.\n\n```{r}\nrownames(ConceptsToExploreWordDistance) <- ConceptsToExploreValidConceptLength$concept\ncolnames(ConceptsToExploreWordDistance) <- ConceptsToExploreValidConceptLength$concept\n```\n\n4. Transform symmetric matrix to long format, eliminating redundant pairwise comparisons.\n\n```{r}\nConceptsToExploreWordDistance[upper.tri(ConceptsToExploreWordDistance, diag = TRUE)] <- NA\n\nConceptsToExploreWordDistanceDt <-\n  as.data.table(ConceptsToExploreWordDistance,\n                keep.rownames = \"Variable Name 1\") |>\n  melt(id.vars = \"Variable Name 1\",\n       variable.name = \"Variable Name 2\",\n       variable.factor = FALSE,\n       value.name = \"Jaccard Distance\",\n       na.rm = TRUE)\n```\n\n\nWith this inter-concept distance we can use the `edge_betweenness` argoritm to define the cluster to use. But as the result for this method will change depending the min Jaccard Distance selected to do the analysis, we need to explore several options before taking a final decision.\n\n```{r}\n#| echo: false\n#| output: false\n\nConceptsToExploreByDistanceFilePath <-\n  here(\"output/cache-data/08-expanding-transportation-socioeconomic/ConceptsToExploreByDistance.fst\")\n\nif(file.exists(ConceptsToExploreByDistanceFilePath)) {\n  ConceptsToExploreByDistance <-\n    fst::read_fst(ConceptsToExploreByDistanceFilePath, as.data.table = TRUE)\n}\n```\n\n\n```{r}\n#| eval: false\n\ndefine_clusters_by_distance <- function(min_jaccard_distance,\n                                        dt){\n  \n  dt_filtered = dt[`Jaccard Distance` <= min_jaccard_distance]\n  \n  temp_graph =\n    graph_from_data_frame(dt_filtered, directed = FALSE)\n  \n  temp_cluster = cluster_edge_betweenness(temp_graph)\n  \n  node_results = data.table(\n    node = V(temp_graph)$name,\n    cluster = temp_cluster$membership,\n    `Jaccard Distance Threshold` = min_jaccard_distance,\n    modularity = max(temp_cluster$modularity),\n    n_clusters = max(temp_cluster$membership)\n  )\n  \n  return(node_results)\n  \n}\n\n\nConceptsToExploreByDistance <-\n  lapply(seq(0.1, 0.8, by = 0.10),\n         FUN = define_clusters_by_distance,\n         dt = ConceptsToExploreWordDistanceDt) |>\n  rbindlist()\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ConceptsToExploreByDistance, ConceptsToExploreByDistanceFilePath)\n```\n\nAfter exploring several configurations we can see that that the as we expand the limit of Jaccard Distance the modularity decrease, but the number of clusters initially increases (peaking around 0.3) and then steadily decreases until 0.7, after which it increases sharply again.\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nConceptsToExploreByDistance[, unique(.SD),\n                            .SDcols = c(\"Jaccard Distance Threshold\",\n                                        \"modularity\",\n                                        \"n_clusters\")] |>\n  melt(id.vars = \"Jaccard Distance Threshold\",\n       variable.name = \"cluster_metric\") |>\n  ggplot(aes(`Jaccard Distance Threshold`, value)) +\n  geom_line(linewidth = 1,\n            color = \"gray40\") +\n  geom_point(size = 3,\n             color = \"gray40\") +\n  geom_vline(xintercept = 0.5,\n             linetype = \"dashed\",\n             color = ColorHighlight,\n             linewidth = 0.9) +\n  # Add the annotation here\n  annotate(\"text\", x = 0.5, y = Inf, label = \"0.5 Threshold\",\n           color = ColorHighlight, vjust = 1.2, hjust = -0.05, size = 4) +\n  scale_y_continuous(labels = \\(x) fifelse(x <= 1 & x > 0,\n                                            label_percent()(x),\n                                            label_comma(accuracy = 1)(x)),\n                     breaks = breaks_pretty(7)) +\n  scale_x_continuous(breaks = breaks_width(0.10)) +\n  expand_limits(y = 0) +\n  facet_wrap(vars(cluster_metric),\n             scale = \"free_y\",\n             ncol = 1L,\n             strip.position = \"top\") +\n  labs(\n    title = \"Modularity and Number of Clusters vs. Jaccard Distance Threshold\",\n    subtitle = \"Assessing the Trade-off: High Modularity and Low Number of Clusters\",\n    x = \"Jaccard Distance Threshold\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"),\n        plot.subtitle = element_text(color = \"gray25\"),\n        strip.text = element_text(size = 12, face = \"bold\"),\n        axis.text = element_text(size = 10))\n```\n\nBased on this information **the best threshold is 0.5 Jaccard Distance** as the modularity is higher than 0.8 and the algorithm found less than 60 clusters. After than point the slope to reduce the number of clusters gets lower as concurrence, we don't see worthy to keep increasing the threshold.\n\n```{r}\nConceptsToExploreClusters <-\n  ConceptsToExploreByDistance[.(0.5),\n                              on = \"Jaccard Distance Threshold\"]\n```\n\n\nTo understand what kind of variables we have in each cluster lets use the **statistic tf-idf** to list the **top 5 words** (after removing stop words) of each cluster and use that reference to select the must meaningful ones for this context.\n\n```{r}\nCustomStopWords <- c(\n  stop_words[stop_words$lexicon == \"snowball\", c(\"word\")][[1]],\n  \"estimate\",\n  \"estimates\",\n  \"percent\",\n  \"months\",\n  \"past\",\n  \"detailed\",\n  \"type\",\n  \"types\",\n  \"current\",\n  \"adjusted\",\n  \"united\",\n  \"round\",\n  \"selected\",\n  \"median\",\n  \"level\",\n   NA_character_\n)\n\nTopWordsByCluster <-\n  ConceptsToExploreClusters[, .(cluster,\n                                concept_lower = tolower(node))\n  ][, unnest_tokens(.SD, word, concept_lower)\n  ][!word %chin% CustomStopWords\n    & !word %like% \"^\\\\d+$\",\n    .N,\n    by = c(\"cluster\", \"word\")\n  ][, dataset_total := sum(N), \n    by = \"cluster\"\n  ][, bind_tf_idf(.SD, word, cluster, N)\n  ][order(-tf_idf)\n  ][, .(`Top Words` =\n          head(word, 5L) |> \n          stringr::str_to_sentence() |>\n          paste0(collapse = \", \")),\n    keyby = \"cluster\"\n  ][ConceptsToExploreClusters,\n    on = \"cluster\",\n    j = .(`Node Count` = .N), \n    by = c(\"Top Words\", \"cluster\")\n  ][order(- `Node Count`, cluster),\n    .(`Cluster ID` = cluster, `Top Words`, `Node Count`)]\n```\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ngt(TopWordsByCluster) |>\n  # Apply visual styling\n  tab_header(\n    title = \"Key Variable Clusters from Census Data Analysis\",\n    subtitle = \"Top representative words and cluster sizes for socioeconomic & transportation patterns\"\n  ) |>\n  tab_style(\n    style = cell_text(color = \"white\", weight = \"bold\"),\n    locations = cells_title()\n  ) |>\n  tab_style(\n    style = cell_fill(color = \"#2c3e50\"),\n    locations = cells_title()\n  ) |>\n  data_color(\n    columns = `Node Count`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", ColorHighlightLow, ColorHighlight),\n      domain = c(0, max(TopWordsByCluster$`Node Count`))\n    )\n  )|>\n  fmt_number(\n    columns = `Node Count`,\n    decimals = 0\n  ) |>\n  cols_align(\n    align = \"left\",\n    columns = `Top Words`\n  ) |>\n  cols_label(\n    `Cluster ID` = \"CLUSTER\",\n    `Top Words` = \"TOP REPRESENTATIVE TERMS\",\n    `Node Count` = \"# VARIABLES\"\n  ) |>\n  tab_options(\n    table.font.names = \"Arial\",\n    table.background.color = \"white\",\n    column_labels.font.weight = \"bold\",\n    column_labels.background.color = \"#f8f9fa\",\n    row_group.background.color = \"#f1f3f5\",\n    #row.striping.include_table_body = TRUE,\n    quarto.disable_processing = TRUE\n  ) |>\n  tab_source_note(\n    source_note = \"Clusters identified through NLP analysis of US Census variable concepts\"\n  )\n```\n\nBased on the analysis, the following clusters are selected for inclusion due to their direct relevance to transportation behavior, income patterns, and population density - key factors influencing taxi demand (`take_current_trip`):\n\n| Cluster ID | Top Words                          | Node Count | Reason for Inclusion                                                                 |\n|------------|------------------------------------|------------|--------------------------------------------------------------------------------------|\n| **18**     | Work, Transportation, Means, Time, Workplace | 32    | **Direct commuting behavior** - primary driver of taxi demand patterns          |\n| **13**     | Population, Years, Civilian, Employed, Age | 32      | **Employment density** - correlates with business/event-related trips           |\n| **36**     | Households, Income, Earnings, Dollars, Security | 19 | **Income capacity** - strongest predictor of taxi affordability and usage frequency|\n\n\n```{r}\nConceptsToExploreCleanConcept <-\n  ConceptsToExploreClusters[.(c(18, 13, 36)),\n                            on = \"cluster\",\n                            .(concept = stringr::str_to_sentence(node),\n                              cluster)]\n\nConceptByDataset <-\n  ValidTablesLastYear[, .(concept = stringr::str_to_sentence(concept),\n                          year,\n                          dataset,\n                          survey)\n  ][, unique(.SD)]\n\n\nConceptByDatasetToExplore <-\n  ConceptByDataset[ConceptsToExploreCleanConcept,\n                   on = \"concept\"]\n```\n\n\nChecking the data we can see that some concept are duplicated in some in some datasets, as we can see in the next table.\n\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nConceptByDatasetToExplore[, .(N = .N, \n                              dataset = sort(dataset) |> paste(collapse = \", \")), \n                          by = \"concept\"\n][N > 1, \n  .(variables =  .N), \n  keyby = \"dataset\"] |>\n  \n  \n  gt() |>\n  \n  # Header\n  tab_header(\n    title = md(\"**Dataset Variable Analysis**\"),\n    subtitle = \"Distribution of shared variables across survey datasets\"\n  ) |>\n  \n  # Col Names\n  cols_label(\n    dataset = \"Dataset Collection\",\n    variables = \"Number of Variables\"\n  ) |>\n  \n  # Number Format\n  fmt_number(columns = variables, decimals = 0) |>\n  \n  # Colors\n  data_color(\n    columns = variables,\n    colors = scales::col_numeric(\n      palette = c(\"white\",ColorHighlightLow, ColorHighlight),\n      domain = NULL\n    )\n  ) |>\n  tab_style(\n    style = cell_text(color = \"white\", weight = \"bold\"),\n    locations = cells_title()\n  ) |>\n  tab_style(\n    style = cell_fill(color = \"#2c3e50\"),\n    locations = cells_title()\n  ) |>\n  \n  # Options\n  tab_options(\n    table.font.names = \"Arial\",\n    table.background.color = \"white\",\n    column_labels.font.weight = \"bold\",\n    column_labels.background.color = \"#f8f9fa\",\n    row_group.background.color = \"#f1f3f5\",\n    #row.striping.include_table_body = TRUE,\n    quarto.disable_processing = TRUE\n  )\n```\n\n\nWe can see that we have 2 main groups of cases:\n\n- Variables shared between acs1 (subject) and acs5 (subject).\n- Variables shared between datasets of the decennial US Census.\n\nBy reading about the differences between each datasets, we are going to focus in the variables available for the next datasets:\n\n- **acs5** and **acs5/subject**\n\n    - We are going to train the model based on one year data but we want to be able to find patterns that remain for future years as our testing data have trips for 2025 we expect that the model keep working after that year.\n    \n\n- **dhc**\n\n    - We are exploring only NYC so won't need information from indepent island and as result the information provided by those datasets won't be relevant for our training goal.\n\n\n```{r}\nConceptByValidDatasetToExplore <-\n  ConceptByDatasetToExplore[c(\"acs5\", \"acs5/subject\", \"dhc\"),\n                            on = \"dataset\",\n                            .(dataset, concept, cluster)]\n\n```\n\nNow we are ready to identify the particular codes (names) related to each variable concept. In this step are going to exclude variables that:\n\n- Only provide information based on sex with the exception of `B08013_001` which provide information about \"travel time to work (in minutes) of workers\".\n- Don't have information at tract level of geography.\n\n```{r}\nValidTablesLastYear[, concept := stringr::str_to_sentence(concept)]\n\nVariablesToUse <-\n  ValidTablesLastYear[\n    ConceptByValidDatasetToExplore,\n    on = c(\"dataset\",\"concept\")\n    \n  # Only provide information based on sex\n  ][!label %ilike% \"Male|Female\"\n  ][, concept_count := .N, \n    by = \"concept\"\n  ][name == \"B08013_001\"\n    | concept_count > 2L\n    \n  # Don't have information at tract \n  ][fcoalesce(geography, \"\") != \"county\"]\n\nVariablesToUse[, .(N = comma(.N)), \n               by = c(\"survey\", \"year\")]\n```\n\n### Download data for Manhattan, Queens and Brooklyn with geometry\n\nAfter defining the variables to use we only need to define the queries to get the data from the API with `{tidycensus}` for the American Community Survey (ACS) and Decennial US Census (DEC).\n\n::: {.callout-note}\nAs we are getting also the geometry information is much efficient to change the `output` argument default from \"tidy\" to \"wide\" to avoid having a list column repeating the same geometries over and over after applying the function `tidyr::gather`.\n\n**This will make the spatial join independent to the number of variables to query.**\n:::\n\n```{r}\n#| echo: false\n#| output: false\n\nAcsVariableTractsFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/AcsVariableTracts.qs\")\n\nif(file.exists(ValidTablesLastYearFilePath)) {\n  AcsVariableTracts <- qs2::qs_read(AcsVariableTractsFilePath)\n}\n\nDecVariableTractsFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/DecVariableTracts.qs\")\n\nif(file.exists(ValidTablesLastYearFilePath)) {\n  DecVariableTracts <- qs2::qs_read(DecVariableTractsFilePath)\n}\n```\n\n```{r}\n#| eval: false\n\nAcsVariableTracts <-\n  get_acs(geography = \"tract\", \n          variables = VariablesToUse[\"acs\", on = \"survey\", name],\n          year = 2022, \n          state = \"NY\",\n          # Manhattan, Brooklyn, Queens\n          county =  c(\"New York\", \"Kings\", \"Queens\"),\n          geometry = TRUE,\n          survey = \"acs5\",\n          output = \"wide\") |>\n  st_transform(crs = st_crs(ZonesShapes)$epsg)\n\nDecVariableTracts <-\n  get_decennial(geography = \"tract\", \n                variables = VariablesToUse[\"dec\", on = \"survey\", name],\n                sumfile = \"dhc\",\n                year = 2020, \n                state = \"NY\",\n                # Manhattan, Brooklyn, Queens\n                county =  c(\"New York\", \"Kings\", \"Queens\"),\n                geometry = TRUE,\n                output = \"wide\") |>\n  st_transform(crs = st_crs(ZonesShapes)$epsg)\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nqs2::qs_save(AcsVariableTracts, AcsVariableTractsFilePath)\n\nqs2::qs_save(DecVariableTracts, DecVariableTractsFilePath)\n```\n\nTo it's the perfect time to join all variables into one table.\n\n```{r}\nConsolidatedVariableTracts <- \n  dplyr::full_join(AcsVariableTracts, \n                   st_drop_geometry(DecVariableTracts), \n                   by = c(\"GEOID\", \"NAME\"))\n\ndim(ConsolidatedVariableTracts)\ndim(AcsVariableTracts)\ndim(DecVariableTracts)\n```\n\n\n## Spatial joins\n\nWe need to join to the original zones of our training set in to 2 steps:\n\n- Join the tracts full contained within the original zone shapes.\n- Join the tracts that overlaps in more than one zone shape.\n\n```{r}\n#| eval: false\n\nConsolidatedVariableTractsJoinedList <-\n  lapply(list(within = st_within,\n              overlaps = st_overlaps), \n         FUN = st_join,\n         x = ConsolidatedVariableTracts,\n         y = ZonesShapes,\n         left = FALSE)\n```\n\nNow we need to go over each of the original zone shapes and cut the tracts parts that are not contained on each zone and consolidate the within and overlaps elements of `TravelTimeToWorkTractJoinedList` into one data frame.\n\n```{r}\n#| echo: false\n#| output: false\n\nConsolidatedVariableTractsJoinedFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/ConsolidatedVariableTractsJoined.qs\")\n\n\nif(file.exists(ConsolidatedVariableTractsJoinedFilePath)) {\n  ConsolidatedVariableTractsJoined <-\n    qs2::qs_read(ConsolidatedVariableTractsJoinedFilePath)\n}\n```\n\n```{r}\n#| eval: false\n\napply_intersection_by_id <- function(features,\n                                     shapes,\n                                     id_col){\n  \n  intersection_by_id = function(id, features, shapes, id_col) {\n    \n    features_sub = features[features[[id_col]] == id, ]\n    shapes_sub_geo = shapes[shapes[[id_col]] == id, ] |> st_geometry()\n    \n    new_df = \n      st_intersection(features_sub,\n                      shapes_sub_geo) |>\n      st_cast(to = \"MULTIPOLYGON\")\n\n    return(new_df)\n  }\n  \n  new_df =\n    lapply(unique(features[[id_col]]),\n           FUN = intersection_by_id,\n           features = features,\n           shapes = shapes,\n           id_col = id_col) |>\n    do.call(what = \"rbind\")\n  \n  return(new_df)\n  \n}\n\nConsolidatedVariableTractsJoinedList$overlaps <-\n  apply_intersection_by_id(ConsolidatedVariableTractsJoinedList$overlaps,\n                           ZonesShapes,\n                           id_col = \"OBJECTID\")\n\nConsolidatedVariableTractsJoined <- \n  do.call(what = \"rbind\", \n          ConsolidatedVariableTractsJoinedList)\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nqs2::qs_save(ConsolidatedVariableTractsJoined, ConsolidatedVariableTractsJoinedFilePath)\n```\n\nPerfect, we have joined the to shapes, but we still need to have only one estimate value per original zone shape. To solve this challenge we only need to use the `weighted arithmetic mean` based on the proportion of the area add each tract to the original zone id.\n\n```{r}\n#| echo: false\n#| output: false\n\nConsolidatedVariableTractsByZoneIdFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/ConsolidatedVariableTractsByZoneId.fst\")\n\n\nif(file.exists(ConsolidatedVariableTractsByZoneIdFilePath)) {\n  ConsolidatedVariableTractsByZoneId <-\n    fst::read_fst(ConsolidatedVariableTractsByZoneIdFilePath,\n                  as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nConsolidatedVariableTractsJoined$tract_area <-\n  st_area(ConsolidatedVariableTractsJoined$geometry)\n\nConsolidatedVariableTractsJoinedDt <-\n  st_drop_geometry(ConsolidatedVariableTractsJoined) |>\n  setDT()\n\nIdColumns <-\n  c(\"GEOID\", \n    \"NAME\", \n    \"tract_area\",\n    \"LocationID\", \n    \"OBJECTID\")\n\nConsolidatedVariableTidy <-\n  melt(ConsolidatedVariableTractsJoinedDt,\n       id.var = IdColumns,\n       variable.factor = FALSE\n  )[,`:=`(variable_type = fifelse(variable %like% \"\\\\dM$\", \"moe\", \"estimate\"),\n          variable = sub(\"(E|M)$\", \"\", variable))\n  ][, dcast(.SD,\n            formula = as.formula(paste0(paste0(c(IdColumns, \"variable\"), collapse = \" + \"), \"~ variable_type\")),\n            value.var = \"value\")\n  ][, `:=`(NAME = NULL,\n           OBJECTID = NULL,\n           moe = NULL,\n           estimate_low = estimate - fcoalesce(moe, 0),\n           estimate_high = estimate + fcoalesce(moe, 0))\n  ][estimate_low < 0, \n    estimate_low := 0\n  ][, area_prop :=  as.double(tract_area / sum(tract_area, na.rm = TRUE)),\n    by = c(\"LocationID\", \"variable\")]\n\n\nConsolidatedVariableTractsByZoneId <-\n  ConsolidatedVariableTidy[\n    j = .(estimate  = sum(estimate * area_prop, na.rm = TRUE),\n          estimate_low  = sum(estimate_low * area_prop, na.rm = TRUE),\n          estimate_high  = sum(estimate_high * area_prop, na.rm = TRUE)),\n    by = c(\"LocationID\", \"variable\")\n  ][, melt(.SD,\n           measure.vars = c(\"estimate\", \"estimate_low\", \"estimate_high\"),\n           variable.factor = FALSE,\n           variable.name = \"variable_type\")\n  ][, dcast(.SD, LocationID ~ variable + variable_type, value.var = \"value\")]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ConsolidatedVariableTractsByZoneId, ConsolidatedVariableTractsByZoneIdFilePath)\n```\n\n\n```{r}\ndim(ConsolidatedVariableTractsByZoneId)\n```\n\n\n## Validation\n\n```{r}\n#| echo: false\n#| output: false\n\nPuVariableTractsMeanDiffFilePath <-\n  here(\"output/cache-data/08-expanding-transportation-socioeconomic/PuVariableTractsMeanDiff.fst\")\n\nDoVariableTractsMeanDiffFilePath <-\n  here(\"output/cache-data/08-expanding-transportation-socioeconomic/DoVariableTractsMeanDiff.fst\")\n\n\nif(file.exists(PuVariableTractsMeanDiffFilePath)) {\n  PuVariableTractsMeanDiff <-\n    fst::read_fst(PuVariableTractsMeanDiffFilePath,\n                  as.data.table = TRUE)\n}\n\nif(file.exists(DoVariableTractsMeanDiffFilePath)) {\n  DoVariableTractsMeanDiff <-\n    fst::read_fst(DoVariableTractsMeanDiffFilePath,\n                  as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nPuVariableTractsMeanDiff <-\n  ConsolidatedVariableTractsByZoneId[TrainingSample[, .(LocationID = PULocationID, \n                                                        take_current_trip)], \n                                     on = \"LocationID\"\n  ][, lapply(.SD, \\(x) if(uniqueN(x) == 1L) x[1L] else x[!is.na(x)] |> scale(center = FALSE) |> median()),\n    .SDcols = !c(\"LocationID\"),\n    by = \"take_current_trip\"\n  ][, melt(.SD,\n           id.vars = \"take_current_trip\",\n           variable.factor = FALSE)\n  ][, .(not_take_vs_take = diff(value)),\n    by = \"variable\"\n  ][order(-abs(not_take_vs_take))]\n\n\nDoVariableTractsMeanDiff <-\n  ConsolidatedVariableTractsByZoneId[TrainingSample[, .(LocationID = DOLocationID,\n                                                        take_current_trip)], \n                                     on = \"LocationID\"\n  ][, lapply(.SD, \\(x) if(uniqueN(x) == 1L) x[1L] else x[!is.na(x)] |> scale(center = FALSE) |> median()),\n    .SDcols = !c(\"LocationID\"),\n    by = \"take_current_trip\"\n  ][, melt(.SD,\n           id.vars = \"take_current_trip\",\n           variable.factor = FALSE)\n  ][, .(not_take_vs_take = diff(value)),\n    by = \"variable\"\n  ][order(-abs(not_take_vs_take))]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(PuVariableTractsMeanDiff, PuVariableTractsMeanDiffFilePath)\nfst::write_fst(DoVariableTractsMeanDiff, DoVariableTractsMeanDiffFilePath)\n```\n\n```{r}\nVariableTractsMeanDiff <-\n  rbindlist(list(PuVariableTractsMeanDiff,\n                 DoVariableTractsMeanDiff)\n  )[!is.na(not_take_vs_take)]\n```\n\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nggplot(VariableTractsMeanDiff) +\n  geom_histogram(aes(not_take_vs_take)) +\n  scale_y_continuous(transform = scales::new_transform(\"signed_log10\",\n                                                       transform = function(x) sign(x) * log10(1 + abs(x)),\n                                                       inverse = function(x) sign(x) * (10^(abs(x)) - 1),\n                                                       breaks = scales::pretty_breaks()),\n                     breaks = c(0, 10^(1:4))) +\n  scale_x_continuous(breaks = breaks_width(0.1)) +\n  labs(y = \"count on log10 scale\")+\n  theme_minimal()\n```\n\n\n```{r}\nTopVariableTractsMeanDiff <- VariableTractsMeanDiff[abs(not_take_vs_take) > 0.30, unique(variable)]\n```\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\n# Enhanced contest-winning gt table with improved styling\nVariablesToUse[.(sub(\"_estimate(_(low|high))*\", \"\", TopVariableTractsMeanDiff) |> unique()), \n               on = \"name\",\n               c(\"dataset\",\n                 \"concept\",\n                 \"label\",\n                 \"name\")\n][order(concept, name)\n  # Add improved row groups with better categorization\n][, category := fcase(\n  grepl(\"Earnings|Income|Poverty\", concept, ignore.case = TRUE), \"ðŸ’° Economic Indicators\",\n  grepl(\"Industry|Occupation|Employment\", concept, ignore.case = TRUE), \"ðŸ­ Employment & Industry\", \n  grepl(\"Transportation|Commuting|Vehicle\", concept, ignore.case = TRUE), \"ðŸš— Transportation & Mobility\",\n  default = \"ðŸ“Š Other Variables\"\n)] |>\n\ngt(groupname_col = \"category\") |>\n\n# Enhanced header with better typography\ntab_header(\n  title = md(\"**ðŸ† Key Census Variables Analysis**\"),\n  subtitle = md(\"*Critical demographic and socioeconomic indicators across geographic clusters*\")\n) |>\n\n# Improved column headers with icons and better descriptions\ncols_label(\n  dataset = md(\"**ðŸ“‹ Dataset**\"),\n  concept = md(\"**ðŸŽ¯ Concept Category**\"),\n  label = md(\"**ðŸ“ Variable Description**\"),\n  name = md(\"**ðŸ”– Variable ID**\")\n) |>\n\n# Hide the grouping column\ncols_hide(category) |>\n\n# Enhanced header styling with gradient effect\ntab_style(\n  style = list(\n    cell_fill(color = ColorHighlight),\n    cell_text(color = \"white\", weight = \"bold\", size = px(13)),\n    cell_borders(sides = \"bottom\", color = \"white\", weight = px(2))\n  ),\n  locations = cells_column_labels()\n) |>\n\n# Stunning group headers with enhanced styling\ntab_style(\n  style = list(\n    cell_fill(color = ColorHighlightLow),\n    cell_text(\n      weight = \"bold\", \n      size = px(15),\n      color = \"#2c3e50\"\n    ),\n    cell_borders(\n      sides = c(\"top\", \"bottom\"), \n      color = ColorHighlight, \n      weight = px(2)\n    )\n  ),\n  locations = cells_row_groups()\n) |>\n\n# Enhanced title styling\ntab_style(\n  style = list(\n    cell_text(\n      color = \"white\", \n      weight = \"bold\",\n      size = px(22)\n    ),\n    cell_fill(color = \"#2c3e50\")\n  ),\n  locations = cells_title(groups = \"title\")\n) |>\n\ntab_style(\n  style = list(\n    cell_text(\n      color = \"#ecf0f1\", \n      style = \"italic\",\n      size = px(16)\n    ),\n    cell_fill(color = \"#34495e\")\n  ),\n  locations = cells_title(groups = \"subtitle\")\n) |>\n\n# Variable ID styling with better monospace presentation\ntab_style(\n  style = list(\n    cell_text(\n      font = \"Consolas, Monaco, monospace\", \n      size = px(11)\n    )\n  ),\n  locations = cells_body(columns = name)\n) |>\n\n# Dataset column styling\ntab_style(\n  style = cell_text(\n    weight = \"bold\",\n    size = px(11)\n  ),\n  locations = cells_body(columns = dataset)\n) |>\n\n# Concept column styling\ntab_style(\n  style = cell_text(\n    size = px(11),\n    weight = \"bold\"\n  ),\n  locations = cells_body(columns = concept)\n) |>\n\n# Label column with improved readability\ntab_style(\n  style = cell_text(\n    size = px(11),\n    color = \"#2c3e50\"\n  ),\n  locations = cells_body(columns = label)\n) |>\n\n# Optimized column widths for better content display\ncols_width(\n  dataset ~ px(100),\n  concept ~ px(180),\n  label ~ px(350),\n  name ~ px(140)\n) |>\n\n# Comprehensive table options for professional appearance\ntab_options(\n  # Typography\n  table.font.names = c(\"Segoe UI\", \"Arial\", \"sans-serif\"),\n  table.font.size = px(12),\n  heading.title.font.size = px(22),\n  heading.subtitle.font.size = px(16),\n  row_group.font.size = px(15),\n  \n  # Background and colors\n  table.background.color = \"#fdfdfd\",\n  \n  # Borders and lines\n  table.border.top.style = \"solid\",\n  table.border.top.width = px(4),\n  table.border.top.color = ColorHighlight,\n  table.border.bottom.style = \"solid\",\n  table.border.bottom.width = px(4),\n  table.border.bottom.color = ColorHighlight,\n  \n  # Column labels\n  column_labels.border.bottom.style = \"solid\",\n  column_labels.border.bottom.width = px(3),\n  column_labels.border.bottom.color = ColorHighlight,\n  column_labels.background.color = ColorHighlight,\n  \n  # Row groups\n  row_group.border.top.style = \"solid\",\n  row_group.border.top.width = px(2),\n  row_group.border.top.color = ColorHighlight,\n  row_group.border.bottom.style = \"solid\",\n  row_group.border.bottom.width = px(1),\n  row_group.border.bottom.color = ColorHighlightLow,\n  \n  # Striping and spacing\n  #row.striping.include_table_body = TRUE,\n  #row.striping.background_color = \"#f8f9fa\",\n  table_body.hlines.style = \"solid\",\n  table_body.hlines.width = px(1),\n  table_body.hlines.color = \"#dee2e6\",\n  \n  # Layout\n  # table.width = pct(100),\n  # container.overflow.x = TRUE,\n  # container.overflow.y = TRUE,\n  # container.height = px(600),\n  \n  quarto.disable_processing = TRUE\n) |>\n\n# Enhanced source note with better formatting\ntab_source_note(\n  source_note = md(\"**ðŸ“Š Data Source:** U.S. Census Bureau, American Community Survey (ACS) 5-Year Estimates | **ðŸ“… Analysis Period:** 2018-2022\")\n) |>\n\n# Style the source note\ntab_style(\n  style = list(\n    cell_text(\n      size = px(12),\n      color = \"#7f8c8d\",\n      style = \"italic\"\n    )\n  ),\n  locations = cells_source_notes()\n)\n```\n\nAfter confirming these promising results we can save the most import features for future use.\n\n```r\nfst::write_fst(\n  ConsolidatedVariableTractsByZoneId[, .SD, \n                                     .SDcols = c(\"LocationID\",\n                                                 TopVariableTractsMeanDiff)], \n  path = here(\"output/AcsVariablesByZoneId.fst\")\n)\n```\n\n## Conclusion\n\n","srcMarkdownNoYaml":"\n\nExpanding on the previous post's geospatial feature extraction, this article will delve into enriching our understanding of NYC taxi zones with socioeconomic and transportation-related data from the **US Census Bureau**. This new layer of information will provide crucial context for analyzing `take_current_trip` by revealing underlying community characteristics and commuting behaviors within each zone.\n\nHere are the steps to follow:\n\n1. **Importing training data with zone shapes**.\n\n1.  **Import Census data**:\n    * Identify relevant variables.\n    * Download data for Manhattan, Queens and Brooklyn with geometry.\n\n2.  **Spatially join ACS data to taxi zones**:\n    * Perform spatial joins to aggregate census tract data to the taxi zone level.\n    * Address overlapping and partially contained tracts.\n\n3.  **Extract new features**:\n    * Calculate proportions, averages, and densities of the ACS variables for each taxi zone.\n\n4.  **Validation**:\n    * Integrate new features with existing data and `take_current_trip`.\n    * Define the correlation between new predictors and `take_current_trip`.\n    \n    \n## Setting up the environment\n\n### Define colors to use\n\n```{r}\nBoroughColors <- c(\n  'Manhattan' = '#e41a1c',\n  'Queens' = '#377eb8',\n  'Brooklyn'= '#4daf4a'\n)\n\nBoroughSelected <- names(BoroughColors)\n\nColorHighlight <- \"lightslateblue\"\nColorHighlightLow <- \"#D3CDF7\"\nColorGray <- \"gray80\"\n```\n\n### Loading packages\n\n```{r}\n## To manage relative paths\nlibrary(here)\n\n## To transform data that fits in RAM\nlibrary(data.table)\n\n## To work with spatial data\nlibrary(sf)\n\n# To work and plot graphs\nlibrary(tidytext)\nlibrary(stringdist)\nlibrary(hashr)\nlibrary(igraph)\n\n# Access to US Census Bureau datasets\nlibrary(tidycensus)\n\n## To create general plots\nlibrary(ggplot2)\nlibrary(scales)\n\n## To create table\nlibrary(gt)\n\n## Custom functions\ndevtools::load_all()\n```\n\n:::{.callout-note title=\"Computational Performance\"}\nThe geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.\n:::\n\n## Importing training data with zone shapes\n\nDetails on how this data was obtained can be found in the **Data Collection Process**.\n\n```{r}\nZonesShapes <-\n  read_sf(here(\"raw-data/taxi_zones/taxi_zones.shp\")) |>\n  (\\(df) df[df[[\"borough\"]] %chin% BoroughSelected,\n            c(\"OBJECTID\", \"LocationID\")])() |>\n  st_cast(to = \"MULTIPOLYGON\")\n\nTrainingSample <-\n  here(\"output/take-trip-fst\") |>\n  list.files(full.names = TRUE) |>\n  (\\(x) data.table(full_path = x,\n                   n_char = nchar(basename(x)),\n                   name = basename(x)))() |>\n  (\\(dt) dt[order(n_char, name), full_path])() |>\n  head(12L) |>\n  lapply(FUN = fst::read_fst,\n         columns = c(\"trip_id\",\"PULocationID\", \"DOLocationID\", \"take_current_trip\"),\n         as.data.table = TRUE) |>\n  rbindlist()\n```\n\n## Import Census data\n\n### Identify relevant variables\n\nBased on the `tidycensus` documentation we have the below tables available for exploration.\n\n```{r}\nValidTables = c(\n  \"sf1\", \"sf2\", \"sf3\", \"sf4\", \"pl\", \"dhc\", \"dp\", \n  \"ddhca\", \"ddhcb\", \"sdhc\", \"as\", \"gu\", \"mp\", \n  \"vi\", \"acsse\", \"dpas\", \"dpgu\", \"dpmp\", \"dpvi\",\n  \"dhcvi\", \"dhcgu\", \"dhcvi\", \"dhcas\", \"acs1\", \n  \"acs3\", \"acs5\", \"acs1/profile\", \"acs3/profile\",\n  \"acs5/profile\", \"acs1/subject\", \"acs3/subject\",\n  \"acs5/subject\", \"acs1/cprofile\", \"acs5/cprofile\",\n  \"sf2profile\", \"sf3profile\", \"sf4profile\", \"aian\",\n  \"aianprofile\", \"cd110h\", \"cd110s\", \"cd110hprofile\",\n  \"cd110sprofile\", \"sldh\", \"slds\", \"sldhprofile\",\n  \"sldsprofile\", \"cqr\", \"cd113\", \"cd113profile\",\n  \"cd115\", \"cd115profile\", \"cd116\", \"plnat\", \"cd118\"\n)\n\nlength(ValidTables)\n```\n\nThat means that we have a lot a data to use, but to be able to get the list of variables available for each table we need to know the different year when each table was updated. In our particular case, we only know  the **last year when each table was updated from 2020 to 2020**.\n\nTo solve that question, we only need to scrap the table available in [https://api.census.gov/data.html](https://api.census.gov/data.html) and apply our particular selection to that huge table. \n\n```{r}\n#| echo: false\n#| output: false\n\nValidTablesYearsFilePath = here(\"output/cache-data/08-expanding-transportation-socioeconomic/ValidTablesYears.fst\")\n\n\nif(file.exists(ValidTablesYearsFilePath)) {\n  ValidTablesYears <- \n    fst::read_fst(ValidTablesYearsFilePath, as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nCensusTables <-\n  rvest::read_html(\"https://api.census.gov/data.html\") |>\n  rvest::html_table() |>\n  (\\(x) x[[1L]])() |>\n  as.data.table() |>\n  subset(!is.na(`API Base URL`)\n         & Vintage != \"N/A\",\n         select = c(\"Title\",\n                    \"Description\",\n                    \"Vintage\",\n                    \"Dataset Type\",\n                    \"Dataset Name\",\n                    \"API Base URL\"))\n\nCensusTables[,`:=`(Vintage = as.integer(Vintage),\n                   `Dataset Name` = gsub(\"â€º \", \"/\", `Dataset Name`))]\n\nCensusTables[, `:=`(survey = fcase(`Dataset Name` %like% \"^dec\",\n                                   \"dec\",\n                                   `Dataset Name` %like% \"^acs\",\n                                   \"acs\"),\n                    `Dataset Name` = gsub(\"(dec|acs)/\", \"\", `Dataset Name`))]\n\nsetorder(CensusTables, -Vintage)\n\nValidTablesYears <-\n  CensusTables[\n    Vintage %between% c(2020, 2022)\n  ][ValidTables,\n    on = \"Dataset Name\",\n    nomatch = NULL,\n    unique(.SD, by = \"Dataset Name\"),\n    .SDcols = c(\"Vintage\", \"Dataset Name\", \"survey\")]\n\nValidTablesYears[, head(.SD, 5),\n                 by = \"survey\"]\n\n```\n\n```{r}\n#| echo: false\n\nValidTablesYears[, head(.SD, 5),\n                 by = \"survey\"]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ValidTablesYears, ValidTablesYearsFilePath)\n```\n\nNow that we have the years related to each table, let's apply `tidycensus::load_variables` for each table and year to see the variables to explore. \n\n```{r}\n#| echo: false\n#| output: false\n\nValidTablesLastYearFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/ValidTablesLastYear.fst\")\n\n\nif(file.exists(ValidTablesLastYearFilePath)) {\n  ValidTablesLastYear <- \n    fst::read_fst(ValidTablesLastYearFilePath, as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nget_all_data = function(x, ref_df){\n  \n  imported_data =\n    load_variables(ref_df$Vintage[x],\n                   ref_df$`Dataset Name`[x])\n  \n  setDT(imported_data)\n  \n  imported_data[, `:=`(year = ref_df$Vintage[x],\n                       dataset = ref_df$`Dataset Name`[x],\n                       survey = ref_df$survey[x])]\n  \n  return(imported_data)\n  \n} \n  \nValidTablesLastYear <-\n  lapply(seq_along(ValidTablesYears$`Dataset Name`), \n         get_all_data,\n         ref_df = ValidTablesYears)|>\n  rbindlist(use.names = TRUE, fill = TRUE)\n\nnrow(ValidTablesLastYear) |> comma()\n```\n\n```{r}\n#| echo: false\n\nnrow(ValidTablesLastYear) |> comma()\n```\n\nAfter making that exploration we can see that we have almost **160K variables** to explore.\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ValidTablesLastYear, ValidTablesLastYearFilePath)\n```\n\nTo understand what have we have let's plot the number of variables by dataset and survey for each year.\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| fig-height: 7.5\n\nVariablesPerDataset <-\n  ValidTablesLastYear[, .N, \n                      .(year = factor(year), \n                        dataset,\n                        survey)\n  ][, dataset := reorder(dataset, N, FUN = sum)] \n\nggplot(VariablesPerDataset) +\n  geom_col(aes(N, dataset, fill = year),\n           color = \"black\",\n           linewidth = 0.2) +\n  scale_fill_manual(values = c(\"2022\" = ColorHighlight,\n                               \"2020\" = ColorGray)) +\n  scale_x_continuous(labels = scales::label_comma(scale = 1/1000, suffix = \"k\"))+\n  expand_limits(x = 1e4) +\n  labs(x = \"Number of variables\",\n       fill = \"Last update year\")+\n  facet_wrap(vars(survey), scale = \"free\") +\n  theme_minimal()+\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.y = element_blank())\n```\n\nIt's clear that **there is one way we can read the variables one by one** and hypothesize the variables to select. Instead, we are going to flow some steps to reduce the number of variable and the classify them in to groups to select.\n\nJust by looking to the raw data, I found that many variables measure the same thing for particular groups based on:\n\n- Race\n- Place of Birth\n- Sex\n\n```{r}\nVariablesToIgnore <- c(\n  \"Hispanic\",\n  \"Latino\",\n  \"American Indian\",\n  \"Alaska Native\",\n  \"White\",\n  \"Black\",\n  \"African American\",\n  \"Asian\",\n  \"Native Hawaiian\",\n  \"Pacific Islander\",\n  \"Races?\",\n  \n  \"Ancestry\",\n  \"Nativity\",\n  \"Citizenship Status\",\n  \"Place of Birth\",\n  \"Naturalization\",\n  \"Current Residence\",\n  \n  \"Male\",\n  \"Female\",\n  \"Women\",\n  \"Men\"\n)\n```\n\nAs we will join the data based on geography it doesn't make sense to have predictors based on the situations of those particular groups as we won't have does details of passengers of the trips.\n\n\n```{r}\nConceptsToExplore <-\n  ValidTablesLastYear[!concept %ilike% paste(VariablesToIgnore, collapse = \"|\")\n                      & !label %ilike% \"Male|Female\",\n                      .(concept = unique(concept))]\n\nnrow(ConceptsToExplore)\n```\n\nAnother aspect we found out by exploring the data it's that some variables names are really long and the long the concept definition the more specific the variable and less useful for our model. For example, below we can see the variable with the longer number of characters:\n\n```{r}\n#| echo: false\n#| output: asis\n\nConceptsToExplore[nchar(concept) == max(nchar(concept)), paste0(\"> \", concept)] |> cat()\n```\n\n- Population Measured: \n  - Grandparents living with own grandchildren under 18 years\n  - 30 years and over\n  - In households (excluding military housing units)\n\n- Data Categories:\n  - By responsibility for own grandchildren\n  - By length of time responsible for grandchildren\n\nBy exploring the cumulative distribution we can see that near 50% of the variable present less **70 characters** per variable which seems the better group to focus the exploration\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nConceptsToExplore[, concept_length := nchar(concept)]\n\nggplot(ConceptsToExplore)+\n  stat_ecdf(aes(concept_length)) +\n  scale_x_continuous(breaks = breaks_width(10))+\n  scale_y_continuous(labels = label_percent(accuracy = 1))+\n  labs(title = \"Empirical Cumulative Distribution\",\n       subtitle = \"Number of Character per Variable\",\n       x = \"Number of Characters\",\n       y = \"Cumulative Proportion\") +\n  theme_minimal()+\n  theme(panel.grid.minor = element_blank())\n```\n\nAfter applying that change we only have 675 pending variables to explore the classify.\n\n```{r}\nConceptsToExploreMaxConceptLength <- 70\n\nConceptsToExploreValidConceptLength <- \n  ConceptsToExplore[concept_length <= ConceptsToExploreMaxConceptLength]\n\nConceptsToExploreValidConceptLength[, .N]\n```\n\nTo confirm if that action make sense we can print the longer variables and confirm that aren't too specific.\n\n```{r}\n#| output: asis\n#| echo: false\n\nConceptsToExploreValidConceptLength[concept_length == max(concept_length), \n                                    paste0(\"- `\", concept, \"`\")] |>\n  head() |>\n  cat(sep = \"\\n\")\n```\n\nNow we can calculate the **pairwise Jaccard string distances** of each of the variables to identify conceptually similar terms based on similar words to then identify variable clusters to select.\n\n1. Create word-level hash tokens (not character-level) after case normalization.\n\n```{r}\nConceptsToExploreHash <- \n  tolower(ConceptsToExploreValidConceptLength$concept) |>\n  strsplit(\"\\\\s+\") |> \n  hash()\n```\n\n2. Compute pairwise Jaccard distances between hashed word representations.\n\n```{r}\nConceptsToExploreWordDistance <-\n  seq_distmatrix(ConceptsToExploreHash, \n                 ConceptsToExploreHash, \n                 method = \"jaccard\", \n                 q = 1)\n```\n\n3. Assign concept names to matrix dimensions for interpretability.\n\n```{r}\nrownames(ConceptsToExploreWordDistance) <- ConceptsToExploreValidConceptLength$concept\ncolnames(ConceptsToExploreWordDistance) <- ConceptsToExploreValidConceptLength$concept\n```\n\n4. Transform symmetric matrix to long format, eliminating redundant pairwise comparisons.\n\n```{r}\nConceptsToExploreWordDistance[upper.tri(ConceptsToExploreWordDistance, diag = TRUE)] <- NA\n\nConceptsToExploreWordDistanceDt <-\n  as.data.table(ConceptsToExploreWordDistance,\n                keep.rownames = \"Variable Name 1\") |>\n  melt(id.vars = \"Variable Name 1\",\n       variable.name = \"Variable Name 2\",\n       variable.factor = FALSE,\n       value.name = \"Jaccard Distance\",\n       na.rm = TRUE)\n```\n\n\nWith this inter-concept distance we can use the `edge_betweenness` argoritm to define the cluster to use. But as the result for this method will change depending the min Jaccard Distance selected to do the analysis, we need to explore several options before taking a final decision.\n\n```{r}\n#| echo: false\n#| output: false\n\nConceptsToExploreByDistanceFilePath <-\n  here(\"output/cache-data/08-expanding-transportation-socioeconomic/ConceptsToExploreByDistance.fst\")\n\nif(file.exists(ConceptsToExploreByDistanceFilePath)) {\n  ConceptsToExploreByDistance <-\n    fst::read_fst(ConceptsToExploreByDistanceFilePath, as.data.table = TRUE)\n}\n```\n\n\n```{r}\n#| eval: false\n\ndefine_clusters_by_distance <- function(min_jaccard_distance,\n                                        dt){\n  \n  dt_filtered = dt[`Jaccard Distance` <= min_jaccard_distance]\n  \n  temp_graph =\n    graph_from_data_frame(dt_filtered, directed = FALSE)\n  \n  temp_cluster = cluster_edge_betweenness(temp_graph)\n  \n  node_results = data.table(\n    node = V(temp_graph)$name,\n    cluster = temp_cluster$membership,\n    `Jaccard Distance Threshold` = min_jaccard_distance,\n    modularity = max(temp_cluster$modularity),\n    n_clusters = max(temp_cluster$membership)\n  )\n  \n  return(node_results)\n  \n}\n\n\nConceptsToExploreByDistance <-\n  lapply(seq(0.1, 0.8, by = 0.10),\n         FUN = define_clusters_by_distance,\n         dt = ConceptsToExploreWordDistanceDt) |>\n  rbindlist()\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ConceptsToExploreByDistance, ConceptsToExploreByDistanceFilePath)\n```\n\nAfter exploring several configurations we can see that that the as we expand the limit of Jaccard Distance the modularity decrease, but the number of clusters initially increases (peaking around 0.3) and then steadily decreases until 0.7, after which it increases sharply again.\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nConceptsToExploreByDistance[, unique(.SD),\n                            .SDcols = c(\"Jaccard Distance Threshold\",\n                                        \"modularity\",\n                                        \"n_clusters\")] |>\n  melt(id.vars = \"Jaccard Distance Threshold\",\n       variable.name = \"cluster_metric\") |>\n  ggplot(aes(`Jaccard Distance Threshold`, value)) +\n  geom_line(linewidth = 1,\n            color = \"gray40\") +\n  geom_point(size = 3,\n             color = \"gray40\") +\n  geom_vline(xintercept = 0.5,\n             linetype = \"dashed\",\n             color = ColorHighlight,\n             linewidth = 0.9) +\n  # Add the annotation here\n  annotate(\"text\", x = 0.5, y = Inf, label = \"0.5 Threshold\",\n           color = ColorHighlight, vjust = 1.2, hjust = -0.05, size = 4) +\n  scale_y_continuous(labels = \\(x) fifelse(x <= 1 & x > 0,\n                                            label_percent()(x),\n                                            label_comma(accuracy = 1)(x)),\n                     breaks = breaks_pretty(7)) +\n  scale_x_continuous(breaks = breaks_width(0.10)) +\n  expand_limits(y = 0) +\n  facet_wrap(vars(cluster_metric),\n             scale = \"free_y\",\n             ncol = 1L,\n             strip.position = \"top\") +\n  labs(\n    title = \"Modularity and Number of Clusters vs. Jaccard Distance Threshold\",\n    subtitle = \"Assessing the Trade-off: High Modularity and Low Number of Clusters\",\n    x = \"Jaccard Distance Threshold\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"),\n        plot.subtitle = element_text(color = \"gray25\"),\n        strip.text = element_text(size = 12, face = \"bold\"),\n        axis.text = element_text(size = 10))\n```\n\nBased on this information **the best threshold is 0.5 Jaccard Distance** as the modularity is higher than 0.8 and the algorithm found less than 60 clusters. After than point the slope to reduce the number of clusters gets lower as concurrence, we don't see worthy to keep increasing the threshold.\n\n```{r}\nConceptsToExploreClusters <-\n  ConceptsToExploreByDistance[.(0.5),\n                              on = \"Jaccard Distance Threshold\"]\n```\n\n\nTo understand what kind of variables we have in each cluster lets use the **statistic tf-idf** to list the **top 5 words** (after removing stop words) of each cluster and use that reference to select the must meaningful ones for this context.\n\n```{r}\nCustomStopWords <- c(\n  stop_words[stop_words$lexicon == \"snowball\", c(\"word\")][[1]],\n  \"estimate\",\n  \"estimates\",\n  \"percent\",\n  \"months\",\n  \"past\",\n  \"detailed\",\n  \"type\",\n  \"types\",\n  \"current\",\n  \"adjusted\",\n  \"united\",\n  \"round\",\n  \"selected\",\n  \"median\",\n  \"level\",\n   NA_character_\n)\n\nTopWordsByCluster <-\n  ConceptsToExploreClusters[, .(cluster,\n                                concept_lower = tolower(node))\n  ][, unnest_tokens(.SD, word, concept_lower)\n  ][!word %chin% CustomStopWords\n    & !word %like% \"^\\\\d+$\",\n    .N,\n    by = c(\"cluster\", \"word\")\n  ][, dataset_total := sum(N), \n    by = \"cluster\"\n  ][, bind_tf_idf(.SD, word, cluster, N)\n  ][order(-tf_idf)\n  ][, .(`Top Words` =\n          head(word, 5L) |> \n          stringr::str_to_sentence() |>\n          paste0(collapse = \", \")),\n    keyby = \"cluster\"\n  ][ConceptsToExploreClusters,\n    on = \"cluster\",\n    j = .(`Node Count` = .N), \n    by = c(\"Top Words\", \"cluster\")\n  ][order(- `Node Count`, cluster),\n    .(`Cluster ID` = cluster, `Top Words`, `Node Count`)]\n```\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ngt(TopWordsByCluster) |>\n  # Apply visual styling\n  tab_header(\n    title = \"Key Variable Clusters from Census Data Analysis\",\n    subtitle = \"Top representative words and cluster sizes for socioeconomic & transportation patterns\"\n  ) |>\n  tab_style(\n    style = cell_text(color = \"white\", weight = \"bold\"),\n    locations = cells_title()\n  ) |>\n  tab_style(\n    style = cell_fill(color = \"#2c3e50\"),\n    locations = cells_title()\n  ) |>\n  data_color(\n    columns = `Node Count`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", ColorHighlightLow, ColorHighlight),\n      domain = c(0, max(TopWordsByCluster$`Node Count`))\n    )\n  )|>\n  fmt_number(\n    columns = `Node Count`,\n    decimals = 0\n  ) |>\n  cols_align(\n    align = \"left\",\n    columns = `Top Words`\n  ) |>\n  cols_label(\n    `Cluster ID` = \"CLUSTER\",\n    `Top Words` = \"TOP REPRESENTATIVE TERMS\",\n    `Node Count` = \"# VARIABLES\"\n  ) |>\n  tab_options(\n    table.font.names = \"Arial\",\n    table.background.color = \"white\",\n    column_labels.font.weight = \"bold\",\n    column_labels.background.color = \"#f8f9fa\",\n    row_group.background.color = \"#f1f3f5\",\n    #row.striping.include_table_body = TRUE,\n    quarto.disable_processing = TRUE\n  ) |>\n  tab_source_note(\n    source_note = \"Clusters identified through NLP analysis of US Census variable concepts\"\n  )\n```\n\nBased on the analysis, the following clusters are selected for inclusion due to their direct relevance to transportation behavior, income patterns, and population density - key factors influencing taxi demand (`take_current_trip`):\n\n| Cluster ID | Top Words                          | Node Count | Reason for Inclusion                                                                 |\n|------------|------------------------------------|------------|--------------------------------------------------------------------------------------|\n| **18**     | Work, Transportation, Means, Time, Workplace | 32    | **Direct commuting behavior** - primary driver of taxi demand patterns          |\n| **13**     | Population, Years, Civilian, Employed, Age | 32      | **Employment density** - correlates with business/event-related trips           |\n| **36**     | Households, Income, Earnings, Dollars, Security | 19 | **Income capacity** - strongest predictor of taxi affordability and usage frequency|\n\n\n```{r}\nConceptsToExploreCleanConcept <-\n  ConceptsToExploreClusters[.(c(18, 13, 36)),\n                            on = \"cluster\",\n                            .(concept = stringr::str_to_sentence(node),\n                              cluster)]\n\nConceptByDataset <-\n  ValidTablesLastYear[, .(concept = stringr::str_to_sentence(concept),\n                          year,\n                          dataset,\n                          survey)\n  ][, unique(.SD)]\n\n\nConceptByDatasetToExplore <-\n  ConceptByDataset[ConceptsToExploreCleanConcept,\n                   on = \"concept\"]\n```\n\n\nChecking the data we can see that some concept are duplicated in some in some datasets, as we can see in the next table.\n\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nConceptByDatasetToExplore[, .(N = .N, \n                              dataset = sort(dataset) |> paste(collapse = \", \")), \n                          by = \"concept\"\n][N > 1, \n  .(variables =  .N), \n  keyby = \"dataset\"] |>\n  \n  \n  gt() |>\n  \n  # Header\n  tab_header(\n    title = md(\"**Dataset Variable Analysis**\"),\n    subtitle = \"Distribution of shared variables across survey datasets\"\n  ) |>\n  \n  # Col Names\n  cols_label(\n    dataset = \"Dataset Collection\",\n    variables = \"Number of Variables\"\n  ) |>\n  \n  # Number Format\n  fmt_number(columns = variables, decimals = 0) |>\n  \n  # Colors\n  data_color(\n    columns = variables,\n    colors = scales::col_numeric(\n      palette = c(\"white\",ColorHighlightLow, ColorHighlight),\n      domain = NULL\n    )\n  ) |>\n  tab_style(\n    style = cell_text(color = \"white\", weight = \"bold\"),\n    locations = cells_title()\n  ) |>\n  tab_style(\n    style = cell_fill(color = \"#2c3e50\"),\n    locations = cells_title()\n  ) |>\n  \n  # Options\n  tab_options(\n    table.font.names = \"Arial\",\n    table.background.color = \"white\",\n    column_labels.font.weight = \"bold\",\n    column_labels.background.color = \"#f8f9fa\",\n    row_group.background.color = \"#f1f3f5\",\n    #row.striping.include_table_body = TRUE,\n    quarto.disable_processing = TRUE\n  )\n```\n\n\nWe can see that we have 2 main groups of cases:\n\n- Variables shared between acs1 (subject) and acs5 (subject).\n- Variables shared between datasets of the decennial US Census.\n\nBy reading about the differences between each datasets, we are going to focus in the variables available for the next datasets:\n\n- **acs5** and **acs5/subject**\n\n    - We are going to train the model based on one year data but we want to be able to find patterns that remain for future years as our testing data have trips for 2025 we expect that the model keep working after that year.\n    \n\n- **dhc**\n\n    - We are exploring only NYC so won't need information from indepent island and as result the information provided by those datasets won't be relevant for our training goal.\n\n\n```{r}\nConceptByValidDatasetToExplore <-\n  ConceptByDatasetToExplore[c(\"acs5\", \"acs5/subject\", \"dhc\"),\n                            on = \"dataset\",\n                            .(dataset, concept, cluster)]\n\n```\n\nNow we are ready to identify the particular codes (names) related to each variable concept. In this step are going to exclude variables that:\n\n- Only provide information based on sex with the exception of `B08013_001` which provide information about \"travel time to work (in minutes) of workers\".\n- Don't have information at tract level of geography.\n\n```{r}\nValidTablesLastYear[, concept := stringr::str_to_sentence(concept)]\n\nVariablesToUse <-\n  ValidTablesLastYear[\n    ConceptByValidDatasetToExplore,\n    on = c(\"dataset\",\"concept\")\n    \n  # Only provide information based on sex\n  ][!label %ilike% \"Male|Female\"\n  ][, concept_count := .N, \n    by = \"concept\"\n  ][name == \"B08013_001\"\n    | concept_count > 2L\n    \n  # Don't have information at tract \n  ][fcoalesce(geography, \"\") != \"county\"]\n\nVariablesToUse[, .(N = comma(.N)), \n               by = c(\"survey\", \"year\")]\n```\n\n### Download data for Manhattan, Queens and Brooklyn with geometry\n\nAfter defining the variables to use we only need to define the queries to get the data from the API with `{tidycensus}` for the American Community Survey (ACS) and Decennial US Census (DEC).\n\n::: {.callout-note}\nAs we are getting also the geometry information is much efficient to change the `output` argument default from \"tidy\" to \"wide\" to avoid having a list column repeating the same geometries over and over after applying the function `tidyr::gather`.\n\n**This will make the spatial join independent to the number of variables to query.**\n:::\n\n```{r}\n#| echo: false\n#| output: false\n\nAcsVariableTractsFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/AcsVariableTracts.qs\")\n\nif(file.exists(ValidTablesLastYearFilePath)) {\n  AcsVariableTracts <- qs2::qs_read(AcsVariableTractsFilePath)\n}\n\nDecVariableTractsFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/DecVariableTracts.qs\")\n\nif(file.exists(ValidTablesLastYearFilePath)) {\n  DecVariableTracts <- qs2::qs_read(DecVariableTractsFilePath)\n}\n```\n\n```{r}\n#| eval: false\n\nAcsVariableTracts <-\n  get_acs(geography = \"tract\", \n          variables = VariablesToUse[\"acs\", on = \"survey\", name],\n          year = 2022, \n          state = \"NY\",\n          # Manhattan, Brooklyn, Queens\n          county =  c(\"New York\", \"Kings\", \"Queens\"),\n          geometry = TRUE,\n          survey = \"acs5\",\n          output = \"wide\") |>\n  st_transform(crs = st_crs(ZonesShapes)$epsg)\n\nDecVariableTracts <-\n  get_decennial(geography = \"tract\", \n                variables = VariablesToUse[\"dec\", on = \"survey\", name],\n                sumfile = \"dhc\",\n                year = 2020, \n                state = \"NY\",\n                # Manhattan, Brooklyn, Queens\n                county =  c(\"New York\", \"Kings\", \"Queens\"),\n                geometry = TRUE,\n                output = \"wide\") |>\n  st_transform(crs = st_crs(ZonesShapes)$epsg)\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nqs2::qs_save(AcsVariableTracts, AcsVariableTractsFilePath)\n\nqs2::qs_save(DecVariableTracts, DecVariableTractsFilePath)\n```\n\nTo it's the perfect time to join all variables into one table.\n\n```{r}\nConsolidatedVariableTracts <- \n  dplyr::full_join(AcsVariableTracts, \n                   st_drop_geometry(DecVariableTracts), \n                   by = c(\"GEOID\", \"NAME\"))\n\ndim(ConsolidatedVariableTracts)\ndim(AcsVariableTracts)\ndim(DecVariableTracts)\n```\n\n\n## Spatial joins\n\nWe need to join to the original zones of our training set in to 2 steps:\n\n- Join the tracts full contained within the original zone shapes.\n- Join the tracts that overlaps in more than one zone shape.\n\n```{r}\n#| eval: false\n\nConsolidatedVariableTractsJoinedList <-\n  lapply(list(within = st_within,\n              overlaps = st_overlaps), \n         FUN = st_join,\n         x = ConsolidatedVariableTracts,\n         y = ZonesShapes,\n         left = FALSE)\n```\n\nNow we need to go over each of the original zone shapes and cut the tracts parts that are not contained on each zone and consolidate the within and overlaps elements of `TravelTimeToWorkTractJoinedList` into one data frame.\n\n```{r}\n#| echo: false\n#| output: false\n\nConsolidatedVariableTractsJoinedFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/ConsolidatedVariableTractsJoined.qs\")\n\n\nif(file.exists(ConsolidatedVariableTractsJoinedFilePath)) {\n  ConsolidatedVariableTractsJoined <-\n    qs2::qs_read(ConsolidatedVariableTractsJoinedFilePath)\n}\n```\n\n```{r}\n#| eval: false\n\napply_intersection_by_id <- function(features,\n                                     shapes,\n                                     id_col){\n  \n  intersection_by_id = function(id, features, shapes, id_col) {\n    \n    features_sub = features[features[[id_col]] == id, ]\n    shapes_sub_geo = shapes[shapes[[id_col]] == id, ] |> st_geometry()\n    \n    new_df = \n      st_intersection(features_sub,\n                      shapes_sub_geo) |>\n      st_cast(to = \"MULTIPOLYGON\")\n\n    return(new_df)\n  }\n  \n  new_df =\n    lapply(unique(features[[id_col]]),\n           FUN = intersection_by_id,\n           features = features,\n           shapes = shapes,\n           id_col = id_col) |>\n    do.call(what = \"rbind\")\n  \n  return(new_df)\n  \n}\n\nConsolidatedVariableTractsJoinedList$overlaps <-\n  apply_intersection_by_id(ConsolidatedVariableTractsJoinedList$overlaps,\n                           ZonesShapes,\n                           id_col = \"OBJECTID\")\n\nConsolidatedVariableTractsJoined <- \n  do.call(what = \"rbind\", \n          ConsolidatedVariableTractsJoinedList)\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nqs2::qs_save(ConsolidatedVariableTractsJoined, ConsolidatedVariableTractsJoinedFilePath)\n```\n\nPerfect, we have joined the to shapes, but we still need to have only one estimate value per original zone shape. To solve this challenge we only need to use the `weighted arithmetic mean` based on the proportion of the area add each tract to the original zone id.\n\n```{r}\n#| echo: false\n#| output: false\n\nConsolidatedVariableTractsByZoneIdFilePath <- here(\"output/cache-data/08-expanding-transportation-socioeconomic/ConsolidatedVariableTractsByZoneId.fst\")\n\n\nif(file.exists(ConsolidatedVariableTractsByZoneIdFilePath)) {\n  ConsolidatedVariableTractsByZoneId <-\n    fst::read_fst(ConsolidatedVariableTractsByZoneIdFilePath,\n                  as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nConsolidatedVariableTractsJoined$tract_area <-\n  st_area(ConsolidatedVariableTractsJoined$geometry)\n\nConsolidatedVariableTractsJoinedDt <-\n  st_drop_geometry(ConsolidatedVariableTractsJoined) |>\n  setDT()\n\nIdColumns <-\n  c(\"GEOID\", \n    \"NAME\", \n    \"tract_area\",\n    \"LocationID\", \n    \"OBJECTID\")\n\nConsolidatedVariableTidy <-\n  melt(ConsolidatedVariableTractsJoinedDt,\n       id.var = IdColumns,\n       variable.factor = FALSE\n  )[,`:=`(variable_type = fifelse(variable %like% \"\\\\dM$\", \"moe\", \"estimate\"),\n          variable = sub(\"(E|M)$\", \"\", variable))\n  ][, dcast(.SD,\n            formula = as.formula(paste0(paste0(c(IdColumns, \"variable\"), collapse = \" + \"), \"~ variable_type\")),\n            value.var = \"value\")\n  ][, `:=`(NAME = NULL,\n           OBJECTID = NULL,\n           moe = NULL,\n           estimate_low = estimate - fcoalesce(moe, 0),\n           estimate_high = estimate + fcoalesce(moe, 0))\n  ][estimate_low < 0, \n    estimate_low := 0\n  ][, area_prop :=  as.double(tract_area / sum(tract_area, na.rm = TRUE)),\n    by = c(\"LocationID\", \"variable\")]\n\n\nConsolidatedVariableTractsByZoneId <-\n  ConsolidatedVariableTidy[\n    j = .(estimate  = sum(estimate * area_prop, na.rm = TRUE),\n          estimate_low  = sum(estimate_low * area_prop, na.rm = TRUE),\n          estimate_high  = sum(estimate_high * area_prop, na.rm = TRUE)),\n    by = c(\"LocationID\", \"variable\")\n  ][, melt(.SD,\n           measure.vars = c(\"estimate\", \"estimate_low\", \"estimate_high\"),\n           variable.factor = FALSE,\n           variable.name = \"variable_type\")\n  ][, dcast(.SD, LocationID ~ variable + variable_type, value.var = \"value\")]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(ConsolidatedVariableTractsByZoneId, ConsolidatedVariableTractsByZoneIdFilePath)\n```\n\n\n```{r}\ndim(ConsolidatedVariableTractsByZoneId)\n```\n\n\n## Validation\n\n```{r}\n#| echo: false\n#| output: false\n\nPuVariableTractsMeanDiffFilePath <-\n  here(\"output/cache-data/08-expanding-transportation-socioeconomic/PuVariableTractsMeanDiff.fst\")\n\nDoVariableTractsMeanDiffFilePath <-\n  here(\"output/cache-data/08-expanding-transportation-socioeconomic/DoVariableTractsMeanDiff.fst\")\n\n\nif(file.exists(PuVariableTractsMeanDiffFilePath)) {\n  PuVariableTractsMeanDiff <-\n    fst::read_fst(PuVariableTractsMeanDiffFilePath,\n                  as.data.table = TRUE)\n}\n\nif(file.exists(DoVariableTractsMeanDiffFilePath)) {\n  DoVariableTractsMeanDiff <-\n    fst::read_fst(DoVariableTractsMeanDiffFilePath,\n                  as.data.table = TRUE)\n}\n```\n\n```{r}\n#| eval: false\n\nPuVariableTractsMeanDiff <-\n  ConsolidatedVariableTractsByZoneId[TrainingSample[, .(LocationID = PULocationID, \n                                                        take_current_trip)], \n                                     on = \"LocationID\"\n  ][, lapply(.SD, \\(x) if(uniqueN(x) == 1L) x[1L] else x[!is.na(x)] |> scale(center = FALSE) |> median()),\n    .SDcols = !c(\"LocationID\"),\n    by = \"take_current_trip\"\n  ][, melt(.SD,\n           id.vars = \"take_current_trip\",\n           variable.factor = FALSE)\n  ][, .(not_take_vs_take = diff(value)),\n    by = \"variable\"\n  ][order(-abs(not_take_vs_take))]\n\n\nDoVariableTractsMeanDiff <-\n  ConsolidatedVariableTractsByZoneId[TrainingSample[, .(LocationID = DOLocationID,\n                                                        take_current_trip)], \n                                     on = \"LocationID\"\n  ][, lapply(.SD, \\(x) if(uniqueN(x) == 1L) x[1L] else x[!is.na(x)] |> scale(center = FALSE) |> median()),\n    .SDcols = !c(\"LocationID\"),\n    by = \"take_current_trip\"\n  ][, melt(.SD,\n           id.vars = \"take_current_trip\",\n           variable.factor = FALSE)\n  ][, .(not_take_vs_take = diff(value)),\n    by = \"variable\"\n  ][order(-abs(not_take_vs_take))]\n```\n\n```{r}\n#| eval: false\n#| echo: false\n#| output: false\n\nfst::write_fst(PuVariableTractsMeanDiff, PuVariableTractsMeanDiffFilePath)\nfst::write_fst(DoVariableTractsMeanDiff, DoVariableTractsMeanDiffFilePath)\n```\n\n```{r}\nVariableTractsMeanDiff <-\n  rbindlist(list(PuVariableTractsMeanDiff,\n                 DoVariableTractsMeanDiff)\n  )[!is.na(not_take_vs_take)]\n```\n\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nggplot(VariableTractsMeanDiff) +\n  geom_histogram(aes(not_take_vs_take)) +\n  scale_y_continuous(transform = scales::new_transform(\"signed_log10\",\n                                                       transform = function(x) sign(x) * log10(1 + abs(x)),\n                                                       inverse = function(x) sign(x) * (10^(abs(x)) - 1),\n                                                       breaks = scales::pretty_breaks()),\n                     breaks = c(0, 10^(1:4))) +\n  scale_x_continuous(breaks = breaks_width(0.1)) +\n  labs(y = \"count on log10 scale\")+\n  theme_minimal()\n```\n\n\n```{r}\nTopVariableTractsMeanDiff <- VariableTractsMeanDiff[abs(not_take_vs_take) > 0.30, unique(variable)]\n```\n\n```{r}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\n# Enhanced contest-winning gt table with improved styling\nVariablesToUse[.(sub(\"_estimate(_(low|high))*\", \"\", TopVariableTractsMeanDiff) |> unique()), \n               on = \"name\",\n               c(\"dataset\",\n                 \"concept\",\n                 \"label\",\n                 \"name\")\n][order(concept, name)\n  # Add improved row groups with better categorization\n][, category := fcase(\n  grepl(\"Earnings|Income|Poverty\", concept, ignore.case = TRUE), \"ðŸ’° Economic Indicators\",\n  grepl(\"Industry|Occupation|Employment\", concept, ignore.case = TRUE), \"ðŸ­ Employment & Industry\", \n  grepl(\"Transportation|Commuting|Vehicle\", concept, ignore.case = TRUE), \"ðŸš— Transportation & Mobility\",\n  default = \"ðŸ“Š Other Variables\"\n)] |>\n\ngt(groupname_col = \"category\") |>\n\n# Enhanced header with better typography\ntab_header(\n  title = md(\"**ðŸ† Key Census Variables Analysis**\"),\n  subtitle = md(\"*Critical demographic and socioeconomic indicators across geographic clusters*\")\n) |>\n\n# Improved column headers with icons and better descriptions\ncols_label(\n  dataset = md(\"**ðŸ“‹ Dataset**\"),\n  concept = md(\"**ðŸŽ¯ Concept Category**\"),\n  label = md(\"**ðŸ“ Variable Description**\"),\n  name = md(\"**ðŸ”– Variable ID**\")\n) |>\n\n# Hide the grouping column\ncols_hide(category) |>\n\n# Enhanced header styling with gradient effect\ntab_style(\n  style = list(\n    cell_fill(color = ColorHighlight),\n    cell_text(color = \"white\", weight = \"bold\", size = px(13)),\n    cell_borders(sides = \"bottom\", color = \"white\", weight = px(2))\n  ),\n  locations = cells_column_labels()\n) |>\n\n# Stunning group headers with enhanced styling\ntab_style(\n  style = list(\n    cell_fill(color = ColorHighlightLow),\n    cell_text(\n      weight = \"bold\", \n      size = px(15),\n      color = \"#2c3e50\"\n    ),\n    cell_borders(\n      sides = c(\"top\", \"bottom\"), \n      color = ColorHighlight, \n      weight = px(2)\n    )\n  ),\n  locations = cells_row_groups()\n) |>\n\n# Enhanced title styling\ntab_style(\n  style = list(\n    cell_text(\n      color = \"white\", \n      weight = \"bold\",\n      size = px(22)\n    ),\n    cell_fill(color = \"#2c3e50\")\n  ),\n  locations = cells_title(groups = \"title\")\n) |>\n\ntab_style(\n  style = list(\n    cell_text(\n      color = \"#ecf0f1\", \n      style = \"italic\",\n      size = px(16)\n    ),\n    cell_fill(color = \"#34495e\")\n  ),\n  locations = cells_title(groups = \"subtitle\")\n) |>\n\n# Variable ID styling with better monospace presentation\ntab_style(\n  style = list(\n    cell_text(\n      font = \"Consolas, Monaco, monospace\", \n      size = px(11)\n    )\n  ),\n  locations = cells_body(columns = name)\n) |>\n\n# Dataset column styling\ntab_style(\n  style = cell_text(\n    weight = \"bold\",\n    size = px(11)\n  ),\n  locations = cells_body(columns = dataset)\n) |>\n\n# Concept column styling\ntab_style(\n  style = cell_text(\n    size = px(11),\n    weight = \"bold\"\n  ),\n  locations = cells_body(columns = concept)\n) |>\n\n# Label column with improved readability\ntab_style(\n  style = cell_text(\n    size = px(11),\n    color = \"#2c3e50\"\n  ),\n  locations = cells_body(columns = label)\n) |>\n\n# Optimized column widths for better content display\ncols_width(\n  dataset ~ px(100),\n  concept ~ px(180),\n  label ~ px(350),\n  name ~ px(140)\n) |>\n\n# Comprehensive table options for professional appearance\ntab_options(\n  # Typography\n  table.font.names = c(\"Segoe UI\", \"Arial\", \"sans-serif\"),\n  table.font.size = px(12),\n  heading.title.font.size = px(22),\n  heading.subtitle.font.size = px(16),\n  row_group.font.size = px(15),\n  \n  # Background and colors\n  table.background.color = \"#fdfdfd\",\n  \n  # Borders and lines\n  table.border.top.style = \"solid\",\n  table.border.top.width = px(4),\n  table.border.top.color = ColorHighlight,\n  table.border.bottom.style = \"solid\",\n  table.border.bottom.width = px(4),\n  table.border.bottom.color = ColorHighlight,\n  \n  # Column labels\n  column_labels.border.bottom.style = \"solid\",\n  column_labels.border.bottom.width = px(3),\n  column_labels.border.bottom.color = ColorHighlight,\n  column_labels.background.color = ColorHighlight,\n  \n  # Row groups\n  row_group.border.top.style = \"solid\",\n  row_group.border.top.width = px(2),\n  row_group.border.top.color = ColorHighlight,\n  row_group.border.bottom.style = \"solid\",\n  row_group.border.bottom.width = px(1),\n  row_group.border.bottom.color = ColorHighlightLow,\n  \n  # Striping and spacing\n  #row.striping.include_table_body = TRUE,\n  #row.striping.background_color = \"#f8f9fa\",\n  table_body.hlines.style = \"solid\",\n  table_body.hlines.width = px(1),\n  table_body.hlines.color = \"#dee2e6\",\n  \n  # Layout\n  # table.width = pct(100),\n  # container.overflow.x = TRUE,\n  # container.overflow.y = TRUE,\n  # container.height = px(600),\n  \n  quarto.disable_processing = TRUE\n) |>\n\n# Enhanced source note with better formatting\ntab_source_note(\n  source_note = md(\"**ðŸ“Š Data Source:** U.S. Census Bureau, American Community Survey (ACS) 5-Year Estimates | **ðŸ“… Analysis Period:** 2018-2022\")\n) |>\n\n# Style the source note\ntab_style(\n  style = list(\n    cell_text(\n      size = px(12),\n      color = \"#7f8c8d\",\n      style = \"italic\"\n    )\n  ),\n  locations = cells_source_notes()\n)\n```\n\nAfter confirming these promising results we can save the most import features for future use.\n\n```r\nfst::write_fst(\n  ConsolidatedVariableTractsByZoneId[, .SD, \n                                     .SDcols = c(\"LocationID\",\n                                                 TopVariableTractsMeanDiff)], \n  path = here(\"output/AcsVariablesByZoneId.fst\")\n)\n```\n\n## Conclusion\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"08-expanding-transportation-socioeconomic.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"source","theme":"cosmo","title":"Exploring Transportation and Socioeconomic Patterns","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}