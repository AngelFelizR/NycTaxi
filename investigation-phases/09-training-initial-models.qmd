---
title: "Model Development & Optimization Pipeline"
subtitle: "A Machine Learning Approach to Taxi Trip Acceptance Prediction"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

After extensive data exploration and feature engineering, we developed a classification model to predict whether drivers should accept incoming trip requests based on expected profitability.

The goal is to maximize driver earnings by helping them identify the most valuable trips while minimizing time spent on low-value assignments. This optimization problem directly impacts driver income and operational efficiency across the NYC taxi network.

# Phase 1: Model Selection & Initial Training

## 1.1 Problem Context & Algorithm Selection

To solve this binary classification problem (accept trip: yes/no), we evaluated a diverse range of machine learning algorithms. Our selection strategy balances interpretability, performance, and computational efficiency:

- **Simple, Interpretable Models:**
  - Regularized Logistic Regression via `glmnet`
  - Flexible Discriminant Analysis (FDA)

- **Non-linear, Moderate Complexity Models:**
  - MARS (Multivariate Adaptive Regression Splines) via `earth`
  - Bagged Trees via `rpart`

- **Advanced Ensemble Models:**
  - Random Forests via `ranger`
  - Gradient Boosted Trees via `xgboost`

This progression from simple to complex allows us to identify the optimal balance between model performance and computational requirements for production deployment.

**Training Strategy:** We employ 5-fold cross-validation with 5 random hyperparameter configurations per model to ensure robust performance estimates while managing computational costs.

## 1.2 Environment Setup

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(lubridate)
library(timeDate)
library(ggtext)

## Tools for modeling
library(tidymodels)
library(embed)
library(themis)
library(discrim)
library(DALEX)

## For table formatting
library(gt)

## Publish data sets, models, and other R objects
library(pins)

## Custom functions
devtools::load_all()

# Defining the pin boards to use
BoardLocal <- board_folder(here("../NycTaxiPins/Board"))

# Loading params
Params <- yaml::read_yaml(here("params.yml"))
Params$BoroughColors <- unlist(Params$BoroughColors)
```

## 1.3 Model Specification

Each model is configured with hyperparameters marked for tuning. The tuning process will identify optimal values within predefined search spaces.

```{r}
# Regularized logistic regression
GlmnetSpec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_mode("classification") |>
  set_engine("glmnet")

# Flexible discriminant analysis
FdaSpec <-
  discrim_flexible(prod_degree = tune()) |>
  set_mode("classification") |>
  set_engine('earth')

# MARS
EarthSpec <-
  mars(num_terms = tune(), prod_degree = tune()) |>
  set_mode("classification")

# Random forests
RangerSpec <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 250) |>
  set_mode("classification") |>
  set_engine("ranger")

# Boosted trees
XgboostSpec <-
  boost_tree(
    trees = 250,
    tree_depth = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune()
  ) |>
  set_mode("classification") |>
  set_engine("xgboost")
```

## 1.4 Data Loading & Preparation

We load pre-processed datasets from our local pin board. These datasets include trip-level features, geographic zone characteristics, demographic variables from the American Community Survey (ACS), and spatial density metrics.

```{r}
AcsVariablesByZoneId <-
  pin_read(BoardLocal, "AcsVariablesByZoneId")[,
    LocationID := as.character(LocationID)
  ]

OmsDensityFeatures <- pin_read(BoardLocal, "OmsDensityFeatures")[,
  LocationID := as.character(LocationID)
]

ZoneCodesRef <- pin_read(BoardLocal, "ZoneCodesRef")[, c(
  "LocationID",
  "Borough",
  "service_zone"
)]

SampledData <-
  pin_list(BoardLocal) |>
  grep(pattern = "^OneMonthData", value = TRUE) |>
  sort() |>
  head(12L) |>
  lapply(FUN = pin_read, board = BoardLocal) |>
  rbindlist() |>
  tibble::as_tibble() |>
  mutate(
    take_current_trip = fifelse(take_current_trip == 1L, "yes", "no") |>
      factor(levels = c("yes", "no")),
    PULocationID = as.character(PULocationID),
    DOLocationID = as.character(DOLocationID)
  )

set.seed(2545)
SampledDataSplit <- initial_split(SampledData, strata = take_current_trip)

TrainingSample <- training(SampledDataSplit)
TestingSample <- testing(SampledDataSplit)
```

This comprehensive recipe transforms raw trip data into a rich feature set by incorporating geospatial, temporal, and contextual information. The pipeline is designed to be reproducible—new predictions require only basic trip data from the [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), as all feature engineering is embedded in the workflow.

**Key Feature Categories:**
- **Geospatial Features:** Zone characteristics, demographic data, density metrics for pickup and dropoff locations
- **Temporal Features:** Daily and weekly cyclical patterns, calendar features, holiday proximity
- **Trip Characteristics:** Duration, distance, payment information
- **Company Identity:** Rideshare platform (Uber, Lyft, Via, Juno)

```{r}
#| eval: false

ConsolidationRecipe <-
  # Starting Recipe
  recipe(
    take_current_trip ~
      PULocationID +
      DOLocationID +
      hvfhs_license_num +
      trip_miles +
      driver_pay +
      request_datetime +
      trip_id +
      performance_per_hour +
      percentile_75_performance,
    data = TrainingSample
  ) |>

  # Updating roles of variables important for trip identification
  update_role(
    trip_id,
    performance_per_hour,
    percentile_75_performance,
    new_role = "additional info"
  ) |>

  # Selecting variables over 2 min
  step_filter(trip_time >= (60 * 2)) |>

  # Renaming variables to join
  step_rename(PU_LocationID = PULocationID, DO_LocationID = DOLocationID) |>

  # Adding Geospatial Data
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = ZoneCodesRef,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = AcsVariablesByZoneId,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = OmsDensityFeatures,
    col_prefix = c("DO_", "PU_")
  ) |>

  # Transforming strings to factors
  step_string2factor(all_string_predictors()) |>

  # Daily cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_daily = request_datetime_sin_1,
    request_datetime_cos_daily = request_datetime_cos_1
  ) |>

  # Weekly cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24 * 7,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_weekly = request_datetime_sin_1,
    request_datetime_cos_weekly = request_datetime_cos_1
  ) |>

  # Extracting additional information
  step_date(
    request_datetime,
    features = c(
      "year",
      "week",
      "decimal",
      "semester",
      "quarter",
      "doy",
      "dow",
      "mday",
      "month"
    )
  ) |>

  step_holiday(
    request_datetime,
    holidays = c(
      'USChristmasDay',
      'USColumbusDay',
      'USCPulaskisBirthday',
      'USDecorationMemorialDay',
      'USElectionDay',
      'USGoodFriday',
      'USInaugurationDay',
      'USIndependenceDay',
      'USJuneteenthNationalIndependenceDay',
      'USLaborDay',
      'USLincolnsBirthday',
      'USMemorialDay',
      'USMLKingsBirthday',
      'USNewYearsDay',
      'USPresidentsDay',
      'USThanksgivingDay',
      'USVeteransDay',
      'USWashingtonsBirthday'
    )
  ) |>

  step_mutate(
    .pkgs = c("data.table", "lubridate", "timeDate"),

    company = fcase(
      hvfhs_license_num == "HV0002" ,
      "Juno"                        ,
      hvfhs_license_num == "HV0003" ,
      "Uber"                        ,
      hvfhs_license_num == "HV0004" ,
      "Via"                         ,
      hvfhs_license_num == "HV0005" ,
      "Lyft"                        ,
      default = "New"
    ) |>
      as.factor(),

    request_datetime_am = am(request_datetime) |> as.integer(),
    request_datetime_pm = pm(request_datetime) |> as.integer(),

    `Days to USChristmasDay` = difftime(
      USChristmasDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USColumbusDay` = difftime(
      USColumbusDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USCPulaskisBirthday` = difftime(
      USCPulaskisBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USDecorationMemorialDay` = difftime(
      USDecorationMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USElectionDay` = difftime(
      USElectionDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USGoodFriday` = difftime(
      USGoodFriday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USInaugurationDay` = difftime(
      USInaugurationDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USIndependenceDay` = difftime(
      USIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USJuneteenthNationalIndependenceDay` = difftime(
      USJuneteenthNationalIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLaborDay` = difftime(
      USLaborDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLincolnsBirthday` = difftime(
      USLincolnsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMemorialDay` = difftime(
      USMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMLKingsBirthday` = difftime(
      USMLKingsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USNewYearsDay` = difftime(
      USNewYearsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USPresidentsDay` = difftime(
      USPresidentsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USThanksgivingDay` = difftime(
      USThanksgivingDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USVeteransDay` = difftime(
      USVeteransDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USWashingtonsBirthday` = difftime(
      USWashingtonsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer()
  ) |>

  # Removing variables
  step_rm(ends_with(c(
    "LocationID",
    "request_datetime",
    "hvfhs_license_num"
  )))

TrainingSampleJoined <-
  prep(ConsolidationRecipe) |>
  bake(new_data = NULL)

pin_write(
  BoardLocal,
  ConsolidationRecipe,
  "ConsolidationRecipe",
  type = "qs2",
  title = "Consolidation Recipe"
)

pin_write(
  BoardLocal,
  TrainingSampleJoined,
  "TrainingSampleJoined",
  type = "qs2",
  title = "Training Sample Joined"
)
```

```{r}
#| echo: false
#| output: false

TrainingSampleJoined <- pin_read(BoardLocal, "TrainingSampleJoined")
```

## 1.5 Feature Engineering Pipeline

As we need to create recipe for each model as they have deferent asumptiones, then it's better to create a simple  function to start all the following recipes as we need to apply the next steps in all of then:

1. Select `take_current_trip` as variable to predict and the rest of columns as predictors.
2. Downsample the train data to about creating models that only predict the most common decision, which is to take the trip.
3. Use the median to impute any missing value based on training data.
4. Confirm that `trip_id`, `performance_per_hour` and `percentile_75_performance` won't be used as predictors, but to track the results.

```{r}
start_recipe <- function(df) {
  new_recipe =
    recipe(take_current_trip ~ ., data = df) |>
    step_downsample(take_current_trip, under_ratio = 1) |>
    step_impute_median(all_numeric_predictors()) |>
    update_role(
      trip_id,
      performance_per_hour,
      percentile_75_performance,
      new_role = "additional info"
    )

  return(new_recipe)
}
```

### 1.5.1 Normalization & Dimensionality Reduction

For models sensitive to the scale and distribution of predictors (such as Logistic Regression, FDA, and MARS), we implement a "Basic Normalized" pipeline. This involves applying a Yeo-Johnson transformation to handle non-normality, creating dummy variables for categorical data, and filtering out near-zero variance (NZV) predictors.

Building upon this base, we define three distinct dimensionality reduction strategies to capture latent patterns in the taxi data:

* **Principal Component Analysis (PCA):** An unsupervised approach to reduce collinearity by creating orthogonal linear combinations of predictors.
* **Partial Least Squares (PLS):** A supervised approach that reduces dimensions while maximizing the covariance between predictors and the trip acceptance outcome.
* **Uniform Manifold Approximation and Projection (UMAP):** A non-linear technique used here to capture complex, manifold-based relationships in the high-dimensional feature space.

```{r}
BasicNormalizedRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

NormalizedPcaRecipe <-
  BasicNormalizedRecipe |>
  step_pca(all_numeric_predictors(), num_comp = tune()) |>
  step_normalize(all_numeric_predictors())

NormalizedPlsRecipe <-
  BasicNormalizedRecipe |>
  step_pls(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())

NormalizedUmapRecipe <-
  BasicNormalizedRecipe |>
  step_umap(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    neighbors = tune(),
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())
```

### 1.5.2 Sparsity Management

Tree-based models like Random Forest and XGBoost are generally robust to monotonic transformations but can struggle with high-cardinality categorical variables (like specific NYC neighborhoods). To address this, we define a "Reduced Levels" recipe. This approach uses `step_other()` to collapse infrequent categories into an "other" bucket, followed by one-hot encoding to allow the ensemble models to partition the data effectively.

```{r}
ReducedLevelsRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_novel(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = tune()) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_nzv(all_predictors())
```

## 1.6 Model Training Workflows

With all recipes and model specifications defined, we now assemble them into unified training workflows using `workflow_set()`. Each workflow pairs a preprocessing recipe with one or more model specifications, allowing tidymodels to jointly tune and evaluate every recipe–model combination in a systematic way.

The workflows are organized into three groups based on their computational profiles and the assumptions of the underlying models:

- **`WorkFlowSimple`** pairs the `BasicNormalizedRecipe` with regularized logistic regression. This group is lightweight in memory and serves as our interpretable baseline, establishing a performance floor against which more complex models are compared.

- **`WorkFlowDimReduction`** combines the three dimensionality-reduction recipes (PCA, PLS, and UMAP) with FDA and MARS. These combinations are memory-intensive due to the additional transformation steps, but allow the non-linear models to operate on a compressed, lower-dimensional representation of the feature space.

- **`WorkFlowTrees`** pairs the `ReducedLevelsRecipe` with Random Forest and XGBoost. Since tree-based models are scale-invariant but sensitive to high-cardinality categoricals, this workflow applies the sparse category collapsing strategy defined in [section 1.5.2](#sparsity-management).

Each workflow set is serialized and saved to the local pin board for downstream hyperparameter tuning and model selection. This **modular structure** makes it straightforward to add new recipe–model combinations or rerun individual groups without retraining the entire pipeline.

```{r}
#| eval: false

# Group 1: Simple linear models (light on memory)
WorkFlowSimple <- workflow_set(
  preproc = list(
    normalized = BasicNormalizedRecipe
  ),
  models = list(
    reg_logistic = GlmnetSpec
  )
)

# Group 2: Dimensionality reduction models (memory intensive)
WorkFlowDimReduction <- workflow_set(
  preproc = list(
    pca = NormalizedPcaRecipe,
    pls = NormalizedPlsRecipe,
    umap = NormalizedUmapRecipe
  ),
  models = list(
    flex_da = FdaSpec,
    mars = EarthSpec
  )
)

# Group 3: Tree-based models
WorkFlowTrees <- workflow_set(
  preproc = list(
    reduced_levels = ReducedLevelsRecipe
  ),
  models = list(
    random_forest = RangerSpec,
    xgboost = XgboostSpec
  )
)

pin_write(
  BoardLocal,
  WorkFlowSimple,
  "WorkFlowSimple",
  type = "qs2",
  title = "Work Flow Simple"
)

pin_write(
  BoardLocal,
  WorkFlowDimReduction,
  "WorkFlowDimReduction",
  type = "qs2",
  title = "Work Flow Dim Reduction"
)

pin_write(
  BoardLocal,
  WorkFlowTrees,
  "WorkFlowTrees",
  type = "qs2",
  title = "Work Flow Trees"
)
```

## 1.7 Tuning Workflows

With the three workflow groups saved to disk, we now run hyperparameter tuning for each group in separate scripts. Running them separately **keeps memory usage manageable**, since some combinations — particularly those involving dimensionality reduction — are considerably more expensive to train than others.

Each script follows the same overall pattern: 

1. Load the saved workflow from the pin board. 
2. Define a 5-fold cross-validation scheme on the joined training data
3. Generate a small space-filling grid of 5 hyperparameter configurations using the Audze-Eglais design.
4. Evaluate every configuration across all folds using both ROC-AUC and Brier Score as metrics.

Once tuning is complete, the results are saved back to the pin board so [section 1.8](#performance-comparison) can load and compare them together.

```{r}
#| eval: false
#| file: ../multicore-scripts/03a-tuning-simple-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03b-tuning-dimreduction-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03c-tuning-tree-models.R
```

## 1.8 Performance Comparison

We load the trained models and compare their cross-validated performance using the Brier score metric.

```{r}
#| eval: false

WorkFlowTuned <- c(
  pin_read(BoardLocal, "WorkFlowSimpleTuned"),
  pin_read(BoardLocal, "WorkFlowDimReductionTuned"),
  pin_read(BoardLocal, "WorkFlowTreesTuned")
)

WorkFlowTunedBest <- lapply(
  names(WorkFlowTuned),
  FUN = \(x) {
    show_best(WorkFlowTuned[[x]], metric = "brier_class") |> mutate(model = x)
  }
) |>
  bind_rows()

pin_write(
  BoardLocal,
  WorkFlowTunedBest,
  "WorkFlowTunedBest",
  type = "qs2",
  title = "Work Flow Tuned Best"
)
```

Now we can confirm that the best models were `Logistic Regression` and `Random Forest` based on the number of correct predictions. However, we also need to consider that the relative cost of taking or rejecting the wrong trip can change from trip to trip. It is crucial that the metric used to select the best model takes these economic implications into consideration.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

pin_read(
  BoardLocal,
  "WorkFlowTunedBest"
) |>
  group_by(model) |>
  mutate(meadian_of_mean_score = quantile(mean, probs = 0.75)) |>
  ungroup() |>
  mutate(
    model = reorder(model, -mean, FUN = median),
    best_models = case_when(
      meadian_of_mean_score < 0.18 ~ "group1",
      meadian_of_mean_score < 0.23 ~ "group2",
      .default = "other"
    )
  ) |>
  ggplot(aes(model, mean)) +
  geom_boxplot(aes(fill = best_models)) +
  scale_fill_manual(
    values = c(
      "group1" = Params$ColorHighlight,
      "group2" = Params$ColorHighlightLow,
      "other" = Params$ColorGray
    )
  ) +
  coord_flip() +
  labs(
    title = paste0(
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Logistic Regression**",
      "</span> ",
      " and ",
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Random Forest**",
      "</span> "
    ),
    subtitle = "were the best models based on **Brier Score**",
    y = "Brier Score",
    x = "Trained Models"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.position = "none"
  )
```

To estimate the cost, we select the best model of each type and calculate the difference between the `performance_per_hour` of each trip and the `performance_per_hour` of the 75th percentile, depending on whether the final prediction was correct or not.

```{r}
collect_prediction_cost <- function(
  training_data,
  tuned_wf,
  model_name,
  threshold,
  metric = "brier_class"
) {
  NycTaxi::collect_predictions_best_config(tuned_wf, model_name, metric) |>
    NycTaxi::add_performance_variables(training_data = training_data) |>
    NycTaxi::add_pred_class(threshold) |>
    NycTaxi::calculate_costs()
}
```

```{r}
#| eval: false

PredictionsCosts <- lapply(
  names(WorkFlowTuned),
  FUN = collect_prediction_cost,
  threshold = 0.5,
  training_data = TrainingSampleJoined,
  tuned_wf = WorkFlowTuned
) |>
  bind_rows()

pin_write(
  BoardLocal,
  x = PredictionsCosts,
  name = "PredictionsCosts",
  type = "qs2",
  title = "Predictions Costs"
)
```

The current method taxis use in the simulation is to accept all trips. We can calculate that, on average, this strategy resulted in a loss of around $3.8 per trip on the training data. We can now compare how that number would change if we used the best trained model for each model type.

Its really important to highlight the following points:

- **normalized_reg_logistic** was the best model in terms of **Brier Score** and cost reduction.
- **reduced_levels_random_forest** and **reduced_levels_xgboost** regardless of performing similarly to *normalized_reg_logistic* in the **Brier Score** didn't achieve similar cost reduction.
- **pls_mars** regardless of presenting a relatively high **Brier Score** presented similar cost reduction to the *normalized_reg_logistic*.

```{r}
PredictionsCosts <- pin_read(
  BoardLocal,
  name = "PredictionsCosts"
)

PredictionsCostsSummary <-
  PredictionsCosts |>
  # Defining cost of model and fold
  group_by(model_name, id) |>
  summarize(
    current_method_cost = mean(current_method_cost, na.rm = TRUE),
    model_method_cost = mean(cost_wrong_total, na.rm = TRUE)
  ) |>
  # Defining cost of model across folds
  summarize(
    current_method_cost = mean(current_method_cost),
    model_method_cost = mean(model_method_cost)
  ) |>
  mutate(
    total_benefit = current_method_cost - model_method_cost,
    model_name = reorder(model_name, total_benefit)
  ) |>
  arrange(desc(total_benefit))
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"

PredictionsCostsSummary |>
  ggplot(aes(model_method_cost, model_name)) +
  geom_vline(
    xintercept = PredictionsCostsSummary$current_method_cost[1L],
    color = Params$ColorGray,
    linetype = 2,
    linewidth = 1.5
  ) +
  geom_segment(
    aes(x = current_method_cost, xend = model_method_cost),
    color = Params$ColorGray,
    linewidth = 1
  ) +
  geom_point(
    shape = 21,
    color = "black",
    fill = Params$ColorHighlight,
    stroke = 0.5,
    size = 3
  ) +
  scale_x_continuous(breaks = scales::breaks_width(0.2)) +
  expand_limits(x = 0) +
  labs(
    title = paste0(
      "**Only** <span style='color:",
      Params$ColorHighlight,
      ";'>",
      "Xgboost",
      "</span> ",
      "is better than current method"
    ),
    y = "Trained Models",
    x = "Average Cost Lost Due Wrong Selection"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none"
  )
```


# Phase 2: XGBoost Deep Dive & Optimization

Based on initial results, we identified XGBoost as the most promising algorithm. This section details an expanded hyperparameter search to maximize model performance.

## 2.1 Expanded XGBoost Hyperparameter Search

The frist step is to define a new model specification with more params to be tuned and update the original workflow used.

```{r}
XgboostSpecToTune <-
  boost_tree(
    trees = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune()
  ) |>
  set_mode("classification") |>
  # Single-thread to leverage fold parallelization
  set_engine("xgboost", nthread = 1)

# Extraer y actualizar el workflow
WorkFlowXgboost <-
  pins::pin_read(BoardLocal, "WorkFlowTrees") |>
  extract_workflow(id = "reduced_levels_xgboost") |>
  update_model(XgboostSpecToTune)
```

## 2.2 Bayesian Hyperparameter Optimization

After identifying XGBoost as the lead candidate, we implement **Bayesian Optimization** to fine-tune the model. Unlike a standard grid search that tests combinations in isolation, Bayesian optimization uses the results of previous evaluations to form a probabilistic model of the hyperparameter space, allowing it to "search" for the most promising configurations more efficiently.

### The Refinement Strategy

In this implementation, we focus the search on specific ranges informed by our initial findings:

- **Targeted Search Spaces:** We defined refined ranges for key parameters such as `trees` (200–350), `min_n` (15–25), and `tree_depth` (5–8) to explore the neighborhood of our best-performing initial results.
- **Resource Management:** The XGBoost engine is set to single-thread (`nthread = 1`) specifically about stop testing models due RAM limitations.
- **Iterative Tuning:** We start with 10 random initial points and allow the algorithm up to 25 iterations to find an optimal "peak" in performance, using both **Brier Score** and **ROC-AUC** as the guiding metrics.
- **Convergence Controls:** To prevent wasted computation, we set a `no_improve` threshold of 20 iterations, ensuring the process stops if it reaches a performance plateau.

Once the tuning completes, the best configuration is extracted based on the lowest Brier Score and saved to the local board for final model finalization.

```{r}
#| eval: false

XgboostParamsRefined <-
  extract_parameter_set_dials(WorkFlowXgboost) |>
  update(
    # Search range slightly above and below 221
    trees = trees(range = c(200L, 350L)),

    # Search range slightly above and below 18
    min_n = min_n(range = c(15L, 25L)),

    # Focused range around 96 (Ensure it doesn't exceed total features - 1)
    mtry = mtry(range = c(70L, min(100L, ncol(TrainingSampleJoined) - 1L))),

    # Focused range around 6 (allowing for slightly deeper or shallower trees)
    tree_depth = tree_depth(range = c(5L, 8L)),

    # Linear search range around 0.104
    learn_rate = learn_rate(range = c(0.05, 0.20), trans = NULL),

    # Focused range around log10(0.000556) which is about -3.25
    loss_reduction = loss_reduction(range = c(-4, -3)),

    # Focused range around 0.784
    sample_size = sample_prop(range = c(0.70, 0.90))
  )

# Defining resamples
set.seed(5878)
TrainingSampleJoinedResamples <- vfold_cv(TrainingSampleJoined, v = 5)

# Optimización Bayesiana
set.seed(91254)
WorkFlowXgboostTuned <-
  tryCatch(
    WorkFlowXgboost |>
      tune_bayes(
        resamples = TrainingSampleJoinedResamples,
        metrics = metric_set(brier_class, roc_auc),
        initial = 10, # Start with 10 random points (recommended)
        param_info = XgboostParamsRefined, # Allow 25 Bayesian optimization iterations to find a better peak
        iter = 25L,

        control = control_bayes(
          verbose = TRUE,
          verbose_iter = TRUE,
          seed = 91254,
          save_pred = FALSE,

          # Parallelize folds (requires xgboost nthread=1)
          parallel_over = "resamples",

          # Allow more iterations without improvement before stopping
          no_improve = 20L,

          # Explore uncertain regions
          uncertain = 5L,

          extract = NULL
        )
      ),
    error = function(e) {
      message("Error during tuning: ", e$message)
      return(NULL)
    }
  )


# Process results if tuning succeeded
if (!is.null(WorkFlowXgboostTuned)) {
  BestXgboostConfig <- select_best(WorkFlowXgboostTuned, metric = "brier_class")
  MetricsXgboostHistory <- collect_metrics(WorkFlowXgboostTuned)

  pin_write(
    BoardLocal,
    x = BestXgboostConfig,
    name = "BestXgboostConfig",
    type = "qs2",
    title = "Best Xgboost Config"
  )

  pin_write(
    BoardLocal,
    x = MetricsXgboostHistory,
    name = "MetricsXgboostHistory",
    type = "qs2",
    title = "Metrics Xgboost History"
  )
}
```

### Results

The new XGBoost model is **more precise and consistent** than the original. By looking at more data and using a deeper, more refined tree structure, we successfully reduced the error rate (Brier Score) by approximately **4%**:

* **Better Accuracy:** The Brier Score dropped from **0.1502 to 0.1443**. Since a lower score is better, this means the new model's predictions are more accurate and reliable.
* **Greater Stability:** The standard error is lower (**0.0008**), meaning the model is more consistent and less likely to give "lucky" or erratic results.

The new model achieved these better results by becoming more complex and thorough:

* **Deeper Learning:** It uses deeper trees (**7 levels** instead of 4) and more "trees" overall (**347**), allowing it to catch more complicated patterns in your data.
* **More Data Usage:** It looks at almost **90%** of the available sample size, compared to only 50% in the first model.
* **Slower, Careful Learning:** The learning rate was lowered to **0.106**. This means it learns more slowly but more precisely, avoiding the "over-correction" issues the initial model might have had.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

OriginalPerformace <-
  pin_read(BoardLocal, "WorkFlowTreesTuned")[["reduced_levels_xgboost"]] |>
  collect_metrics() |>
  filter(.metric == "brier_class") |>
  select(mean, std_err, mtry:threshold) |>
  bind_cols(source = "initial_model", raw = _) |>
  arrange(mean) |>
  head(1L)

NewPerformance <-
  pin_read(BoardLocal, name = "MetricsXgboostHistory") |>
  filter(.metric == "brier_class") |>
  select(mean, std_err, mtry:threshold) |>
  bind_cols(source = "new_model", raw = _) |>
  arrange(mean) |>
  head(1L)

ModelComparison <- bind_rows(OriginalPerformace, NewPerformance)

create_performance_gt <- function(
  df,
  title = "XGBoost Model Performance Comparison",
  subtitle = "Comparing Brier Score and Hyperparameters: Initial vs. New Model",
  color_highlight_low = Params$ColorHighlightLow
) {
  created_table =
    gt(df) |>
    # 1. Header & Titles
    tab_header(
      title = title,
      subtitle = subtitle
    ) |>

    # 2. Apply styling to Header (Matching your example)
    tab_style(
      style = cell_text(color = "white", weight = "bold"),
      locations = cells_title()
    ) |>
    tab_style(
      style = cell_fill(color = "#2c3e50"),
      locations = cells_title()
    ) |>

    # 3. Formatting Numbers
    fmt_number(
      columns = c(mean, std_err),
      decimals = 4
    ) |>
    fmt_number(
      columns = c(learn_rate, loss_reduction, threshold),
      decimals = 3
    ) |>

    # 4. Conditional Formatting (Highlighting the lower Brier Score)
    data_color(
      columns = mean,
      colors = function(x) {
        ifelse(
          x == min(x, na.rm = TRUE),
          color_highlight_low,
          "white"
        )
      }
    ) |>

    # 5. Column Labels
    cols_label(
      source = "MODEL SOURCE",
      mean = "BRIER SCORE (MEAN)",
      std_err = "STD ERROR",
      mtry = "MTRY",
      min_n = "MIN N",
      tree_depth = "DEPTH",
      learn_rate = "LEARN RATE",
      loss_reduction = "LOSS RED.",
      sample_size = "SAMPLE SIZE",
      threshold = "THRESHOLD",
      trees = "TREES"
    ) |>

    # 6. General Table Options
    tab_options(
      table.font.names = "Arial",
      table.background.color = "white",
      column_labels.font.weight = "bold",
      column_labels.background.color = "#f8f9fa",
      table.width = pct(100),
      quarto.disable_processing = TRUE
    )

  return(created_table)
}

create_performance_gt(ModelComparison)

```

```{r}
#| output: false
#| echo: false

rm(list = grep("^WorkFlowTuned$|^(Normalized|Basic)", ls(), value = TRUE))
gc()
```


# Phase 3: Exploring Variable Importance

Another way to improve model perfomance is to reduce feature with low prediction power. To make this possible the need to idenfity feature importance of all feature in under to select the one keep using.

As we didn't define a validation set to create DALEX explaners let's split the testing data.

```{r}
set.seed(2971)
ExplainerSplit <-
  initial_split(TrainingSampleJoined, strata = take_current_trip)

ExplainerTraining <- training(ExplainerSplit)
ExplainerTesting <- testing(ExplainerSplit)
```

Using the new configuration, let's fit a model with the corresponding part of the training set.

```{r}
#| eval: false

BestXgboostConfig = pin_read(
  BoardLocal,
  name = "BestXgboostConfig"
)

XgboostWfFitted <- WorkFlowXgboost |>
  finalize_workflow(parameters = BestXgboostConfig) |>
  fit(data = ExplainerTraining)

pin_write(
  BoardLocal,
  x = XgboostWfFitted,
  name = "XgboostWfFitted",
  type = "qs2",
  title = "Xgboost Wf Fitted"
)
```

Then we can create the explainer for `DALEX`.

```{r}
XgboostWfFitted <- pin_read(BoardLocal, "XgboostWfFitted")

ExplainerXgboost <- DALEXtra::explain_tidymodels(
  XgboostWfFitted,
  data = ExplainerTesting |> select(-take_current_trip),
  y = ExplainerTesting$take_current_trip
)
```

Now we can evaluate how the model performs after permutating each of the variables.

```{r}
#| eval: false

set.seed(1980)
ExplainerXgboostVip <- model_parts(
  explainer = ExplainerXgboost,
  loss_function = get_loss_yardstick(brier_class),
  B = 50,
  type = "difference"
)

ExplainerXgboostVipSummary <-
  as.data.frame(ExplainerXgboostVip) |>
  filter(
    !variable %in%
      c(
        "_baseline_",
        "_full_model_",
        "trip_id",
        "percentile_75_performance",
        "performance_per_hour"
      )
  ) |>
  group_by(variable) |>
  summarize(dropout_loss = mean(dropout_loss)) |>
  mutate(
    is_positive = dropout_loss >= 0,
    variable = reorder(variable, -dropout_loss)
  ) |>
  arrange(dropout_loss)


pin_write(
  BoardLocal,
  x = ExplainerXgboostVip,
  name = "ExplainerXgboostVip",
  type = "qs2",
  title = "Explainer Xgboost Vip"
)

pin_write(
  BoardLocal,
  x = ExplainerXgboostVipSummary,
  name = "ExplainerXgboostVipSummary",
  type = "qs2",
  title = "Explainer Xgboost Vip Summary"
)
```

Check the the distribution of the importance of variables we can confirm that:

- The most important variables present a negative metric change.
- More that 95% of variable present an importance close to 0, but around 43% of variables a positive

```{r}
#| code-fold: true
#| code-summary: "Show the code"

ExplainerXgboostVipSummary <- pin_read(
  BoardLocal,
  "ExplainerXgboostVipSummary"
)

ggplot(
  ExplainerXgboostVipSummary,
  aes(x = dropout_loss, y = after_stat(count / sum(count)))
) +
  geom_histogram(
    aes(fill = is_positive),
    color = "black",
    binwidth = 0.01
  ) +
  scale_fill_manual(
    values = c("FALSE" = Params$ColorGray, "TRUE" = Params$ColorHighlight)
  ) +
  scale_y_continuous(labels = percent_format(), breaks = breaks_width(0.1)) +
  scale_x_continuous(n.breaks = 12) +
  labs(
    title = "Big Majority of Variables Present Low Importance",
    x = "Brier Class Change",
    y = "Proportion of Variables",
    fill = "The Importance is Positive"
  ) +
  theme_minimal() +
  theme(
    legend.position = "top",
    plot.title = element_text(face = "bold", size = 15),
    panel.grid.minor = element_blank()
  )
```

By takin the top 20 variables and worst 5 we can see that:

- `trip_time`, `driver_pay`, `trip_miles`, `cos_daily` and `company` are the most important variable.
- All the rest of the variables are really close to 0 as all of they are related to the position where the trips start or end.

As result we say that `PULocationID` and `DOLocationID` are very important to predict the target variable but studing the individual effect of each variable don't make much sense. To explain this, we can imagine that the signal the model can find in one variable is also available combining other variables keeping the response simular to the original.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

bind_rows(
  head(ExplainerXgboostVipSummary, 5L),
  tail(ExplainerXgboostVipSummary, 20L)
) |>
  ggplot(aes(dropout_loss, variable)) +
  geom_col(aes(fill = is_positive), color = "black", width = 0.5) +
  scale_fill_manual(
    values = c("FALSE" = Params$ColorGray, "TRUE" = Params$ColorHighlight)
  ) +
  scale_x_continuous(labels = comma_format()) +
  scale_y_discrete(label = \(x) {
    if_else(nchar(x) > 20, substr(x, nchar(x) - 20, nchar(x)), x)
  }) +
  labs(
    title = "Location Related Variable Present Low Individual VIP",
    y = "Variables",
    x = "Brier Class Change",
    fill = "The Importance is Positive"
  ) +
  theme_minimal() +
  theme(
    legend.position = "top",
    panel.grid.major.y = element_blank(),
    plot.title = element_text(face = "bold", size = 15),
    axis.text.x.top = element_text(angle = 90, hjust = 0)
  )
```

To have a better understanding of variable importance let's also take in consideartion the variabe importance metric provided by the training data, as it has the avantage that all the importance metric for all variables is positive and can be added to confirm the total importance based on the original variables used.

```{r}
ExplainerXgboostVipNativeSummary <-
  extract_fit_engine(XgboostWfFitted) |>
  xgboost::xgb.importance(model = _) |>
  transmute(Feature = reorder(Feature, Gain), Gain) |>
  arrange(desc(Gain))
```

By plotting the results we can see similar distribution where `trip_time`, `driver_pay`, `trip_miles` and `cos_daily` keep as top variables leaving `company` out of the top list but the rest of the variable keep having a low importance.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

bind_rows(
  head(ExplainerXgboostVipNativeSummary, 5L),
  tail(ExplainerXgboostVipNativeSummary, 20L)
) |>
  ggplot(aes(Gain, Feature)) +
  geom_col(fill = Params$ColorGray, color = "black", width = 0.5) +
  scale_x_continuous(labels = comma_format()) +
  scale_y_discrete(label = \(x) {
    if_else(nchar(x) > 20, substr(x, nchar(x) - 20, nchar(x)), x)
  }) +
  labs(
    title = "Xgboost present similar VIP",
    y = "Variables",
    x = "Gain Importance"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    plot.title = element_text(face = "bold", size = 15),
    axis.text.x.top = element_text(angle = 90, hjust = 0)
  )
```

# Phase 4: Dimentional reduction

As we cannot confirm the individual importance of available features, let's take a different strategy by using **Partial Least Squares (PLS)** as tool for dimensional reduction.

To make the model more affective let's also exclude cases where the `driver_pay` or `trip_miles` are 0 or lower as those trips must be errors in the data to update the workflow.

```{r}
TrainingAfterFilterNewFeatures <-
  TrainingSampleJoined |>
  filter(driver_pay > 0, trip_miles > 0) |>
  mutate(
    driver_pay_per_hour = driver_pay / (trip_time / 3600),
    driver_pay_per_mile = driver_pay / trip_miles
  )

ReducedLevelsPls <-
  start_recipe(TrainingAfterFilterNewFeatures) |>
  step_novel(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = tune()) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_nzv(all_predictors()) |>
  step_pls(all_predictors(), outcome = take_current_trip, num_comp = tune())

WorkFlowPls <-
  WorkFlowXgboost |>
  update_recipe(ReducedLevelsPls)
```

Now we just need to finetune the new workflow to find out whether we can find a better model to predict the target variable.

```{r}
#| eval: false

set.seed(65416)
TrainingAfterFilterNewFeaturesResamples <- vfold_cv(
  TrainingAfterFilterNewFeatures,
  v = 5,
  strata = take_current_trip
)

XgboostParamsPls <-
  extract_parameter_set_dials(WorkFlowPls) |>
  update(
    mtry = mtry(range = c(5L, ncol(TrainingAfterVarRemoval) - 1L)),
    num_comp = num_comp(range = c(5L, ncol(TrainingAfterVarRemoval) - 1L))
  )


set.seed(91254)
WorkFlowXgboostTunedPls <-
  tryCatch(
    WorkFlowPls |>
      tune_bayes(
        resamples = TrainingAfterFilterNewFeaturesResamples,
        metrics = metric_set(brier_class, sensitivity, specificity),
        initial = 10L,
        iter = 25L,
        param_info = XgboostParamsPls,
        control = control_bayes(
          verbose = TRUE,
          verbose_iter = TRUE,
          seed = 91254,
          save_pred = FALSE,

          # Parallelize folds (requires xgboost nthread=1)
          parallel_over = "resamples",

          # Allow more iterations without improvement before stopping
          no_improve = 20L,

          # Explore uncertain regions
          uncertain = 5L,

          extract = NULL
        )
      ),
    error = function(e) {
      message("Error during tuning: ", e$message)
      return(NULL)
    }
  )


# Process results if tuning succeeded
if (!is.null(WorkFlowXgboostTunedPls)) {
  BestXgboostConfigPls <- select_best(
    WorkFlowXgboostTunedPls,
    metric = "brier_class"
  )
  MetricsXgboostHistoryPls <- collect_metrics(WorkFlowXgboostTunedPls)

  pin_write(
    BoardLocal,
    x = BestXgboostConfigPls,
    name = "BestXgboostConfigPls",
    type = "qs2",
    title = "Best Xgboost Config Pls"
  )

  pin_write(
    BoardLocal,
    x = MetricsXgboostHistoryPls,
    name = "MetricsXgboostHistoryPls",
    type = "qs2",
    title = "Metrics Xgboost History Pls"
  )
}
```

After running several iteretation we weren't able to find a better model after reducing the number of features.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

NewPerformancePls <-
  pin_read(
    BoardLocal,
    name = "MetricsXgboostHistoryPls"
  ) |>
  select(mean, std_err, mtry:threshold) |>
  bind_cols(source = "pls_model", raw = _) |>
  arrange(mean) |>
  head(1L)

ModelComparisonPls <- bind_rows(
  ModelComparison,
  NewPerformancePls
)

create_performance_gt(
  ModelComparisonPls,
  subtitle = "Comparing Brier Score and Hyperparameters: Initial vs. New Model vs. PLS Model"
)
```
