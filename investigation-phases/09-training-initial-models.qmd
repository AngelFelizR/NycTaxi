---
title: "Training Initial Models"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

After exploring the data and getting new features it's time to explore the data using ML models to extract insights that will be used to create the final model.

In this section we will start defining the models to train. We trained a diverse set of models, from simple, interpretable ones (Logistic Regression) to complex, powerful ones (Random Forests), to see which approach works best for our data and hardware constraints:

- Regularized Regression Logistic Regression via glmnet
- Flexible discriminant analysis
- MARS via earth
- Bagged trees via rpart
- Random forests via ranger
- Boosted trees via xgboost

We are going to fit 5 random variations of the different parameters to tune over 5 fold cross validation, then explore the results of each model.
    
## Setting up the environment

Here are the loaded libraries to start the process.

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(lubridate)
library(timeDate)
library(ggtext)

## Tools for modeling
library(tidymodels)
library(embed)
library(themis)
library(discrim)

## Publish data sets, models, and other R objects
library(pins)

## Custom functions
devtools::load_all()

# Defining the pin boards to use
BoardLocal <- board_folder(here("../NycTaxiPins/Board"))

# Loading params
Params <- yaml::read_yaml(here("params.yml"))
Params$BoroughColors <- unlist(Params$BoroughColors)
```


## Defining models to train

Now we can define the models to be trained and tuned.

```{r}
# Regularized regression logistic regression
GlmnetSpec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_mode("classification") |>
  set_engine("glmnet")

# Flexible discriminant analysis
FdaSpec <-
  discrim_flexible(prod_degree = tune()) |>
  set_mode("classification") |>
  set_engine('earth')

# MARS
EarthSpec <-
  mars(num_terms = tune(), prod_degree = tune()) |>
  set_mode("classification")

# Random forests
RangerSpec <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 250) |>
  set_mode("classification") |>
  set_engine("ranger")

# Boosted trees
XgboostSpec <-
  boost_tree(
    trees = 250,
    tree_depth = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune()
  ) |>
  set_mode("classification") |>
  set_engine("xgboost")
```

## Importing data

Here we import the data to use from the remote Board located in a github repo.

```{r}
#| eval: false

AcsVariablesByZoneId <-
  pin_read(BoardLocal, "AcsVariablesByZoneId")[,
    LocationID := as.character(LocationID)
  ]

OmsDensityFeatures <- pin_read(BoardLocal, "OmsDensityFeatures")[,
  LocationID := as.character(LocationID)
]

ZoneCodesRef <- pin_read(BoardLocal, "ZoneCodesRef")[, c(
  "LocationID",
  "Borough",
  "service_zone"
)]

SampledData <-
  pin_list(BoardLocal) |>
  grep(pattern = "^OneMonthData", value = TRUE) |>
  sort() |>
  head(12L) |>
  lapply(FUN = pin_read, board = BoardLocal) |>
  rbindlist() |>
  tibble::as_tibble() |>
  mutate(
    take_current_trip = fifelse(take_current_trip == 1L, "yes", "no") |>
      factor(levels = c("yes", "no")),
    PULocationID = as.character(PULocationID),
    DOLocationID = as.character(DOLocationID)
  )

set.seed(2545)
SampledDataSplit <- initial_split(SampledData, strata = take_current_trip)

TrainingSample <- training(SampledDataSplit)
TestingSample <- testing(SampledDataSplit)
```

## Creating consolidation recipe

As we want to create a final recipe that can reproduce the whole pipeline we start creating a recipe that combine all features that can be used to train the model, so we we run `predict` we would only need to provide the basic data from the original source of [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and all the rest of feature would be added as part of the tidymodels workflow. 

```{r}
#| eval: false

ConsolidationRecipe <-
  # Starting Recipe
  recipe(
    take_current_trip ~
      PULocationID +
      DOLocationID +
      wav_match_flag +
      hvfhs_license_num +
      trip_miles +
      trip_time +
      driver_pay +
      request_datetime +
      trip_id +
      performance_per_hour +
      percentile_75_performance,
    data = TrainingSample
  ) |>

  # Updating roles of variables important for trip identification
  update_role(
    trip_id,
    performance_per_hour,
    percentile_75_performance,
    new_role = "additional info"
  ) |>

  # Selecting variables over 2 min
  step_filter(trip_time >= (60 * 2)) |>

  # Renaming variables to join
  step_rename(PU_LocationID = PULocationID, DO_LocationID = DOLocationID) |>

  # Adding Geospatial Data
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = ZoneCodesRef,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = AcsVariablesByZoneId,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = OmsDensityFeatures,
    col_prefix = c("DO_", "PU_")
  ) |>

  # Transforming strings to factors
  step_string2factor(all_string_predictors()) |>

  # Daily cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_daily = request_datetime_sin_1,
    request_datetime_cos_daily = request_datetime_cos_1
  ) |>

  # Weekly cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24 * 7,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_weekly = request_datetime_sin_1,
    request_datetime_cos_weekly = request_datetime_cos_1
  ) |>

  # Extracting additional information
  step_date(
    request_datetime,
    features = c(
      "year",
      "week",
      "decimal",
      "semester",
      "quarter",
      "doy",
      "dow",
      "mday",
      "month"
    )
  ) |>

  step_holiday(
    request_datetime,
    holidays = c(
      'USChristmasDay',
      'USColumbusDay',
      'USCPulaskisBirthday',
      'USDecorationMemorialDay',
      'USElectionDay',
      'USGoodFriday',
      'USInaugurationDay',
      'USIndependenceDay',
      'USJuneteenthNationalIndependenceDay',
      'USLaborDay',
      'USLincolnsBirthday',
      'USMemorialDay',
      'USMLKingsBirthday',
      'USNewYearsDay',
      'USPresidentsDay',
      'USThanksgivingDay',
      'USVeteransDay',
      'USWashingtonsBirthday'
    )
  ) |>

  step_mutate(
    .pkgs = c("data.table", "lubridate", "timeDate"),

    company = fcase(
      hvfhs_license_num == "HV0002" ,
      "Juno"                        ,
      hvfhs_license_num == "HV0003" ,
      "Uber"                        ,
      hvfhs_license_num == "HV0004" ,
      "Via"                         ,
      hvfhs_license_num == "HV0005" ,
      "Lyft"                        ,
      default = "New"
    ) |>
      as.factor(),

    request_datetime_am = am(request_datetime) |> as.integer(),
    request_datetime_pm = pm(request_datetime) |> as.integer(),

    `Days to USChristmasDay` = difftime(
      USChristmasDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USColumbusDay` = difftime(
      USColumbusDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USCPulaskisBirthday` = difftime(
      USCPulaskisBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USDecorationMemorialDay` = difftime(
      USDecorationMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USElectionDay` = difftime(
      USElectionDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USGoodFriday` = difftime(
      USGoodFriday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USInaugurationDay` = difftime(
      USInaugurationDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USIndependenceDay` = difftime(
      USIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USJuneteenthNationalIndependenceDay` = difftime(
      USJuneteenthNationalIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLaborDay` = difftime(
      USLaborDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLincolnsBirthday` = difftime(
      USLincolnsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMemorialDay` = difftime(
      USMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMLKingsBirthday` = difftime(
      USMLKingsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USNewYearsDay` = difftime(
      USNewYearsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USPresidentsDay` = difftime(
      USPresidentsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USThanksgivingDay` = difftime(
      USThanksgivingDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USVeteransDay` = difftime(
      USVeteransDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USWashingtonsBirthday` = difftime(
      USWashingtonsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer()
  ) |>

  # Removing variables
  step_rm(ends_with(c(
    "LocationID",
    "request_datetime",
    "hvfhs_license_num"
  ))) |>

  # Balancing data
  step_downsample(take_current_trip, under_ratio = 1)

pin_write(
  BoardLocal,
  ConsolidationRecipe,
  "ConsolidationRecipe",
  type = "qs2",
  title = "Consolidation Recipe"
)
```

## Consolidating features

As we are planing to train many models it's better to apply this initial recipes to the training and testing data to avoid having to apply the the same steps for each resample to train.

```{r}
#| eval: false

TrainingSampleJoined <-
  prep(ConsolidationRecipe) |>
  bake(new_data = NULL)

pin_write(
  BoardLocal,
  TrainingSampleJoined,
  "TrainingSampleJoined",
  type = "qs2",
  title = "Training Sample Joined"
)

```

```{r}
#| echo: false
#| output: false

TrainingSampleJoined <- pin_read(BoardLocal, "TrainingSampleJoined")
```

## Common steps for all recipes

Before creating the the first recipe it's  important to take in consideration that we want to keep `performance_per_hour` and `percentile_75_performance` to see how much money we could be losing for bad predictions.

```{r}
start_recipe <- function(df) {
  new_recipe =
    recipe(take_current_trip ~ ., data = df) |>
    step_impute_median(all_numeric_predictors()) |>
    update_role(
      trip_id,
      performance_per_hour,
      percentile_75_performance,
      new_role = "additional info"
    )

  return(new_recipe)
}
```

## Defining recipes for models that need normalized data

As the models that will be trained based on this data are very affect by class imbalance will downsample to avoid those problems and speed the training time, take in consideration that we have many examples of both cases and are looking for general rules that could used by taxi drivers. 

```{r}
BasicNormalizedRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

NormalizedPcaRecipe <-
  BasicNormalizedRecipe |>
  step_pca(all_numeric_predictors(), num_comp = tune()) |>
  step_normalize(all_numeric_predictors())

NormalizedPlsRecipe <-
  BasicNormalizedRecipe |>
  step_pls(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())

NormalizedUmapRecipe <-
  BasicNormalizedRecipe |>
  step_umap(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    neighbors = tune(),
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())
```

## Defining recipes for tree based models

```{r}
ReducedLevelsRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_novel(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = tune()) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_nzv(all_predictors())
```

## Defining workflows to evaluate

```{r}
#| eval: false

# Group 1: Simple linear models (ligeros en memoria)
WorkFlowSimple <- workflow_set(
  preproc = list(
    normalized = BasicNormalizedRecipe
  ),
  models = list(
    reg_logistic = GlmnetSpec
  )
)

# Group 2: Dimensionality reduction models (intensivos en memoria)
WorkFlowDimReduction <- workflow_set(
  preproc = list(
    pca = NormalizedPcaRecipe,
    pls = NormalizedPlsRecipe,
    umap = NormalizedUmapRecipe
  ),
  models = list(
    flex_da = FdaSpec,
    mars = EarthSpec
  )
)

# Group 3: Tree-based models
WorkFlowTrees <- workflow_set(
  preproc = list(
    reduced_levels = ReducedLevelsRecipe
  ),
  models = list(
    random_forest = RangerSpec,
    xgboost = XgboostSpec
  )
)

pin_write(
  BoardLocal,
  WorkFlowSimple,
  "WorkFlowSimple",
  type = "qs2",
  title = "Work Flow Simple"
)

pin_write(
  BoardLocal,
  WorkFlowDimReduction,
  "WorkFlowDimReduction",
  type = "qs2",
  title = "Work Flow Dim Reduction"
)

pin_write(
  BoardLocal,
  WorkFlowTrees,
  "WorkFlowTrees",
  type = "qs2",
  title = "Work Flow Trees"
)
```

## Tuning grid for each workflow

Here is the code used to train and eval de different models.

```{r}
#| eval: false
#| file: ../multicore-scripts/03a-tuning-simple-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03b-tuning-dimreduction-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03c-tuning-tree-models.R
```

## Exploring results

Our initial model exploration revealed that **tree-based models**, specifically Random Forest and Bagged Trees, delivered the best performance. However, a regularized Logistic Regression was a strong and much simpler contender."

Let's import the results of the exploration.

```{r}
WorkFlowTuned <- c(
  pin_read(BoardLocal, "WorkFlowSimpleTuned"),
  pin_read(BoardLocal, "WorkFlowDimReductionTuned"),
  pin_read(BoardLocal, "WorkFlowTreesTuned")
)
```

Consolidate the results for one of the metrics.

```{r}
WorkFlowTunedBest <- lapply(
  names(WorkFlowTuned),
  FUN = \(x) {
    show_best(WorkFlowTuned[[x]], metric = "brier_class") |> mutate(model = x)
  }
) |>
  do.call(what = "bind_rows")
```

Now we can confirm that best models were `Logistic Regression` and `Random Forest` based on number of predictios correct, but we need to also take in consideration that relative cost of taking or reject the wrong trip can change from trip to trip and it's really important that the metric that selects the best model take that into consideration.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

WorkFlowTunedBest |>
  group_by(model) |>
  mutate(meadian_of_mean_score = quantile(mean, probs = 0.75)) |>
  ungroup() |>
  mutate(
    model = reorder(model, -mean, FUN = median),
    best_models = case_when(
      meadian_of_mean_score < 0.18 ~ "group1",
      meadian_of_mean_score < 0.23 ~ "group2",
      .default = "other"
    )
  ) |>
  ggplot(aes(model, mean)) +
  geom_boxplot(aes(fill = best_models)) +
  scale_fill_manual(
    values = c(
      "group1" = Params$ColorHighlight,
      "group2" = Params$ColorHighlightLow,
      "other" = Params$ColorGray
    )
  ) +
  coord_flip() +
  labs(
    title = paste0(
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Logistic Regression**",
      "</span> ",
      " and ",
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Random Forest**",
      "</span> "
    ),
    subtitle = "were the best models based on **Bier Score**",
    y = "Brier Score",
    x = "Trained Models"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.position = "none"
  )
```

To estimate the cost we are selecting the best model of each model type and take the difference of the performace per hour of each trip and the performance per hour or the 75 percentile depending the final value was correct o no.

```{r}
#| eval: false

collect_prediction_cost <- function(
  training_data,
  tuned_wf,
  model_name,
  threshold,
  metric = "brier_class"
) {
  NycTaxi::collect_predictions_best_config(tuned_wf, model_name, metric) |>
    NycTaxi::add_performance_variables(training_data = training_data) |>
    NycTaxi::add_pred_class(threshold) |>
    NycTaxi::calculate_costs()
}

PredictionsCosts <- lapply(
  names(WorkFlowTuned),
  FUN = collect_prediction_cost,
  threshold = 0.5,
  training_data = TrainingSampleJoined,
  tuned_wf = WorkFlowTuned
) |>
  bind_rows()

pin_write(
  BoardLocal,
  x = PredictionsCosts,
  name = "PredictionsCosts",
  type = "qs2",
  title = "Predictions Costs"
)
```

As the current method that the taxis are using in the simulation is to take all trips we can calculate that on average of how much money was lost due this strategy on the training data (around $4 per trip) and compore how that number could change if we just the best trained model for each model type.

The resuls were really surprising as the `Flexible Dicrim Analisys` was performing really similar to `Logistic Regression` and `Random Forest` (that was in 2nd place) it's almost producing the same results as taking al trips.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

PredictionsCosts <- pin_read(
  BoardLocal,
  name = "PredictionsCosts"
)

PredictionsCosts |>
  group_by(model_name) |>
  summarize(
    current_method_cost = mean(current_method_cost, na.rm = TRUE),
    model_method_cost = mean(cost_wrong_total, na.rm = TRUE)
  ) |>
  ungroup() |>
  mutate(
    total_benefit = current_method_cost - model_method_cost,
    model_name = reorder(model_name, total_benefit)
  ) |>
  ggplot(aes(model_method_cost, model_name)) +
  geom_segment(
    aes(x = current_method_cost, xend = model_method_cost),
    color = Params$ColorGray,
    linewidth = 1
  ) +
  geom_point(
    shape = 21,
    color = "black",
    fill = Params$ColorHighlight,
    stroke = 0.5,
    size = 3
  ) +
  scale_x_continuous(breaks = scales::breaks_width(1)) +
  expand_limits(x = 0) +
  labs(
    title = paste0(
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Logistic Regression**",
      "</span> ",
      "losses less money than current method"
    ),
    subtitle = "**Flexible DA** is also producing similar results",
    y = "Trained Models",
    x = "Average Cost Lost Due Wrong Selection"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none"
  )
```

```{r}
compare_model_predictions <- function(
  predictions_df,
  model1_name,
  model2_name,
  truth_col = "take_current_trip",
  pred_class_col = ".pred_class"
) {
  # Filter for the two models and calculate correctness
  comparison <- predictions_df |>
    filter(model_name %in% c(model1_name, model2_name)) |>
    mutate(correct = .data[[truth_col]] == .data[[pred_class_col]]) |>
    pivot_wider(
      id_cols = c("id", "trip_id", all_of(truth_col)),
      names_from = "model_name",
      values_from = c(".pred_yes", "correct")
    )

  # Create dynamic column names
  correct_col1 <- paste0("correct_", model1_name)
  correct_col2 <- paste0("correct_", model2_name)

  # Categorize predictions
  comparison |>
    mutate(
      correct_model = case_when(
        .data[[correct_col1]] & !.data[[correct_col2]] ~ model1_name,
        .data[[correct_col2]] & !.data[[correct_col1]] ~ model2_name,
        .data[[correct_col1]] & .data[[correct_col2]] ~ "Both Correct",
        TRUE ~ "Both Incorrect"
      )
    ) |>
    count(correct_model, sort = TRUE) |>
    mutate(percentage = scales::percent(n / sum(n), accuracy = 0.1))
}

ModelsToCompare <-
  PredictionsCosts |>
  pull(model_name) |>
  unique() |>
  setdiff(y = "normalized_reg_logistic")

lapply(
  ModelsToCompare,
  FUN = compare_model_predictions,
  predictions_df = PredictionsCosts,
  model1_name = "normalized_reg_logistic"
) |>
  bind_rows() |>
  filter(correct_model %in% ModelsToCompare) |>
  arrange(desc(n))
```

Logistic Regression and xgboost highlighted complementary strengths: logistic regression surfaces stable, global linear effects you can act on immediately (easy to inspect, calibrate, and deploy), while xgboost captures nonlinear interactions and local patterns that improve raw performance but need explainability and calibration for safe use.

```{r}
compare_model_predictions(
  predictions_df = PredictionsCosts,
  model1_name = "normalized_reg_logistic",
  model2_name = "reduced_levels_xgboost"
)
```

  ## Creating workflows to explore
  
  ```{r}
# eval: false

pin_write(
  BoardLocal,
  InitialFinalizedRandomForestWf,
  "InitialFinalizedRandomForestWf",
  type = "qs2",
  title = "Initial Finalized Random Forest Wf"
)

pin_write(
  BoardLocal,
  InitialFinalizedXgboostWf,
  "InitialFinalizedXgboostWf",
  type = "qs2",
  title = "Initial Finalized Xgboost Wf"
)

pin_write(
  BoardLocal,
  InitialFinalizedLogisticWf,
  "InitialFinalizedLogisticWf",
  type = "qs2",
  title = "Initial Finalized Logistic Wf"
)
```

## Conclusion

The initial model training and evaluation revealed two key findings:

1. **Tree-based models (`Random Forest` and `Xgboost`)** achieved the best performance, indicating they are well-suited to capture the complex patterns in the NYC taxi data.

2. **A simpler `Regularized Logistic Regression` model performed nearly as well**. This suggests that a strong linear relationship exists within the features, making it a highly efficient and interpretable candidate for a final model.

Therefore, the next steps should focus on two parallel paths: 

- Further tuning the `xgboost` models to maximize performance
- Investing in feature engineering to improve the already-competitive logistic model.