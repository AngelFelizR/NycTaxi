---
title: "Training Model to use"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

After exploring the data and engineering new features, the next step is to evaluate the data using Machine Learning models. Our goal is to extract insights that will inform the creation of the final production model.

In this section, we define the models to be trained. We have selected a diverse set of algorithms, ranging from simple, interpretable ones (like Logistic Regression) to complex, powerful ones (like Random Forests). This allows us to determine which approach best suits our data and hardware constraints:

- Regularized Logistic Regression via `glmnet`
- Flexible Discriminant Analysis (FDA)
- MARS via `earth`
- Bagged Trees via `rpart`
- Random Forests via `ranger`
- Boosted Trees via `xgboost`

We will fit 5 random variations of the tuning parameters over 5-fold cross-validation, then explore the results of each model.
    
## Setting up the environment

Here are the loaded libraries to start the process.

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(lubridate)
library(timeDate)
library(ggtext)

## Tools for modeling
library(tidymodels)
library(embed)
library(themis)
library(discrim)
library(DALEX)

## Publish data sets, models, and other R objects
library(pins)

## For table formatting
library(gt)

## Custom functions
devtools::load_all()

# Defining the pin boards to use
BoardLocal <- board_folder(here("../NycTaxiPins/Board"))

# Loading params
Params <- yaml::read_yaml(here("params.yml"))
Params$BoroughColors <- unlist(Params$BoroughColors)
```

## Training initial models

### Defining models to train

Now we can define the models to be trained and tuned.

```{r}
# Regularized logistic regression
GlmnetSpec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_mode("classification") |>
  set_engine("glmnet")

# Flexible discriminant analysis
FdaSpec <-
  discrim_flexible(prod_degree = tune()) |>
  set_mode("classification") |>
  set_engine('earth')

# MARS
EarthSpec <-
  mars(num_terms = tune(), prod_degree = tune()) |>
  set_mode("classification")

# Random forests
RangerSpec <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 250) |>
  set_mode("classification") |>
  set_engine("ranger")

# Boosted trees
XgboostSpec <-
  boost_tree(
    trees = 250,
    tree_depth = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune()
  ) |>
  set_mode("classification") |>
  set_engine("xgboost")
```

### Importing training data

Here we import the data to use from the remote Board located in a GitHub repo.

```{r}
#| eval: false

AcsVariablesByZoneId <-
  pin_read(BoardLocal, "AcsVariablesByZoneId")[,
    LocationID := as.character(LocationID)
  ]

OmsDensityFeatures <- pin_read(BoardLocal, "OmsDensityFeatures")[,
  LocationID := as.character(LocationID)
]

ZoneCodesRef <- pin_read(BoardLocal, "ZoneCodesRef")[, c(
  "LocationID",
  "Borough",
  "service_zone"
)]

SampledData <-
  pin_list(BoardLocal) |>
  grep(pattern = "^OneMonthData", value = TRUE) |>
  sort() |>
  head(12L) |>
  lapply(FUN = pin_read, board = BoardLocal) |>
  rbindlist() |>
  tibble::as_tibble() |>
  mutate(
    take_current_trip = fifelse(take_current_trip == 1L, "yes", "no") |>
      factor(levels = c("yes", "no")),
    PULocationID = as.character(PULocationID),
    DOLocationID = as.character(DOLocationID)
  )

set.seed(2545)
SampledDataSplit <- initial_split(SampledData, strata = take_current_trip)

TrainingSample <- training(SampledDataSplit)
TestingSample <- testing(SampledDataSplit)
```

### Adding all features to training data

We want to create a final recipe that reproduces the whole pipeline. We start by creating a recipe that combines all features used to train the model. This ensures that when we run `predict`, we only need to provide the basic data from the original source of [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), and all other features will be added as part of the `tidymodels` workflow.

```{r}
#| eval: false

ConsolidationRecipe <-
  # Starting Recipe
  recipe(
    take_current_trip ~
      PULocationID +
      DOLocationID +
      wav_match_flag +
      hvfhs_license_num +
      trip_miles +
      trip_time +
      driver_pay +
      request_datetime +
      trip_id +
      performance_per_hour +
      percentile_75_performance,
    data = TrainingSample
  ) |>

  # Updating roles of variables important for trip identification
  update_role(
    trip_id,
    performance_per_hour,
    percentile_75_performance,
    new_role = "additional info"
  ) |>

  # Selecting variables over 2 min
  step_filter(trip_time >= (60 * 2)) |>

  # Renaming variables to join
  step_rename(PU_LocationID = PULocationID, DO_LocationID = DOLocationID) |>

  # Adding Geospatial Data
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = ZoneCodesRef,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = AcsVariablesByZoneId,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = OmsDensityFeatures,
    col_prefix = c("DO_", "PU_")
  ) |>

  # Transforming strings to factors
  step_string2factor(all_string_predictors()) |>

  # Daily cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_daily = request_datetime_sin_1,
    request_datetime_cos_daily = request_datetime_cos_1
  ) |>

  # Weekly cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24 * 7,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_weekly = request_datetime_sin_1,
    request_datetime_cos_weekly = request_datetime_cos_1
  ) |>

  # Extracting additional information
  step_date(
    request_datetime,
    features = c(
      "year",
      "week",
      "decimal",
      "semester",
      "quarter",
      "doy",
      "dow",
      "mday",
      "month"
    )
  ) |>

  step_holiday(
    request_datetime,
    holidays = c(
      'USChristmasDay',
      'USColumbusDay',
      'USCPulaskisBirthday',
      'USDecorationMemorialDay',
      'USElectionDay',
      'USGoodFriday',
      'USInaugurationDay',
      'USIndependenceDay',
      'USJuneteenthNationalIndependenceDay',
      'USLaborDay',
      'USLincolnsBirthday',
      'USMemorialDay',
      'USMLKingsBirthday',
      'USNewYearsDay',
      'USPresidentsDay',
      'USThanksgivingDay',
      'USVeteransDay',
      'USWashingtonsBirthday'
    )
  ) |>

  step_mutate(
    .pkgs = c("data.table", "lubridate", "timeDate"),

    company = fcase(
      hvfhs_license_num == "HV0002" ,
      "Juno"                        ,
      hvfhs_license_num == "HV0003" ,
      "Uber"                        ,
      hvfhs_license_num == "HV0004" ,
      "Via"                         ,
      hvfhs_license_num == "HV0005" ,
      "Lyft"                        ,
      default = "New"
    ) |>
      as.factor(),

    request_datetime_am = am(request_datetime) |> as.integer(),
    request_datetime_pm = pm(request_datetime) |> as.integer(),

    `Days to USChristmasDay` = difftime(
      USChristmasDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USColumbusDay` = difftime(
      USColumbusDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USCPulaskisBirthday` = difftime(
      USCPulaskisBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USDecorationMemorialDay` = difftime(
      USDecorationMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USElectionDay` = difftime(
      USElectionDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USGoodFriday` = difftime(
      USGoodFriday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USInaugurationDay` = difftime(
      USInaugurationDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USIndependenceDay` = difftime(
      USIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USJuneteenthNationalIndependenceDay` = difftime(
      USJuneteenthNationalIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLaborDay` = difftime(
      USLaborDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLincolnsBirthday` = difftime(
      USLincolnsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMemorialDay` = difftime(
      USMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMLKingsBirthday` = difftime(
      USMLKingsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USNewYearsDay` = difftime(
      USNewYearsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USPresidentsDay` = difftime(
      USPresidentsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USThanksgivingDay` = difftime(
      USThanksgivingDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USVeteransDay` = difftime(
      USVeteransDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USWashingtonsBirthday` = difftime(
      USWashingtonsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer()
  ) |>

  # Removing variables
  step_rm(ends_with(c(
    "LocationID",
    "request_datetime",
    "hvfhs_license_num"
  )))

pin_write(
  BoardLocal,
  ConsolidationRecipe,
  "ConsolidationRecipe",
  type = "qs2",
  title = "Consolidation Recipe"
)
```

As we are planning to train many models, it is efficient to apply these initial recipes to the training and testing data once. This avoids repeating the same steps for each training resample.

```{r}
#| eval: false

TrainingSampleJoined <-
  prep(ConsolidationRecipe) |>
  bake(new_data = NULL)

pin_write(
  BoardLocal,
  TrainingSampleJoined,
  "TrainingSampleJoined",
  type = "qs2",
  title = "Training Sample Joined"
)
```

```{r}
#| echo: false
#| output: false

TrainingSampleJoined <- pin_read(BoardLocal, "TrainingSampleJoined")
```

### Defining recipes based on model type

Before creating the first recipe, it is important to consider that we want to keep `performance_per_hour` and `percentile_75_performance` available to calculate how much money we could be losing due to incorrect predictions.

```{r}
start_recipe <- function(df) {
  new_recipe =
    recipe(take_current_trip ~ ., data = df) |>
    step_downsample(take_current_trip, under_ratio = 1) |>
    step_impute_median(all_numeric_predictors()) |>
    update_role(
      trip_id,
      performance_per_hour,
      percentile_75_performance,
      new_role = "additional info"
    )

  return(new_recipe)
}
```

The models trained on this data are very affected by class imbalance. We will downsample to avoid these problems and speed up training time. Note that we have many examples of both cases and are looking for general rules that could be used by taxi drivers.

```{r}
BasicNormalizedRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

NormalizedPcaRecipe <-
  BasicNormalizedRecipe |>
  step_pca(all_numeric_predictors(), num_comp = tune()) |>
  step_normalize(all_numeric_predictors())

NormalizedPlsRecipe <-
  BasicNormalizedRecipe |>
  step_pls(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())

NormalizedUmapRecipe <-
  BasicNormalizedRecipe |>
  step_umap(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    neighbors = tune(),
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())
```

As tree based models are affected due having too many categories and xgboost needs numeric predictors here is the need changes.

```{r}
ReducedLevelsRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_novel(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = tune()) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_nzv(all_predictors())
```

### Consolidating workflows to evaluate

```{r}
#| eval: false

# Group 1: Simple linear models (light on memory)
WorkFlowSimple <- workflow_set(
  preproc = list(
    normalized = BasicNormalizedRecipe
  ),
  models = list(
    reg_logistic = GlmnetSpec
  )
)

# Group 2: Dimensionality reduction models (memory intensive)
WorkFlowDimReduction <- workflow_set(
  preproc = list(
    pca = NormalizedPcaRecipe,
    pls = NormalizedPlsRecipe,
    umap = NormalizedUmapRecipe
  ),
  models = list(
    flex_da = FdaSpec,
    mars = EarthSpec
  )
)

# Group 3: Tree-based models
WorkFlowTrees <- workflow_set(
  preproc = list(
    reduced_levels = ReducedLevelsRecipe
  ),
  models = list(
    random_forest = RangerSpec,
    xgboost = XgboostSpec
  )
)

pin_write(
  BoardLocal,
  WorkFlowSimple,
  "WorkFlowSimple",
  type = "qs2",
  title = "Work Flow Simple"
)

pin_write(
  BoardLocal,
  WorkFlowDimReduction,
  "WorkFlowDimReduction",
  type = "qs2",
  title = "Work Flow Dim Reduction"
)

pin_write(
  BoardLocal,
  WorkFlowTrees,
  "WorkFlowTrees",
  type = "qs2",
  title = "Work Flow Trees"
)
```

### Tuning grid for each workflow

Here is the code used to train and evaluate the different models.

```{r}
#| eval: false
#| file: ../multicore-scripts/03a-tuning-simple-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03b-tuning-dimreduction-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03c-tuning-tree-models.R
```

### Exploring results

Our initial model exploration revealed that **tree-based models**, specifically Random Forest and Bagged Trees, delivered the best raw performance. However, a regularized Logistic Regression was a strong and much simpler contender.

Let's import the results of the exploration.

```{r}
#| eval: false

WorkFlowTuned <- c(
  pin_read(BoardLocal, "WorkFlowSimpleTuned"),
  pin_read(BoardLocal, "WorkFlowDimReductionTuned"),
  pin_read(BoardLocal, "WorkFlowTreesTuned")
)


WorkFlowTunedBest <- lapply(
  names(WorkFlowTuned),
  FUN = \(x) {
    show_best(WorkFlowTuned[[x]], metric = "brier_class") |> mutate(model = x)
  }
) |>
  bind_rows()

pin_write(
  BoardLocal,
  WorkFlowTunedBest,
  "WorkFlowTunedBest",
  type = "qs2",
  title = "Work Flow Tuned Best"
)
```

Now we can confirm that the best models were `Logistic Regression` and `Random Forest` based on the number of correct predictions. However, we also need to consider that the relative cost of taking or rejecting the wrong trip can change from trip to trip. It is crucial that the metric used to select the best model takes these economic implications into consideration.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

pin_read(
  BoardLocal,
  "WorkFlowTunedBest"
) |>
  group_by(model) |>
  mutate(meadian_of_mean_score = quantile(mean, probs = 0.75)) |>
  ungroup() |>
  mutate(
    model = reorder(model, -mean, FUN = median),
    best_models = case_when(
      meadian_of_mean_score < 0.18 ~ "group1",
      meadian_of_mean_score < 0.23 ~ "group2",
      .default = "other"
    )
  ) |>
  ggplot(aes(model, mean)) +
  geom_boxplot(aes(fill = best_models)) +
  scale_fill_manual(
    values = c(
      "group1" = Params$ColorHighlight,
      "group2" = Params$ColorHighlightLow,
      "other" = Params$ColorGray
    )
  ) +
  coord_flip() +
  labs(
    title = paste0(
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Logistic Regression**",
      "</span> ",
      " and ",
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Random Forest**",
      "</span> "
    ),
    subtitle = "were the best models based on **Brier Score**",
    y = "Brier Score",
    x = "Trained Models"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.position = "none"
  )
```

To estimate the cost, we select the best model of each type and calculate the difference between the `performance_per_hour` of each trip and the `performance_per_hour` of the 75th percentile, depending on whether the final prediction was correct or not.

```{r}
#| eval: false

collect_prediction_cost <- function(
  training_data,
  tuned_wf,
  model_name,
  threshold,
  metric = "brier_class"
) {
  NycTaxi::collect_predictions_best_config(tuned_wf, model_name, metric) |>
    NycTaxi::add_performance_variables(training_data = training_data) |>
    NycTaxi::add_pred_class(threshold) |>
    NycTaxi::calculate_costs()
}

PredictionsCosts <- lapply(
  names(WorkFlowTuned),
  FUN = collect_prediction_cost,
  threshold = 0.5,
  training_data = TrainingSampleJoined,
  tuned_wf = WorkFlowTuned
) |>
  bind_rows()

pin_write(
  BoardLocal,
  x = PredictionsCosts,
  name = "PredictionsCosts",
  type = "qs2",
  title = "Predictions Costs"
)
```

The current method taxis use in the simulation is to accept all trips. We can calculate that, on average, this strategy resulted in a loss of around $3.8 per trip on the training data. We can now compare how that number would change if we used the best trained model for each model type.

Its really important to highlight the following points:

- **normalized_reg_logistic** was the best model in terms of **Brier Score** and cost reduction.
- **reduced_levels_random_forest** and **reduced_levels_xgboost** regardless of performing similarly to *normalized_reg_logistic* in the **Brier Score** didn't achieve similar cost reduction.
- **pls_mars** regardless of presenting a relatively high **Brier Score** presented similar cost reduction to the *normalized_reg_logistic*.

```{r}
PredictionsCosts <- pin_read(
  BoardLocal,
  name = "PredictionsCosts"
)

PredictionsCostsSummary =
  PredictionsCosts |>
  # Defining cost of model and fold
  group_by(model_name, id) |>
  summarize(
    current_method_cost = mean(current_method_cost, na.rm = TRUE),
    model_method_cost = mean(cost_wrong_total, na.rm = TRUE)
  ) |>
  # Defining cost of model across folds
  summarize(
    current_method_cost = mean(current_method_cost),
    model_method_cost = mean(model_method_cost)
  ) |>
  mutate(
    total_benefit = current_method_cost - model_method_cost,
    model_name = reorder(model_name, total_benefit)
  ) |>
  arrange(desc(total_benefit))

PredictionsCostsSummary
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"

PredictionsCostsSummary |>
  ggplot(aes(model_method_cost, model_name)) +
  geom_segment(
    aes(x = current_method_cost, xend = model_method_cost),
    color = Params$ColorGray,
    linewidth = 1
  ) +
  geom_point(
    shape = 21,
    color = "black",
    fill = Params$ColorHighlight,
    stroke = 0.5,
    size = 3
  ) +
  scale_x_continuous(breaks = scales::breaks_width(1)) +
  expand_limits(x = 0) +
  labs(
    title = paste0(
      "**Only** <span style='color:",
      Params$ColorHighlight,
      ";'>",
      "Xgboost",
      "</span> ",
      "is better than current method"
    ),
    y = "Trained Models",
    x = "Average Cost Lost Due Wrong Selection"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none"
  )
```

## Fine tunning Xgboost


```{r}
# Especificaci칩n del modelo XGBoost con par치metros a tunear
XgboostSpecToTune <-
  boost_tree(
    trees = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune()
  ) |>
  set_mode("classification") |>
  # Single-thread to leverage fold parallelization
  set_engine("xgboost", nthread = 1)

# Extraer y actualizar el workflow
WorkFlowXgboost <-
  pins::pin_read(BoardLocal, "WorkFlowTrees") |>
  extract_workflow(id = "reduced_levels_xgboost") |>
  update_model(XgboostSpecToTune)
```

```{r}
#| eval: false

# Setting very narrow ranges centered around the best-performing iteration (Iter 9):
# mtry=96, trees=221, min_n=18, tree_depth=6, learn_rate=0.104, loss_reduction=0.000556, sample_size=0.784
XgboostParamsRefined <-
  extract_parameter_set_dials(WorkFlowXgboost) |>
  update(
    # Search range slightly above and below 221
    trees = trees(range = c(200L, 350L)),

    # Search range slightly above and below 18
    min_n = min_n(range = c(15L, 25L)),

    # Focused range around 96 (Ensure it doesn't exceed total features - 1)
    mtry = mtry(range = c(70L, min(100L, ncol(TrainingSampleJoined) - 1L))),

    # Focused range around 6 (allowing for slightly deeper or shallower trees)
    tree_depth = tree_depth(range = c(5L, 8L)),

    # Linear search range around 0.104
    learn_rate = learn_rate(range = c(0.05, 0.20), trans = NULL),

    # Focused range around log10(0.000556) which is about -3.25
    loss_reduction = loss_reduction(range = c(-4, -3)),

    # Focused range around 0.784
    sample_size = sample_prop(range = c(0.70, 0.90))
  )

# Defining resamples
set.seed(5878)
TrainingSampleJoinedResamples <- vfold_cv(TrainingSampleJoined, v = 5)

# Optimizaci칩n Bayesiana
set.seed(91254)
WorkFlowXgboostTuned <-
  tryCatch(
    WorkFlowXgboost |>
      tune_bayes(
        resamples = TrainingSampleJoinedResamples,
        metrics = metric_set(brier_class, roc_auc),
        initial = 10, # Start with 10 random points (recommended)
        param_info = XgboostParamsRefined, # Allow 25 Bayesian optimization iterations to find a better peak
        iter = 25L,

        control = control_bayes(
          verbose = TRUE,
          verbose_iter = TRUE,
          seed = 91254,
          save_pred = FALSE,

          # Parallelize folds (requires xgboost nthread=1)
          parallel_over = "resamples",

          # Allow more iterations without improvement before stopping
          no_improve = 20L,

          # Explore uncertain regions
          uncertain = 5L,

          extract = NULL
        )
      ),
    error = function(e) {
      message("Error during tuning: ", e$message)
      return(NULL)
    }
  )


# Process results if tuning succeeded
if (!is.null(WorkFlowXgboostTuned)) {
  BestXgboostConfig <- select_best(WorkFlowXgboostTuned, metric = "brier_class")
  MetricsXgboostHistory <- collect_metrics(WorkFlowXgboostTuned)

  pin_write(
    BoardLocal,
    x = BestXgboostConfig,
    name = "BestXgboostConfig",
    type = "qs2",
    title = "Best Xgboost Config"
  )

  pin_write(
    BoardLocal,
    x = MetricsXgboostHistory,
    name = "MetricsXgboostHistory",
    type = "qs2",
    title = "Metrics Xgboost History"
  )
}
```

### Comparting results

Here is the initial metric.

```{r}
pin_read(BoardLocal, "WorkFlowTreesTuned")[["reduced_levels_xgboost"]] |>
  collect_metrics() |>
  filter(.metric == "brier_class") |>
  select(mtry:threshold, mean, std_err) |>
  arrange(mean) |>
  head(3L)
```


```{r}
MetricsXgboostHistory = pin_read(
  BoardLocal,
  name = "MetricsXgboostHistory"
)

MetricsXgboostHistory |>
  filter(.metric == "brier_class") |>
  select(mtry:threshold, mean, std_err, .iter) |>
  arrange(mean) |>
  head(3L)
```

## Exploring variable importance

```{r}
#| output: false
#| echo: false

rm(list = grep("^WorkFlowTuned$|^(Normalized|Basic)", ls(), value = TRUE))
gc()
```

Now we can explit the training data to validate what the the most important features behind the predictions.

```{r}
set.seed(2971)
ExplainerSplit <-
  initial_split(TrainingSampleJoined, strata = take_current_trip)

ExplainerTraining <- training(ExplainerSplit)
ExplainerTesting <- testing(ExplainerSplit)
```

We need to fit a new model based on this data.

```{r}
#| eval: false

BestXgboostConfig = pin_read(
  BoardLocal,
  name = "BestXgboostConfig"
)

XgboostWfFitted <- WorkFlowXgboost |>
  finalize_workflow(parameters = BestXgboostConfig) |>
  fit(data = ExplainerTraining)

pin_write(
  BoardLocal,
  x = XgboostWfFitted,
  name = "XgboostWfFitted",
  type = "qs2",
  title = "Xgboost Wf Fitted"
)
```

Then create the explainer for `DALEX`.

```{r}
XgboostWfFitted <- pin_read(BoardLocal, "XgboostWfFitted")

ExplainerXgboost <- DALEXtra::explain_tidymodels(
  XgboostWfFitted,
  data = ExplainerTesting |> select(-take_current_trip),
  y = ExplainerTesting$take_current_trip
)
```

Now we can evaluate how the model performs after permutating each of the variables.

```{r}
#| eval: false

set.seed(1980)
ExplainerXgboostVip <- model_parts(
  explainer = ExplainerXgboost,
  loss_function = get_loss_yardstick(brier_class),
  B = 50,
  type = "difference"
)

pin_write(
  BoardLocal,
  x = ExplainerXgboostVip,
  name = "ExplainerXgboostVip",
  type = "qs2",
  title = "Explainer Xgboost Vip"
)
```

The results are interesting as the model didn't show strong positive but strong negative predictors as we can see the after comparting the top 10 of positive predictors and the top 20 of negative predictors.

```{r}
ExplainerXgboostVip <- pin_read(BoardLocal, "ExplainerXgboostVip")

ExplainerXgboostVipSummary <-
  as.data.frame(ExplainerXgboostVip) |>
  group_by(variable) |>
  summarize(dropout_loss = mean(dropout_loss)) |>
  arrange(desc(dropout_loss))

ExplainerXgboostVipSummary |>
  (\(x) bind_rows(head(x, 10L), tail(x, 20L)))() |>
  as.data.frame()
```

### Removing variables with negative importance

Based on this results let's validate the if the brier class score decreasing after removing all negative variables.

```{r}
AllVariablesToRemove <-
  ExplainerXgboostVipSummary |>
  filter(dropout_loss < 0) |>
  pull(variable) |>
  head(n = -1L)

AllVariablesToRemove
```

```{r}
#| eval: false

BestXgboostConfig = pin_read(
  BoardLocal,
  name = "BestXgboostConfig"
)

set.seed(65416)
TrainingSampleJoinedResamples <- vfold_cv(
  TrainingSampleJoined,
  v = 5,
  strata = take_current_trip
)

RemovingAllRecipe <-
  TrainingSampleJoined |>
  select(-all_of(AllVariablesToRemove)) |>
  start_recipe() |>
  step_novel(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = tune()) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_nzv(all_predictors())

WorkFlowXgboostRevAll <-
  WorkFlowXgboost |>
  update_recipe(RemovingAllRecipe) |>
  finalize_workflow(BestXgboostConfig)

XgboostRevAllEvaluted <- fit_resamples(
  WorkFlowXgboostRevAll,
  resamples = TrainingSampleJoinedResamples,
  metrics = metric_set(brier_class),
  control = control_resamples(
    verbose = TRUE,
    allow_par = FALSE,
    save_pred = TRUE
  )
)

pin_write(
  BoardLocal,
  x = XgboostRevAllEvaluted,
  name = "XgboostRevAllEvaluted",
  type = "qs2",
  title = "Xgboost Rev All Evaluted"
)
```

After evaluating the resamples we can confirm that the **108 variables** ere not needed to keep the same level of predictions, which is a great new  due the computation constrains we have.

```{r}
MetricsXgboostHistory <- pin_read(
  BoardLocal,
  name = "MetricsXgboostHistory"
)

XgboostRevAllEvaluted <- pin_read(
  BoardLocal,
  name = "XgboostRevAllEvaluted"
)

estimate_prediction_benefit <- function(evaluted_model, thresholdl = 0.5) {
  collect_predictions(evaluted_model) |>
    NycTaxi::add_performance_variables(training_data = TrainingSampleJoined) |>
    NycTaxi::add_pred_class(0.5) |>
    NycTaxi::calculate_costs() |>
    group_by(id) |>
    summarize(
      current_method_cost = mean(current_method_cost, na.rm = TRUE),
      model_method_cost = mean(cost_wrong_total, na.rm = TRUE)
    ) |>
    # Defining cost of model across folds
    summarize(
      current_method_cost = mean(current_method_cost),
      model_method_cost = mean(model_method_cost)
    ) |>
    mutate(
      total_benefit = current_method_cost - model_method_cost
    )
}

MetricsXgboostHistory |>
  filter(.metric == "brier_class") |>
  select(mean, std_err, .iter) |>
  arrange(mean) |>
  head(5L)

collect_metrics(XgboostRevAllEvaluted)

estimate_prediction_benefit(XgboostRevAllEvaluted)

```

#### Variable importance of new model

```{r}
#| eval: false

XgboostWfRevAllFitted <- fit(WorkFlowXgboostRevAll, data = ExplainerTraining)

pin_write(
  BoardLocal,
  x = XgboostWfRevAllFitted,
  name = "XgboostWfRevAllFitted",
  type = "qs2",
  title = "Xgboost Wf Rev All Fitted"
)
```


```{r}
XgboostWfRevAllFitted <- pin_read(BoardLocal, "XgboostWfRevAllFitted")

ExplainerTestingRevAll <-
  ExplainerTesting |>
  select(-all_of(c(AllVariablesToRemove, "take_current_trip")))

ExplainerXgboostRevAll <- DALEXtra::explain_tidymodels(
  XgboostWfRevAllFitted,
  data = ExplainerTestingRevAll,
  y = ExplainerTesting$take_current_trip
)
```

```{r}
#| eval: false

set.seed(1980)
ExplainerXgboostRevAllVip <- model_parts(
  explainer = ExplainerXgboostRevAll,
  loss_function = get_loss_yardstick(brier_class),
  B = 50,
  type = "difference"
)

pin_write(
  BoardLocal,
  x = ExplainerXgboostRevAllVip,
  name = "ExplainerXgboostRevAllVip",
  type = "qs2",
  title = "Explainer Xgboost Rev All Vip"
)
```

```{r}
ExplainerXgboostRevAllVip <- pin_read(BoardLocal, "ExplainerXgboostRevAllVip")

ExplainerXgboostVipRevAllSummary <-
  as.data.frame(ExplainerXgboostRevAllVip) |>
  group_by(variable) |>
  summarize(dropout_loss = mean(dropout_loss)) |>
  arrange(desc(dropout_loss))

ExplainerXgboostVipRevAllSummary |>
  (\(x) bind_rows(head(x, 10L), tail(x, 20L)))() |>
  as.data.frame()
```

As we have new negative variables let's remove the new negative variables.

### Removing new variables with negative importance

```{r}
AllVariablesToRemove2 <-
  ExplainerXgboostVipRevAllSummary |>
  filter(dropout_loss < 0) |>
  pull(variable) |>
  head(n = -1L)

AllVariablesToRemove2
```


```{r}
#| eval: false

BestXgboostConfig = pin_read(
  BoardLocal,
  name = "BestXgboostConfig"
)

set.seed(65416)
TrainingSampleJoinedResamples <- vfold_cv(
  TrainingSampleJoined,
  v = 5,
  strata = take_current_trip
)

RemovingAllRecipe2 <-
  TrainingSampleJoined |>
  select(-all_of(c(AllVariablesToRemove, AllVariablesToRemove2))) |>
  start_recipe() |>
  step_novel(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = tune()) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_nzv(all_predictors())

WorkFlowXgboost2 <-
  WorkFlowXgboost |>
  update_recipe(RemovingAllRecipe2)

XgboostRevAllEvaluted2 <-
  WorkFlowXgboost2 |>
  finalize_workflow(BestXgboostConfig) |>
  fit_resamples(
    WorkFlowXgboostRevAll2,
    resamples = TrainingSampleJoinedResamples,
    metrics = metric_set(brier_class),
    control = control_resamples(
      verbose = TRUE,
      allow_par = FALSE,
      save_pred = TRUE
    )
  )

pin_write(
  BoardLocal,
  x = WorkFlowXgboost2,
  name = "WorkFlowXgboost2",
  type = "qs2",
  title = "Work Flow Xgboost 2"
)

pin_write(
  BoardLocal,
  x = XgboostRevAllEvaluted2,
  name = "XgboostRevAllEvaluted2",
  type = "qs2",
  title = "Xgboost Rev All Evaluted2"
)
```

After evaluating the resamples we can confirm that the **108 variables** ere not needed to keep the same level of predictions, which is a great new  due the computation constrains we have.

```{r}
XgboostRevAllEvaluted2 <- pin_read(
  BoardLocal,
  name = "XgboostRevAllEvaluted2"
)

collect_metrics(XgboostRevAllEvaluted2)

estimate_prediction_benefit(XgboostRevAllEvaluted2)

```

#### Fine tuning the new model

```{r}
#| eval: false

WorkFlowXgboost2 <- pin_read(
  BoardLocal,
  name = "WorkFlowXgboost2"
)

XgboostParamsRefined2 <-
  extract_parameter_set_dials(WorkFlowXgboost2) |>
  update(
    # Search range slightly above and below 221
    trees = trees(range = c(200L, 350L)),

    # Search range slightly above and below 18
    min_n = min_n(range = c(15L, 25L)),

    # Focused range around 96 (Ensure it doesn't exceed total features - 1)
    mtry = mtry(range = c(70L, min(100L, ncol(TrainingSampleJoined) - 1L))),

    # Focused range around 6 (allowing for slightly deeper or shallower trees)
    tree_depth = tree_depth(range = c(5L, 8L)),

    # Linear search range around 0.104
    learn_rate = learn_rate(range = c(0.05, 0.20), trans = NULL),

    # Focused range around log10(0.000556) which is about -3.25
    loss_reduction = loss_reduction(range = c(-4, -3)),

    # Focused range around 0.784
    sample_size = sample_prop(range = c(0.70, 0.90))
  )

# Defining resamples
set.seed(5878)
TrainingSampleJoinedResamples <- vfold_cv(TrainingSampleJoined, v = 5)

# Optimizaci칩n Bayesiana
set.seed(91254)
WorkFlowXgboostTuned <-
  tryCatch(
    WorkFlowXgboost2 |>
      tune_bayes(
        resamples = TrainingSampleJoinedResamples,
        metrics = metric_set(brier_class, roc_auc),
        initial = 10, # Start with 10 random points (recommended)
        param_info = XgboostParamsRefined2, # Allow 25 Bayesian optimization iterations to find a better peak
        iter = 25L,

        control = control_bayes(
          verbose = TRUE,
          verbose_iter = TRUE,
          seed = 91254,
          save_pred = FALSE,

          # Parallelize folds (requires xgboost nthread=1)
          parallel_over = "resamples",

          # Allow more iterations without improvement before stopping
          no_improve = 20L,

          # Explore uncertain regions
          uncertain = 5L,

          extract = NULL
        )
      ),
    error = function(e) {
      message("Error during tuning: ", e$message)
      return(NULL)
    }
  )


# Process results if tuning succeeded
if (!is.null(WorkFlowXgboostTuned)) {
  BestXgboostConfig2 <- select_best(
    WorkFlowXgboostTuned,
    metric = "brier_class"
  )
  MetricsXgboostHistory2 <- collect_metrics(WorkFlowXgboostTuned)

  pin_write(
    BoardLocal,
    x = BestXgboostConfig2,
    name = "BestXgboostConfig2",
    type = "qs2",
    title = "Best Xgboost Config2"
  )

  pin_write(
    BoardLocal,
    x = MetricsXgboostHistory2,
    name = "MetricsXgboostHistory2",
    type = "qs2",
    title = "Metrics Xgboost History2"
  )
}
```

Even after fine tuning the model the metrics keep working worst that the model after first removal

```{r}
MetricsXgboostHistory2 <- pin_read(
  BoardLocal,
  name = "MetricsXgboostHistory2"
)

MetricsXgboostHistory2 |>
  filter(.metric == "brier_class") |>
  select(mean, std_err, .iter) |>
  arrange(mean) |>
  head(5L)
```

```{r}
ExplainerXgboostVipRevAllSummary |>
  mutate(dropout_loss = round(dropout_loss, 5)) |>
  filter(dropout_loss > 0)
```


### Defining the best theosold




