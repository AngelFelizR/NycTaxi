---
title: "Training Initial Models"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

After exploring the data and engineering new features, the next step is to evaluate the data using Machine Learning models. Our goal is to extract insights that will inform the creation of the final production model.

In this section, we define the models to be trained. We have selected a diverse set of algorithms, ranging from simple, interpretable ones (like Logistic Regression) to complex, powerful ones (like Random Forests). This allows us to determine which approach best suits our data and hardware constraints:

- Regularized Logistic Regression via `glmnet`
- Flexible Discriminant Analysis (FDA)
- MARS via `earth`
- Bagged Trees via `rpart`
- Random Forests via `ranger`
- Boosted Trees via `xgboost`

We will fit 5 random variations of the tuning parameters over 5-fold cross-validation, then explore the results of each model.
    
## Setting up the environment

Here are the loaded libraries to start the process.

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(lubridate)
library(timeDate)
library(ggtext)

## Tools for modeling
library(tidymodels)
library(embed)
library(themis)
library(discrim)

## Publish data sets, models, and other R objects
library(pins)

## For table formatting
library(gt)

## Custom functions
devtools::load_all()

# Defining the pin boards to use
BoardLocal <- board_folder(here("../NycTaxiPins/Board"))

# Loading params
Params <- yaml::read_yaml(here("params.yml"))
Params$BoroughColors <- unlist(Params$BoroughColors)
```

## Defining models to train

Now we can define the models to be trained and tuned.

```{r}
# Regularized logistic regression
GlmnetSpec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_mode("classification") |>
  set_engine("glmnet")

# Flexible discriminant analysis
FdaSpec <-
  discrim_flexible(prod_degree = tune()) |>
  set_mode("classification") |>
  set_engine('earth')

# MARS
EarthSpec <-
  mars(num_terms = tune(), prod_degree = tune()) |>
  set_mode("classification")

# Random forests
RangerSpec <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 250) |>
  set_mode("classification") |>
  set_engine("ranger")

# Boosted trees
XgboostSpec <-
  boost_tree(
    trees = 250,
    tree_depth = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune()
  ) |>
  set_mode("classification") |>
  set_engine("xgboost")
```

## Importing data

Here we import the data to use from the remote Board located in a GitHub repo.

```{r}
#| eval: false

AcsVariablesByZoneId <-
  pin_read(BoardLocal, "AcsVariablesByZoneId")[,
    LocationID := as.character(LocationID)
  ]

OmsDensityFeatures <- pin_read(BoardLocal, "OmsDensityFeatures")[,
  LocationID := as.character(LocationID)
]

ZoneCodesRef <- pin_read(BoardLocal, "ZoneCodesRef")[, c(
  "LocationID",
  "Borough",
  "service_zone"
)]

SampledData <-
  pin_list(BoardLocal) |>
  grep(pattern = "^OneMonthData", value = TRUE) |>
  sort() |>
  head(12L) |>
  lapply(FUN = pin_read, board = BoardLocal) |>
  rbindlist() |>
  tibble::as_tibble() |>
  mutate(
    take_current_trip = fifelse(take_current_trip == 1L, "yes", "no") |>
      factor(levels = c("yes", "no")),
    PULocationID = as.character(PULocationID),
    DOLocationID = as.character(DOLocationID)
  )

set.seed(2545)
SampledDataSplit <- initial_split(SampledData, strata = take_current_trip)

TrainingSample <- training(SampledDataSplit)
TestingSample <- testing(SampledDataSplit)
```

## Creating consolidation recipe

We want to create a final recipe that reproduces the whole pipeline. We start by creating a recipe that combines all features used to train the model. This ensures that when we run `predict`, we only need to provide the basic data from the original source of [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), and all other features will be added as part of the `tidymodels` workflow.

```{r}
#| eval: false

ConsolidationRecipe <-
  # Starting Recipe
  recipe(
    take_current_trip ~
      PULocationID +
      DOLocationID +
      wav_match_flag +
      hvfhs_license_num +
      trip_miles +
      trip_time +
      driver_pay +
      request_datetime +
      trip_id +
      performance_per_hour +
      percentile_75_performance,
    data = TrainingSample
  ) |>

  # Updating roles of variables important for trip identification
  update_role(
    trip_id,
    performance_per_hour,
    percentile_75_performance,
    new_role = "additional info"
  ) |>

  # Selecting variables over 2 min
  step_filter(trip_time >= (60 * 2)) |>

  # Renaming variables to join
  step_rename(PU_LocationID = PULocationID, DO_LocationID = DOLocationID) |>

  # Adding Geospatial Data
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = ZoneCodesRef,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = AcsVariablesByZoneId,
    col_prefix = c("DO_", "PU_")
  ) |>
  step_join_geospatial_features(
    ends_with("LocationID"),
    spatial_features = OmsDensityFeatures,
    col_prefix = c("DO_", "PU_")
  ) |>

  # Transforming strings to factors
  step_string2factor(all_string_predictors()) |>

  # Daily cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_daily = request_datetime_sin_1,
    request_datetime_cos_daily = request_datetime_cos_1
  ) |>

  # Weekly cycle
  step_harmonic(
    request_datetime,
    frequency = 1,
    cycle_size = 3600 * 24 * 7,
    keep_original_cols = TRUE
  ) |>
  step_rename(
    request_datetime_sin_weekly = request_datetime_sin_1,
    request_datetime_cos_weekly = request_datetime_cos_1
  ) |>

  # Extracting additional information
  step_date(
    request_datetime,
    features = c(
      "year",
      "week",
      "decimal",
      "semester",
      "quarter",
      "doy",
      "dow",
      "mday",
      "month"
    )
  ) |>

  step_holiday(
    request_datetime,
    holidays = c(
      'USChristmasDay',
      'USColumbusDay',
      'USCPulaskisBirthday',
      'USDecorationMemorialDay',
      'USElectionDay',
      'USGoodFriday',
      'USInaugurationDay',
      'USIndependenceDay',
      'USJuneteenthNationalIndependenceDay',
      'USLaborDay',
      'USLincolnsBirthday',
      'USMemorialDay',
      'USMLKingsBirthday',
      'USNewYearsDay',
      'USPresidentsDay',
      'USThanksgivingDay',
      'USVeteransDay',
      'USWashingtonsBirthday'
    )
  ) |>

  step_mutate(
    .pkgs = c("data.table", "lubridate", "timeDate"),

    company = fcase(
      hvfhs_license_num == "HV0002" ,
      "Juno"                        ,
      hvfhs_license_num == "HV0003" ,
      "Uber"                        ,
      hvfhs_license_num == "HV0004" ,
      "Via"                         ,
      hvfhs_license_num == "HV0005" ,
      "Lyft"                        ,
      default = "New"
    ) |>
      as.factor(),

    request_datetime_am = am(request_datetime) |> as.integer(),
    request_datetime_pm = pm(request_datetime) |> as.integer(),

    `Days to USChristmasDay` = difftime(
      USChristmasDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USColumbusDay` = difftime(
      USColumbusDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USCPulaskisBirthday` = difftime(
      USCPulaskisBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USDecorationMemorialDay` = difftime(
      USDecorationMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USElectionDay` = difftime(
      USElectionDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USGoodFriday` = difftime(
      USGoodFriday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USInaugurationDay` = difftime(
      USInaugurationDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USIndependenceDay` = difftime(
      USIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USJuneteenthNationalIndependenceDay` = difftime(
      USJuneteenthNationalIndependenceDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLaborDay` = difftime(
      USLaborDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USLincolnsBirthday` = difftime(
      USLincolnsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMemorialDay` = difftime(
      USMemorialDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USMLKingsBirthday` = difftime(
      USMLKingsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USNewYearsDay` = difftime(
      USNewYearsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USPresidentsDay` = difftime(
      USPresidentsDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USThanksgivingDay` = difftime(
      USThanksgivingDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USVeteransDay` = difftime(
      USVeteransDay(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer(),
    `Days to USWashingtonsBirthday` = difftime(
      USWashingtonsBirthday(year(request_datetime)),
      request_datetime,
      units = 'days'
    ) |>
      as.integer()
  ) |>

  # Removing variables
  step_rm(ends_with(c(
    "LocationID",
    "request_datetime",
    "hvfhs_license_num"
  ))) |>

  # Balancing data
  step_downsample(take_current_trip, under_ratio = 1)

pin_write(
  BoardLocal,
  ConsolidationRecipe,
  "ConsolidationRecipe",
  type = "qs2",
  title = "Consolidation Recipe"
)
```

## Consolidating features

As we are planning to train many models, it is efficient to apply these initial recipes to the training and testing data once. This avoids repeating the same steps for each training resample.

```{r}
#| eval: false

TrainingSampleJoined <-
  prep(ConsolidationRecipe) |>
  bake(new_data = NULL)

pin_write(
  BoardLocal,
  TrainingSampleJoined,
  "TrainingSampleJoined",
  type = "qs2",
  title = "Training Sample Joined"
)

```

```{r}
#| echo: false
#| output: false

TrainingSampleJoined <- pin_read(BoardLocal, "TrainingSampleJoined")
```

## Common steps for all recipes

Before creating the first recipe, it is important to consider that we want to keep `performance_per_hour` and `percentile_75_performance` available to calculate how much money we could be losing due to incorrect predictions.

```{r}
start_recipe <- function(df) {
  new_recipe =
    recipe(take_current_trip ~ ., data = df) |>
    step_impute_median(all_numeric_predictors()) |>
    update_role(
      trip_id,
      performance_per_hour,
      percentile_75_performance,
      new_role = "additional info"
    )

  return(new_recipe)
}
```

## Defining recipes for models that need normalized data

The models trained on this data are very affected by class imbalance. We will downsample to avoid these problems and speed up training time. Note that we have many examples of both cases and are looking for general rules that could be used by taxi drivers.

```{r}
BasicNormalizedRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

NormalizedPcaRecipe <-
  BasicNormalizedRecipe |>
  step_pca(all_numeric_predictors(), num_comp = tune()) |>
  step_normalize(all_numeric_predictors())

NormalizedPlsRecipe <-
  BasicNormalizedRecipe |>
  step_pls(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())

NormalizedUmapRecipe <-
  BasicNormalizedRecipe |>
  step_umap(
    all_numeric_predictors(),
    outcome = "take_current_trip",
    neighbors = tune(),
    num_comp = tune()
  ) |>
  step_normalize(all_numeric_predictors())
```

## Defining recipes for tree based models

```{r}
ReducedLevelsRecipe <-
  start_recipe(TrainingSampleJoined) |>
  step_novel(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = tune()) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_nzv(all_predictors())
```

## Defining workflows to evaluate

```{r}
#| eval: false

# Group 1: Simple linear models (light on memory)
WorkFlowSimple <- workflow_set(
  preproc = list(
    normalized = BasicNormalizedRecipe
  ),
  models = list(
    reg_logistic = GlmnetSpec
  )
)

# Group 2: Dimensionality reduction models (memory intensive)
WorkFlowDimReduction <- workflow_set(
  preproc = list(
    pca = NormalizedPcaRecipe,
    pls = NormalizedPlsRecipe,
    umap = NormalizedUmapRecipe
  ),
  models = list(
    flex_da = FdaSpec,
    mars = EarthSpec
  )
)

# Group 3: Tree-based models
WorkFlowTrees <- workflow_set(
  preproc = list(
    reduced_levels = ReducedLevelsRecipe
  ),
  models = list(
    random_forest = RangerSpec,
    xgboost = XgboostSpec
  )
)

pin_write(
  BoardLocal,
  WorkFlowSimple,
  "WorkFlowSimple",
  type = "qs2",
  title = "Work Flow Simple"
)

pin_write(
  BoardLocal,
  WorkFlowDimReduction,
  "WorkFlowDimReduction",
  type = "qs2",
  title = "Work Flow Dim Reduction"
)

pin_write(
  BoardLocal,
  WorkFlowTrees,
  "WorkFlowTrees",
  type = "qs2",
  title = "Work Flow Trees"
)
```

## Tuning grid for each workflow

Here is the code used to train and evaluate the different models.

```{r}
#| eval: false
#| file: ../multicore-scripts/03a-tuning-simple-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03b-tuning-dimreduction-models.R
```

```{r}
#| eval: false
#| file: ../multicore-scripts/03c-tuning-tree-models.R
```

## Exploring results

Our initial model exploration revealed that **tree-based models**, specifically Random Forest and Bagged Trees, delivered the best raw performance. However, a regularized Logistic Regression was a strong and much simpler contender.

Let's import the results of the exploration.

```{r}
WorkFlowTuned <- c(
  pin_read(BoardLocal, "WorkFlowSimpleTuned"),
  pin_read(BoardLocal, "WorkFlowDimReductionTuned"),
  pin_read(BoardLocal, "WorkFlowTreesTuned")
)
```

We will consolidate the results for one of the metrics.

```{r}
WorkFlowTunedBest <- lapply(
  names(WorkFlowTuned),
  FUN = \(x) {
    show_best(WorkFlowTuned[[x]], metric = "brier_class") |> mutate(model = x)
  }
) |>
  bind_rows()
```

Now we can confirm that the best models were `Logistic Regression` and `Random Forest` based on the number of correct predictions. However, we also need to consider that the relative cost of taking or rejecting the wrong trip can change from trip to trip. It is crucial that the metric used to select the best model takes these economic implications into consideration.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

WorkFlowTunedBest |>
  group_by(model) |>
  mutate(meadian_of_mean_score = quantile(mean, probs = 0.75)) |>
  ungroup() |>
  mutate(
    model = reorder(model, -mean, FUN = median),
    best_models = case_when(
      meadian_of_mean_score < 0.18 ~ "group1",
      meadian_of_mean_score < 0.23 ~ "group2",
      .default = "other"
    )
  ) |>
  ggplot(aes(model, mean)) +
  geom_boxplot(aes(fill = best_models)) +
  scale_fill_manual(
    values = c(
      "group1" = Params$ColorHighlight,
      "group2" = Params$ColorHighlightLow,
      "other" = Params$ColorGray
    )
  ) +
  coord_flip() +
  labs(
    title = paste0(
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Logistic Regression**",
      "</span> ",
      " and ",
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Random Forest**",
      "</span> "
    ),
    subtitle = "were the best models based on **Brier Score**",
    y = "Brier Score",
    x = "Trained Models"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.position = "none"
  )
```

To estimate the cost, we select the best model of each type and calculate the difference between the `performance_per_hour` of each trip and the `performance_per_hour` of the 75th percentile, depending on whether the final prediction was correct or not.

```{r}
#| eval: false

collect_prediction_cost <- function(
  training_data,
  tuned_wf,
  model_name,
  threshold,
  metric = "brier_class"
) {
  NycTaxi::collect_predictions_best_config(tuned_wf, model_name, metric) |>
    NycTaxi::add_performance_variables(training_data = training_data) |>
    NycTaxi::add_pred_class(threshold) |>
    NycTaxi::calculate_costs()
}

PredictionsCosts <- lapply(
  names(WorkFlowTuned),
  FUN = collect_prediction_cost,
  threshold = 0.5,
  training_data = TrainingSampleJoined,
  tuned_wf = WorkFlowTuned
) |>
  bind_rows()

pin_write(
  BoardLocal,
  x = PredictionsCosts,
  name = "PredictionsCosts",
  type = "qs2",
  title = "Predictions Costs"
)
```

The current method taxis use in the simulation is to accept all trips. We can calculate that, on average, this strategy resulted in a loss of around $4 per trip on the training data. We can now compare how that number would change if we used the best trained model for each model type.

The results were promising, showing that **Flexible Discriminant Analysis** (specifically using PLS) performed similarly to **Logistic Regression** and **Random Forest**. Logistic Regression proved to be the most cost-effective, significantly reducing the average loss compared to the current baseline.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

PredictionsCosts <- pin_read(
  BoardLocal,
  name = "PredictionsCosts"
)

PredictionsCosts |>
  group_by(model_name) |>
  summarize(
    current_method_cost = mean(current_method_cost, na.rm = TRUE),
    model_method_cost = mean(cost_wrong_total, na.rm = TRUE)
  ) |>
  ungroup() |>
  mutate(
    total_benefit = current_method_cost - model_method_cost,
    model_name = reorder(model_name, total_benefit)
  ) |>
  ggplot(aes(model_method_cost, model_name)) +
  geom_segment(
    aes(x = current_method_cost, xend = model_method_cost),
    color = Params$ColorGray,
    linewidth = 1
  ) +
  geom_point(
    shape = 21,
    color = "black",
    fill = Params$ColorHighlight,
    stroke = 0.5,
    size = 3
  ) +
  scale_x_continuous(breaks = scales::breaks_width(1)) +
  expand_limits(x = 0) +
  labs(
    title = paste0(
      "<span style='color:",
      Params$ColorHighlight,
      ";'>",
      "**Logistic Regression**",
      "</span> ",
      "losses less money than current method"
    ),
    subtitle = "**Flexible DA** is also producing similar results",
    y = "Trained Models",
    x = "Average Cost Lost Due Wrong Selection"
  ) +
  theme_light() +
  theme(
    plot.title = element_markdown(size = 14),
    plot.subtitle = element_markdown(size = 12),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none"
  )
```

## Is any of the models making a better predictions for some observations?

To solve this question we need to explit the training data.


```{r}
ModelsToCompare <-
  PredictionsCosts |>
  pull(model_name) |>
  unique() |>
  setdiff(y = "normalized_reg_logistic")

lapply(
  ModelsToCompare,
  FUN = NycTaxi::compare_model_predictions,
  predictions_df = PredictionsCosts,
  model_col = "model_name",
  model1_name = "normalized_reg_logistic",
  pivot_wider_id_cols = c("id", "trip_id")
) |>
  bind_rows() |>
  filter(correct_model %in% ModelsToCompare) |>
  arrange(desc(n)) |>
  gt() |>
  tab_header(title = "Model Prediction Agreement Comparison") |>
  cols_label(
    correct_model = "Model Compared",
    n = "Count",
    percentage = "Percentage"
  ) |>
  fmt_number(columns = n, decimals = 0) |>
  fmt_percent(columns = percentage, decimals = 0)
```

Logistic Regression and XGBoost highlighted complementary strengths: Logistic Regression surfaces stable, global linear effects suitable for immediate deployment due to their interpretability. However, XGBoost captures non-linear interactions and local patterns. **Critically, XGBoost achieved correct predictions on a unique 10% segment where Logistic Regression failed**. This specific edge identifies XGBoost as the primary candidate for deeper investigation, as dissecting these unique predictions can reveal complex patterns necessary to improve the final model.

```{r}
compare_model_predictions(
  predictions_df = PredictionsCosts,
  model_col = "model_name",
  model1_name = "normalized_reg_logistic",
  model2_name = "reduced_levels_xgboost",
  pivot_wider_id_cols = c("id", "trip_id")
) |>
  gt() |>
  tab_header(title = "Logistic Regression vs XGBoost") |>
  cols_label(
    correct_model = "Prediction State",
    n = "Count",
    percentage = "Percentage"
  ) |>
  fmt_number(columns = n, decimals = 0) |>
  fmt_percent(columns = percentage, decimals = 0)
```

## Creating workflows to explore

Now the strategy will be to understand what patterns have been caught by `xgboost` that were not caught by `Logistic Regression`. We will then decide whether we need to modify the features of `Logistic Regression` or create a stacked model to utilize the best aspects of both models.

```{r}
#| echo: false

WorkFlowSimple <- pin_read(
  BoardLocal,
  "WorkFlowSimple"
)

WorkFlowTrees <- pin_read(
  BoardLocal,
  "WorkFlowTrees"
)
```

```{r}
#| eval: false

InitialLogisticWf <- WorkFlowSimple |>
  extract_workflow(id = "normalized_reg_logistic") |>
  finalize_workflow(
    parameters = select_best(
      WorkFlowTuned[["normalized_reg_logistic"]],
      metric = "brier_class"
    )
  )

InitialXgboostWf <- WorkFlowTrees |>
  extract_workflow(id = "reduced_levels_xgboost") |>
  finalize_workflow(
    parameters = select_best(
      WorkFlowTuned[["reduced_levels_xgboost"]],
      metric = "brier_class"
    )
  )

pin_write(
  BoardLocal,
  InitialLogisticWf,
  "InitialLogisticWf",
  type = "qs2",
  title = "Initial Logistic Wf"
)

pin_write(
  BoardLocal,
  InitialXgboostWf,
  "InitialXgboostWf",
  type = "qs2",
  title = "Initial Xgboost Wf"
)
```

## Conclusion

The initial modeling phase successfully identified two distinct but high-performing approaches: **Regularized Logistic Regression** and **XGBoost**. While XGBoost displayed superior raw pattern recognition capabilities—likely due to its ability to capture non-linear interactions between location, time, and external factors—the Regularized Logistic Regression model demonstrated remarkable resilience and interpretability, yielding the lowest estimated financial loss per trip.

Given that the business objective focuses on minimizing cost (Performance Per Hour lost) rather than purely maximizing prediction accuracy, the **Logistic Regression model currently stands as the preferred candidate for deployment**. However, the unique patterns captured by XGBoost suggest there is still room for improvement. The next steps will focus on feature engineering to expose these non-linear patterns to the linear model, or potentially exploring a stacked ensemble to combine the global stability of the linear model with the local precision of the boosted trees.
