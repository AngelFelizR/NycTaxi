---
title: "Exploring Transportation and Socioeconomic Patterns"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

Expanding on the previous post's geospatial feature extraction, this article will delve into enriching our understanding of NYC taxi zones with socioeconomic and transportation-related data from the **American Community Survey (ACS)**. This new layer of information will provide crucial context for analyzing `take_current_trip` by revealing underlying community characteristics and commuting behaviors within each zone.

Here are the steps to follow:

1.  **Import ACS data**:
    * Identify relevant ACS variables for:
        * Means of transportation to work.
        * Travel time to work.
        * Household income.
        * Geographical mobility.
    * Download ACS data for NYC census tracts.

2.  **Spatially join ACS data to taxi zones**:
    * Perform spatial joins to aggregate census tract data to the taxi zone level.
    * Address overlapping and partially contained tracts.

3.  **Extract new features**:
    * Calculate proportions, averages, and densities of the ACS variables for each taxi zone.

4.  **Validation**:
    * Integrate new features with existing data and `take_current_trip`.
    * Define the correlation between new predictors and `take_current_trip`.
    
    
## Setting up the environment

### Define colors to use

```{r}
BoroughColors <- c(
  'Manhattan' = '#e41a1c',
  'Queens' = '#377eb8',
  'Brooklyn'= '#4daf4a'
)

BoroughSelected <- names(BoroughColors)

ColorHighlight <- "lightslateblue"
ColorGray <- "gray80"
```

### Define features to import



### Loading packages

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(data.table)
library(tidytext)

## To work with spatial data
library(sf)

## To create interactive maps
library(tmap)
tmap_mode("view")

# Access to US Census Bureau datasets
library(tidycensus)

## To create general plots
library(ggplot2)
library(scales)
```

:::{.callout-note title="Computational Performance"}
The geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.
:::

## Import data

Details on how this data was obtained can be found in the **Data Collection Process**.

```{r}
ZonesShapes <-
  read_sf(here("raw-data/taxi_zones/taxi_zones.shp")) |>
  subset(borough %in% BoroughSelected) |>
  transform(Shape_Area = st_area(geometry))

TrainingSample <-
  here("output/take-trip-fst") |>
  list.files(full.names = TRUE) |>
  (\(x) data.table(full_path = x,
                   n_char = nchar(basename(x)),
                   name = basename(x)))() |>
  (\(dt) dt[order(n_char, name), full_path])() |>
  head(12L) |>
  lapply(FUN = fst::read_fst,
         columns = c("trip_id","PULocationID", "DOLocationID", "take_current_trip"),
         as.data.table = TRUE) |>
  rbindlist()
```

## Exploring census variables


```{r}
#| echo: false
#| output: false

ValidTablesLastYear <- 
  fst::read_fst(here("output/cache-data/ValidTablesLastYear.fst"),
                as.data.table = TRUE)
```

```r
CensusTables <-
  rvest::read_html("https://api.census.gov/data.html") |>
  rvest::html_table() |>
  (\(x) x[[1L]])() |>
  as.data.table() |>
  subset(!is.na(`API Base URL`)
         & Vintage != "N/A",
         select = c("Title",
                    "Description",
                    "Vintage",
                    "Dataset Type",
                    "Dataset Name",
                    "API Base URL"))

CensusTables[,`:=`(Vintage = as.integer(Vintage),
                   `Dataset Name` = gsub("› ", "/", `Dataset Name`))]

CensusTables[, `:=`(survey = fcase(`Dataset Name` %like% "^dec",
                                   "dec",
                                   `Dataset Name` %like% "^acs",
                                   "acs"),
                    `Dataset Name` = gsub("(dec|acs)/", "", `Dataset Name`))]

setorder(CensusTables, -Vintage)

ValidTables = c(
  "sf1", "sf2", "sf3", "sf4", "pl", "dhc", "dp", 
  "ddhca", "ddhcb", "sdhc", "as", "gu", "mp", 
  "vi", "acsse", "dpas", "dpgu", "dpmp", "dpvi",
  "dhcvi", "dhcgu", "dhcvi", "dhcas", "acs1", 
  "acs3", "acs5", "acs1/profile", "acs3/profile",
  "acs5/profile", "acs1/subject", "acs3/subject",
  "acs5/subject", "acs1/cprofile", "acs5/cprofile",
  "sf2profile", "sf3profile", "sf4profile", "aian",
  "aianprofile", "cd110h", "cd110s", "cd110hprofile",
  "cd110sprofile", "sldh", "slds", "sldhprofile",
  "sldsprofile", "cqr", "cd113", "cd113profile",
  "cd115", "cd115profile", "cd116", "plnat", "cd118"
)

ValidTablesYears <-
  CensusTables[
    Vintage <= 2022
  ][ValidTables,
    on = "Dataset Name",
    nomatch = NULL,
    unique(.SD, by = "Dataset Name"),
    .SDcols = c("Vintage", "Dataset Name", "survey")]

get_all_data = function(x, ref_df){
  
  imported_data =
    load_variables(ref_df$Vintage[x],
                   ref_df$`Dataset Name`[x])
  
  setDT(imported_data)
  
  imported_data[, `:=`(year = ref_df$Vintage[x],
                       dataset = ref_df$`Dataset Name`[x],
                       survey = ref_df$survey[x])]
  
  return(imported_data)
  
} 
  
ValidTablesLastYear <-
  lapply(seq_along(ValidTablesYears$`Dataset Name`), 
         get_all_data,
         ref_df = ValidTablesYears)|>
  rbindlist(use.names = TRUE, fill = TRUE)

dim(ValidTablesLastYear)
```

```{r}
#| echo: false

dim(ValidTablesLastYear)
```

```{r}
#| eval: false
#| echo: false

fst::write_fst(ValidTablesLastYear, here("output/cache-data/ValidTablesLastYear.fst"))
```


```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

YearBreaks <- unique(ValidTablesLastYear$year) |> sort(decreasing = TRUE)

VariablesPerDataset <-
  ValidTablesLastYear[, .N, 
                      .(year = factor(year), 
                        dataset,
                        survey)
  ][, dataset := reorder(dataset, N, FUN = sum)] 

ggplot(VariablesPerDataset) +
  geom_col(aes(N, dataset, fill = year)) +
  scale_fill_brewer(palette = "Purples",
                    breaks = YearBreaks) +
  scale_x_continuous(labels = scales::label_comma(scale = 1/1000, suffix = "k"),
                     breaks = scales::breaks_width(5e3))+
  labs(x = "Number of variables",
       fill = "Update year")+
  facet_wrap(vars(survey), scale = "free") +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

CustomStopWords <- c(
  stop_words$word,
  "estimate",
  "estimates",
  "percent",
  NA_character_,
  "months",
  "past",
  "detailed",
  "type",
  "types",
  "current",
  "adjusted",
  "united",
  "round"
)

MostImportWordsPerDataset <-
  ValidTablesLastYear[, .(dataset,
                          name,
                          label = tolower(concept))
  ][, unnest_tokens(.SD, word, label)
  ][!word %chin% CustomStopWords
    & !word %like% "^\\d+$",
    .N,
    by = c("dataset", "word")
  ][, dataset_total := sum(N), 
    by = "dataset"
  ][, bind_tf_idf(.SD, word, dataset, N)]

setorder(MostImportWordsPerDataset, -tf_idf)

TopDataset <- levels(VariablesPerDataset$dataset) |> rev()

MostImportWordsPerDataset[dataset %chin% TopDataset[1], 
                          head(.SD, 30L),
                          by = "dataset"] |>
  ggplot(aes(tf_idf,
             reorder(word, tf_idf, FUN = sum))) +
  geom_col(fill = ColorGray,
           color = "black",
           linewidth = 0.2) +
  scale_x_continuous(labels = percent_format()) +
  labs(title = TopDataset[1],
       x = "tf-idf",
       y = NULL) +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

MostImportWordsPerDataset[dataset %chin% TopDataset[2], 
                          head(.SD, 30L),
                          by = "dataset"] |>
  ggplot(aes(tf_idf,
             reorder(word, tf_idf, FUN = sum))) +
  geom_col(fill = ColorGray,
           color = "black",
           linewidth = 0.2) +
  scale_x_continuous(labels = percent_format()) +
  labs(title = TopDataset[2],
       x = "tf-idf",
       y = NULL) +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```


### Listing variables to select

```r
temp[c("MEANS OF TRANSPORTATION TO WORK",
       "MEANS OF TRANSPORTATION TO WORK BY TIME OF DEPARTURE TO GO TO WORK",
       "MEANS OF TRANSPORTATION TO WORK BY TRAVEL TIME TO WORK",
       "AGGREGATE TRAVEL TIME TO WORK (IN MINUTES) OF WORKERS BY MEANS OF TRANSPORTATION TO WORK",
       "MEANS OF TRANSPORTATION TO WORK BY VEHICLES AVAILABLE",
       
       "MEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) BY AGE OF HOUSEHOLDER"),
     on = "concept",
     j = !c("concept")
][label != "Estimate!!Total:"]

# Geographical Mobility - B07203_008
temp[name %like% "^[A-Z]\\d+_"] |> View()

temp[name %like% "^B08124_"
     & label %ilike% ".+:.+:$", View(.SD)]
```


### Getting data from API

### Saving the data

```r
qs2::qs_save(OsmDataComplete, 
             here("output/cache-data/OsmDataComplete.qs"))
```


## Spatial joins

We perform two main types of spatial joins to associate the OpenStreetMap features with our taxi zones: first, to identify features entirely within each zone, and second, to identify features that intersect or overlap with the zone boundaries.

### Joining elements **contained** by each zone

Using the `st_within` function, this section identifies OpenStreetMap features—such as points, lines, polygons, multilinestrings, and multipolygons—that are **fully contained** within each taxi zone's boundaries. `st_within` considers multilinestrings and multipolygons to be within a zone only if all their constituent parts are inside.

```r
PolygonWithinZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_within,
          left = FALSE)
```


### Joining lines that are **partially contained** for each zone. 

In this section, we identify OSM features that are not entirely within a taxi zone but do intersect or overlap with its boundary. For lines and multilinestrings, we use `st_crosses`, which identifies features that intersect the interior of a zone but are not completely contained within it. For polygons and multipolygons, we use `st_overlaps`, which identifies features that have some area in common with a zone but are not identical to or completely contained within it.

```r
PolygonInSeveralZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_overlaps,
          left = FALSE)
```

#### Cropping features overlapping multiple zones


## Validation


## Conclusion

