---
title: "Exploring Transportation and Socioeconomic Patterns"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

Expanding on the previous post's geospatial feature extraction, this article will delve into enriching our understanding of NYC taxi zones with socioeconomic and transportation-related data from the **American Community Survey (ACS)**. This new layer of information will provide crucial context for analyzing `take_current_trip` by revealing underlying community characteristics and commuting behaviors within each zone.

Here are the steps to follow:

1.  **Import ACS data**:
    * Identify relevant ACS variables for:
        * Means of transportation to work.
        * Travel time to work.
        * Household income.
        * Geographical mobility.
    * Download ACS data for NYC census tracts.

2.  **Spatially join ACS data to taxi zones**:
    * Perform spatial joins to aggregate census tract data to the taxi zone level.
    * Address overlapping and partially contained tracts.

3.  **Extract new features**:
    * Calculate proportions, averages, and densities of the ACS variables for each taxi zone.

4.  **Validation**:
    * Integrate new features with existing data and `take_current_trip`.
    * Define the correlation between new predictors and `take_current_trip`.
    
    
## Setting up the environment

### Define colors to use

```{r}
BoroughColors <- c(
  'Manhattan' = '#e41a1c',
  'Queens' = '#377eb8',
  'Brooklyn'= '#4daf4a'
)

BoroughSelected <- names(BoroughColors)

ColorHighlight <- "lightslateblue"
ColorGray <- "gray80"
```

### Define features to import



### Loading packages

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(data.table)
library(tidytext)
library(stringdist)

## To work with spatial data
library(sf)

## To create interactive maps
library(tmap)
tmap_mode("view")

# To work and plot graphs
library(igraph)
library(ggraph)

# Access to US Census Bureau datasets
library(tidycensus)

## To create general plots
library(ggplot2)
library(scales)

## Custom functions
devtools::load_all()
```

:::{.callout-note title="Computational Performance"}
The geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.
:::

## Import data

Details on how this data was obtained can be found in the **Data Collection Process**.

```{r}
ZonesShapes <-
  read_sf(here("raw-data/taxi_zones/taxi_zones.shp")) |>
  subset(borough %in% BoroughSelected) |>
  transform(Shape_Area = st_area(geometry))

TrainingSample <-
  here("output/take-trip-fst") |>
  list.files(full.names = TRUE) |>
  (\(x) data.table(full_path = x,
                   n_char = nchar(basename(x)),
                   name = basename(x)))() |>
  (\(dt) dt[order(n_char, name), full_path])() |>
  head(12L) |>
  lapply(FUN = fst::read_fst,
         columns = c("trip_id","PULocationID", "DOLocationID", "take_current_trip"),
         as.data.table = TRUE) |>
  rbindlist()
```

## Exploring census variables

Based on the `tidycensus` documentation we have 55 tables available for exploration. That's sounds great but a don't have any idea which variables I want use in this project.

To solve this initial problem we are going to scrap the official web site table to confirm:

- The corresponding survey for each dataset.
- Select the last year update of each dataset in 2022 or prior (our trainingset comes from 2022).

```{r}
#| echo: false
#| output: false

ValidTablesLastYear <- 
  fst::read_fst(here("output/cache-data/ValidTablesLastYear.fst"),
                as.data.table = TRUE)
```

```r
CensusTables <-
  rvest::read_html("https://api.census.gov/data.html") |>
  rvest::html_table() |>
  (\(x) x[[1L]])() |>
  as.data.table() |>
  subset(!is.na(`API Base URL`)
         & Vintage != "N/A",
         select = c("Title",
                    "Description",
                    "Vintage",
                    "Dataset Type",
                    "Dataset Name",
                    "API Base URL"))

CensusTables[,`:=`(Vintage = as.integer(Vintage),
                   `Dataset Name` = gsub("â€º ", "/", `Dataset Name`))]

CensusTables[, `:=`(survey = fcase(`Dataset Name` %like% "^dec",
                                   "dec",
                                   `Dataset Name` %like% "^acs",
                                   "acs"),
                    `Dataset Name` = gsub("(dec|acs)/", "", `Dataset Name`))]

setorder(CensusTables, -Vintage)

ValidTables = c(
  "sf1", "sf2", "sf3", "sf4", "pl", "dhc", "dp", 
  "ddhca", "ddhcb", "sdhc", "as", "gu", "mp", 
  "vi", "acsse", "dpas", "dpgu", "dpmp", "dpvi",
  "dhcvi", "dhcgu", "dhcvi", "dhcas", "acs1", 
  "acs3", "acs5", "acs1/profile", "acs3/profile",
  "acs5/profile", "acs1/subject", "acs3/subject",
  "acs5/subject", "acs1/cprofile", "acs5/cprofile",
  "sf2profile", "sf3profile", "sf4profile", "aian",
  "aianprofile", "cd110h", "cd110s", "cd110hprofile",
  "cd110sprofile", "sldh", "slds", "sldhprofile",
  "sldsprofile", "cqr", "cd113", "cd113profile",
  "cd115", "cd115profile", "cd116", "plnat", "cd118"
)

ValidTablesYears <-
  CensusTables[
    Vintage <= 2022
  ][ValidTables,
    on = "Dataset Name",
    nomatch = NULL,
    unique(.SD, by = "Dataset Name"),
    .SDcols = c("Vintage", "Dataset Name", "survey")]

get_all_data = function(x, ref_df){
  
  imported_data =
    load_variables(ref_df$Vintage[x],
                   ref_df$`Dataset Name`[x])
  
  setDT(imported_data)
  
  imported_data[, `:=`(year = ref_df$Vintage[x],
                       dataset = ref_df$`Dataset Name`[x],
                       survey = ref_df$survey[x])]
  
  return(imported_data)
  
} 
  
ValidTablesLastYear <-
  lapply(seq_along(ValidTablesYears$`Dataset Name`), 
         get_all_data,
         ref_df = ValidTablesYears)|>
  rbindlist(use.names = TRUE, fill = TRUE)

nrow(ValidTablesLastYear) |> comma()
```

```{r}
#| echo: false

nrow(ValidTablesLastYear) |> comma()
```

After making that exploration we can see that we have more 340K variables to explore.

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(ValidTablesLastYear, here("output/cache-data/ValidTablesLastYear.fst"))
```

### Selecting dataset to use

To understand have we have let's plot the number of variables by dataset and survey for each year.

Based on the below chart we can see that `acs1` was updated in 2022 and have more than 35k variables.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

YearBreaks <- unique(ValidTablesLastYear$year) |> sort(decreasing = TRUE)

VariablesPerDataset <-
  ValidTablesLastYear[, .N, 
                      .(year = factor(year), 
                        dataset,
                        survey)
  ][, dataset := reorder(dataset, N, FUN = sum)] 

ggplot(VariablesPerDataset) +
  geom_col(aes(N, dataset, fill = year)) +
  scale_fill_brewer(palette = "Purples",
                    breaks = YearBreaks) +
  scale_x_continuous(labels = scales::label_comma(scale = 1/1000, suffix = "k"),
                     breaks = scales::breaks_width(5e3))+
  labs(x = "Number of variables",
       fill = "Last update year")+
  facet_wrap(vars(survey), scale = "free") +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

To have a general idea of the variable available in that data set we can use the **statistic tf-idf** to list the top 30 more important words of the variables present in the dataset after excluding some stopwords.

Based on the results we see some interesting words like:

-**Occupation**, **employed**, **employment** and **Industry**: I think work variable can affect daily patterns that could benefict taxi drivers.
-**Population**: The number of people requesting one service could affect the price.
-**Income**, **earnings**, **poverty**: The average income can affect if people prefer to take a taxi, have their on car or just take public transportation.
-**Transportation**: Any information about transportation will be useful.

So in general it seems this dataset have a lot of potential and we can concentrate the time to select the more important variables.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

CustomStopWords <- c(
  stop_words$word,
  "estimate",
  "estimates",
  "percent",
  NA_character_,
  "months",
  "past",
  "detailed",
  "type",
  "types",
  "current",
  "adjusted",
  "united",
  "round"
)

MostImportWordsPerDataset <-
  ValidTablesLastYear[, .(dataset,
                          name,
                          label = tolower(concept))
  ][, unnest_tokens(.SD, word, label)
  ][!word %chin% CustomStopWords
    & !word %like% "^\\d+$",
    .N,
    by = c("dataset", "word")
  ][, dataset_total := sum(N), 
    by = "dataset"
  ][, bind_tf_idf(.SD, word, dataset, N)]

setorder(MostImportWordsPerDataset, -tf_idf)


MostImportWordsPerDataset[dataset == "acs1", 
                          head(.SD, 30L),
                          by = "dataset"] |>
  ggplot(aes(tf_idf,
             reorder(word, tf_idf, FUN = sum))) +
  geom_col(fill = ColorGray,
           color = "black",
           linewidth = 0.2) +
  scale_x_continuous(labels = percent_format()) +
  labs(title = "acs1",
       x = "tf-idf",
       y = NULL) +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```


### Selecting variables to use

Just by looking to the raw data, I found that some variables are related to very specific topics that I don't need to model on like:

- Race
- Place of Birth
- Sex

These are indicated in parentheses, so we can create a vector to make exclusion and confirm that any similar was left.

```{r}
VariablesToIgnore <- c(
  "Hispanic",
  "Latino",
  "American Indian",
  "Alaska Native",
  "White",
  "Black",
  "African American",
  "Asian",
  "Native Hawaiian",
  "Pacific Islander",
  "Races?",
  
  "Ancestry",
  "Nativity",
  "Citizenship Status",
  "Place of Birth",
  "Naturalization",
  "Current Residence",
  
  "Male",
  "Female",
  "Women",
  "Men"
)
```



```{r}
#| eval: false

Acs1ParenthesesInformation <-
  ValidTablesLastYear[dataset == "acs1"
                      & year == 2022
                      & concept %like% "\\("
                      & !concept %like% paste(VariablesToIgnore, collapse = "|")
                      & !label %like% "Male|Female",
                      .(sub_concept = stringr::str_match(concept, "\\((.| )+\\)")[, 1L])
  ][, .N, sub_concept]

setorder(Acs1ParenthesesInformation, -N)

Acs1ParenthesesInformation
```


```{r}
#| echo: false

Acs1ParenthesesInformation <- fst::read_fst(here("output/cache-data/Acs1ParenthesesInformation.fst"),
                                            as.data.table = TRUE)

Acs1ParenthesesInformation
```

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(Acs1ParenthesesInformation, here("output/cache-data/Acs1ParenthesesInformation.fst"))
```


Now we just need to apply the same filter for all variables in the dataset.

```{r}
#| eval: false

Asc1BasicConcept <-
  ValidTablesLastYear[dataset == "acs1"
                      & year == 2022
                      & !concept %like% paste(VariablesToIgnore, collapse = "|")
                      & !label %like% "Male|Female",
                      .(concept = unique(concept))]
```

```{r}
#| echo: false

Asc1BasicConcept <- fst::read_fst(here("output/cache-data/Asc1BasicConcept.fst"),
                                  as.data.table = TRUE)
```

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(Asc1BasicConcept, here("output/cache-data/Asc1BasicConcept.fst"))
```

After excluding those cases, we still have **`r nrow(Asc1BasicConcept)``** variables to select from, which is still too much if we need to apply spatial joins on each variable to the see which variable have some signal with the target variables.

To explore more efficiently the variables names we can use the **Jaccard distance**, but as the variable names present repetitive words we don't need to evaluate the distance based on letters but **based on words**. To make this possible we need to:

1. Transform each of the variables in to list of 32bit integers by using `hashr::hash`.

```{r}
Asc1BasicConceptHash <- 
  Asc1BasicConcept$concept |> 
  tolower() |>
  strsplit("\\s+") |> 
  hashr::hash()

Asc1BasicConcept$concept[1:2]

Asc1BasicConceptHash[1:2]
```

2. Compute pairwise string distances between hashed words.

```{r}
Asc1BasicConceptWordDistance <-
  seq_distmatrix(Asc1BasicConceptHash, 
                 Asc1BasicConceptHash, 
                 method = "jaccard", 
                 q = 1)
```

To be able to explore the result, let's transform the returned distance matrix into a long data.table, making sure to exclude any duplicated values using the `upper.tri` function from base R and then using argument `na.rm = FALSE` in the `data.table::melt` function.

```{r}
rownames(Asc1BasicConceptWordDistance) <- Asc1BasicConcept$concept
colnames(Asc1BasicConceptWordDistance) <- Asc1BasicConcept$concept

Asc1BasicConceptWordDistance[upper.tri(Asc1BasicConceptWordDistance, diag = TRUE)] <- NA

Asc1WordDistanceDt <-
  as.data.table(Asc1BasicConceptWordDistance,
                keep.rownames = "Variable Name 1") |>
  melt(id.vars = "Variable Name 1",
       variable.name = "Variable Name 2",
       value.name = "Jaccard Distance",
       na.rm = TRUE)
```

```{r}
ggplot(Asc1WordDistanceDt) + 
  geom_histogram(aes(`Jaccard Distance`))
```

::: {.panel-tabset}

```{r results='asis'}
for (cluster_i in 1:20) {
  group_vars <- Asc1BasicConcept[.(cluster_i), concept]
  cat(paste("\n#### Group", cluster_i, "(",length(group_vars)," vars)", "\n\n"))
  paste0("- `", group_vars,"`") |> cat(sep = "\n")
}
```

:::

Based on the context provided, the most useful variables from the American Community Survey (ACS) for analyzing transportation and socioeconomic patterns in NYC taxi zones would likely include:

1. **Means of Transportation to Work**: This variable can provide insights into how people commute to work, which can be correlated with taxi usage patterns.

2. **Travel Time to Work**: Understanding the average travel time to work can help identify areas where taxis might be more frequently used due to longer commutes.

3. **Household Income**: Income levels can influence the likelihood of using taxis. Higher-income areas might see more taxi usage.

5. **Occupation and Industry**: These variables can provide insights into the types of jobs people have and their work schedules, which can affect taxi usage patterns.


These variables can be spatially joined to taxi zone data to create a comprehensive dataset that can be used to analyze and model taxi usage patterns in relation to socioeconomic and transportation characteristics.

### Listing variables to select

```r
temp[c("MEANS OF TRANSPORTATION TO WORK",
       "MEANS OF TRANSPORTATION TO WORK BY TIME OF DEPARTURE TO GO TO WORK",
       "MEANS OF TRANSPORTATION TO WORK BY TRAVEL TIME TO WORK",
       "AGGREGATE TRAVEL TIME TO WORK (IN MINUTES) OF WORKERS BY MEANS OF TRANSPORTATION TO WORK",
       "MEANS OF TRANSPORTATION TO WORK BY VEHICLES AVAILABLE",
       
       "MEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) BY AGE OF HOUSEHOLDER"),
     on = "concept",
     j = !c("concept")
][label != "Estimate!!Total:"]

# Geographical Mobility - B07203_008
temp[name %like% "^[A-Z]\\d+_"] |> View()

temp[name %like% "^B08124_"
     & label %ilike% ".+:.+:$", View(.SD)]
```


### Getting data from API

### Saving the data

```r
qs2::qs_save(OsmDataComplete, 
             here("output/cache-data/OsmDataComplete.qs"))
```


## Spatial joins

We perform two main types of spatial joins to associate the OpenStreetMap features with our taxi zones: first, to identify features entirely within each zone, and second, to identify features that intersect or overlap with the zone boundaries.

### Joining elements **contained** by each zone

Using the `st_within` function, this section identifies OpenStreetMap featuresâ€”such as points, lines, polygons, multilinestrings, and multipolygonsâ€”that are **fully contained** within each taxi zone's boundaries. `st_within` considers multilinestrings and multipolygons to be within a zone only if all their constituent parts are inside.

```r
PolygonWithinZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_within,
          left = FALSE)
```


### Joining lines that are **partially contained** for each zone. 

In this section, we identify OSM features that are not entirely within a taxi zone but do intersect or overlap with its boundary. For lines and multilinestrings, we use `st_crosses`, which identifies features that intersect the interior of a zone but are not completely contained within it. For polygons and multipolygons, we use `st_overlaps`, which identifies features that have some area in common with a zone but are not identical to or completely contained within it.

```r
PolygonInSeveralZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_overlaps,
          left = FALSE)
```

#### Cropping features overlapping multiple zones


## Validation


## Conclusion

