---
title: "Exploring Transportation and Socioeconomic Patterns"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

Expanding on the previous post's geospatial feature extraction, this article will delve into enriching our understanding of NYC taxi zones with socioeconomic and transportation-related data from the **US Census Bureau**. This new layer of information will provide crucial context for analyzing `take_current_trip` by revealing underlying community characteristics and commuting behaviors within each zone.

Here are the steps to follow:

1. **Importing training data with zone shapes**.

1.  **Import Census data**:
    * Identify relevant variables.
    * Download data for Manhattan, Queens and Brooklyn with geometry.

2.  **Spatially join ACS data to taxi zones**:
    * Perform spatial joins to aggregate census tract data to the taxi zone level.
    * Address overlapping and partially contained tracts.

3.  **Extract new features**:
    * Calculate proportions, averages, and densities of the ACS variables for each taxi zone.

4.  **Validation**:
    * Integrate new features with existing data and `take_current_trip`.
    * Define the correlation between new predictors and `take_current_trip`.
    
    
## Setting up the environment

### Define colors to use

```{r}
BoroughColors <- c(
  'Manhattan' = '#e41a1c',
  'Queens' = '#377eb8',
  'Brooklyn'= '#4daf4a'
)

BoroughSelected <- names(BoroughColors)

ColorHighlight <- "lightslateblue"
ColorGray <- "gray80"
```

### Loading packages

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(data.table)

## To work with spatial data
library(sf)

## To create interactive maps
library(tmap)
tmap_mode("view")

# To work and plot graphs
library(tidytext)
library(stringdist)
library(hashr)
library(igraph)

# Access to US Census Bureau datasets
library(tidycensus)

## To create general plots
library(ggplot2)
library(scales)

## To create table
library(gt)

## Custom functions
devtools::load_all()
```

:::{.callout-note title="Computational Performance"}
The geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.
:::

## Importing training data with zone shapes

Details on how this data was obtained can be found in the **Data Collection Process**.

```{r}
ZonesShapes <-
  read_sf(here("raw-data/taxi_zones/taxi_zones.shp")) |>
  subset(borough %in% BoroughSelected) |>
  transform(Shape_Area = st_area(geometry))

TrainingSample <-
  here("output/take-trip-fst") |>
  list.files(full.names = TRUE) |>
  (\(x) data.table(full_path = x,
                   n_char = nchar(basename(x)),
                   name = basename(x)))() |>
  (\(dt) dt[order(n_char, name), full_path])() |>
  head(12L) |>
  lapply(FUN = fst::read_fst,
         columns = c("trip_id","PULocationID", "DOLocationID", "take_current_trip"),
         as.data.table = TRUE) |>
  rbindlist()
```

## Import Census data

### Identify relevant variables

Based on the `tidycensus` documentation we have the below tables available for exploration.

```{r}
ValidTables = c(
  "sf1", "sf2", "sf3", "sf4", "pl", "dhc", "dp", 
  "ddhca", "ddhcb", "sdhc", "as", "gu", "mp", 
  "vi", "acsse", "dpas", "dpgu", "dpmp", "dpvi",
  "dhcvi", "dhcgu", "dhcvi", "dhcas", "acs1", 
  "acs3", "acs5", "acs1/profile", "acs3/profile",
  "acs5/profile", "acs1/subject", "acs3/subject",
  "acs5/subject", "acs1/cprofile", "acs5/cprofile",
  "sf2profile", "sf3profile", "sf4profile", "aian",
  "aianprofile", "cd110h", "cd110s", "cd110hprofile",
  "cd110sprofile", "sldh", "slds", "sldhprofile",
  "sldsprofile", "cqr", "cd113", "cd113profile",
  "cd115", "cd115profile", "cd116", "plnat", "cd118"
)

length(ValidTables)
```

That means that we have a lot a data to use, but to be able to get the list of variables available for each table we need to know the different year when each table was updated. In our particular case, we only know  the **last year when each table was updated from 2020 to 2020**.

To solve that question, we only need to scrap the table available in [https://api.census.gov/data.html](https://api.census.gov/data.html) and apply our particular selection to that huge table. 

```{r}
#| echo: false
#| output: false

ValidTablesYearsFilePath = here("output/cache-data/08-expanding-transportation-socioeconomic/ValidTablesYears.fst")


if(file.exists(ValidTablesYearsFilePath)) {
  ValidTablesYears <- 
    fst::read_fst(ValidTablesYearsFilePath, as.data.table = TRUE)
}
```

```{r}
#| eval: false

CensusTables <-
  rvest::read_html("https://api.census.gov/data.html") |>
  rvest::html_table() |>
  (\(x) x[[1L]])() |>
  as.data.table() |>
  subset(!is.na(`API Base URL`)
         & Vintage != "N/A",
         select = c("Title",
                    "Description",
                    "Vintage",
                    "Dataset Type",
                    "Dataset Name",
                    "API Base URL"))

CensusTables[,`:=`(Vintage = as.integer(Vintage),
                   `Dataset Name` = gsub("â€º ", "/", `Dataset Name`))]

CensusTables[, `:=`(survey = fcase(`Dataset Name` %like% "^dec",
                                   "dec",
                                   `Dataset Name` %like% "^acs",
                                   "acs"),
                    `Dataset Name` = gsub("(dec|acs)/", "", `Dataset Name`))]

setorder(CensusTables, -Vintage)

ValidTablesYears <-
  CensusTables[
    Vintage %between% c(2020, 2022)
  ][ValidTables,
    on = "Dataset Name",
    nomatch = NULL,
    unique(.SD, by = "Dataset Name"),
    .SDcols = c("Vintage", "Dataset Name", "survey")]

ValidTablesYears[, head(.SD, 5),
                 by = "survey"]

```

```{r}
#| echo: false

ValidTablesYears[, head(.SD, 5),
                 by = "survey"]
```

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(ValidTablesYears, ValidTablesYearsFilePath)
```

Now that we have the years related to each table, let's apply `tidycensus::load_variables` for each table and year to see the variables to explore. 

```{r}
#| echo: false
#| output: false

ValidTablesLastYearFilePath <- here("output/cache-data/08-expanding-transportation-socioeconomic/ValidTablesLastYear.fst")


if(file.exists(ValidTablesLastYearFilePath)) {
  ValidTablesLastYear <- 
    fst::read_fst(ValidTablesLastYearFilePath, as.data.table = TRUE)
}
```

```{r}
#| eval: false

get_all_data = function(x, ref_df){
  
  imported_data =
    load_variables(ref_df$Vintage[x],
                   ref_df$`Dataset Name`[x])
  
  setDT(imported_data)
  
  imported_data[, `:=`(year = ref_df$Vintage[x],
                       dataset = ref_df$`Dataset Name`[x],
                       survey = ref_df$survey[x])]
  
  return(imported_data)
  
} 
  
ValidTablesLastYear <-
  lapply(seq_along(ValidTablesYears$`Dataset Name`), 
         get_all_data,
         ref_df = ValidTablesYears)|>
  rbindlist(use.names = TRUE, fill = TRUE)

nrow(ValidTablesLastYear) |> comma()
```

```{r}
#| echo: false

nrow(ValidTablesLastYear) |> comma()
```

After making that exploration we can see that we have almost **160K variables** to explore.

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(ValidTablesLastYear, ValidTablesLastYearFilePath)
```

To understand what have we have let's plot the number of variables by dataset and survey for each year.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

VariablesPerDataset <-
  ValidTablesLastYear[, .N, 
                      .(year = factor(year), 
                        dataset,
                        survey)
  ][, dataset := reorder(dataset, N, FUN = sum)] 

ggplot(VariablesPerDataset) +
  geom_col(aes(N, dataset, fill = year),
           color = "black",
           linewidth = 0.2) +
  scale_fill_manual(values = c("2022" = ColorHighlight,
                               "2020" = ColorGray)) +
  scale_x_continuous(labels = scales::label_comma(scale = 1/1000, suffix = "k"))+
  expand_limits(x = 1e4) +
  labs(x = "Number of variables",
       fill = "Last update year")+
  facet_wrap(vars(survey), scale = "free") +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

It's clear that **there is one way we can read the variables one by one** and hypothesize the variables to select. Instead, we are going to flow some steps to reduce the number of variable and the classify them in to groups to select.

Just by looking to the raw data, I found that many variables measure the same thing for particular groups based on:

- Race
- Place of Birth
- Sex

```{r}
VariablesToIgnore <- c(
  "Hispanic",
  "Latino",
  "American Indian",
  "Alaska Native",
  "White",
  "Black",
  "African American",
  "Asian",
  "Native Hawaiian",
  "Pacific Islander",
  "Races?",
  
  "Ancestry",
  "Nativity",
  "Citizenship Status",
  "Place of Birth",
  "Naturalization",
  "Current Residence",
  
  "Male",
  "Female",
  "Women",
  "Men"
)
```

As we will join the data based on geography it doesn't make sense to have predictors based on the situations of those particular groups as we won't have does details of passengers of the trips.

```{r}
#| echo: false
#| output: false

ConceptsToExploreFilePath <-
  here("output/cache-data/08-expanding-transportation-socioeconomic/ConceptsToExplore.fst")

if(file.exists(ConceptsToExploreFilePath)) {
  ConceptsToExplore <-
    fst::read_fst(ConceptsToExploreFilePath, as.data.table = TRUE)
}
```

```{r}
#| eval: false

ConceptsToExplore <-
  ValidTablesLastYear[!concept %ilike% paste(VariablesToIgnore, collapse = "|")
                      & !label %ilike% "Male|Female",
                      .(concept = unique(concept))]

nrow(ConceptsToExplore)
```

```{r}
#| echo: false

nrow(ConceptsToExplore)
```

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(ConceptsToExplore, ConceptsToExploreFilePath)
```

Another aspect we found out by exploring the data it's that some variables names are really long and the long the concept definition the more specific the variable and less useful for our model. For example, below we can see the variable with the longer number of characters:

```{r}
#| echo: false
#| output: asis

ConceptsToExplore[nchar(concept) == max(nchar(concept)), paste0("> ", concept)] |> cat()
```

- Population Measured: 
  - Grandparents living with own grandchildren under 18 years
  - 30 years and over
  - In households (excluding military housing units)

- Data Categories:
  - By responsibility for own grandchildren
  - By length of time responsible for grandchildren

By exploring the cumulative distribution we can see that near 50% of the variable present less **70 characters** per variable which seems the better group to focus the exploration

```{r}
#| code-fold: true
#| code-summary: "Show the code"

ConceptsToExplore[, concept_length := nchar(concept)]

ggplot(ConceptsToExplore)+
  stat_ecdf(aes(concept_length)) +
  scale_x_continuous(breaks = breaks_width(10))+
  scale_y_continuous(labels = label_percent(accuracy = 1))+
  labs(title = "Empirical Cumulative Distribution",
       subtitle = "Number of Character per Variable",
       x = "Number of Characters",
       y = "Cumulative Proportion") +
  theme_minimal()+
  theme(panel.grid.minor = element_blank())
```

After applying that change we only have 675 pending variables to explore the classify.

```{r}
ConceptsToExploreMaxConceptLength <- 70

ConceptsToExploreValidConceptLength <- 
  ConceptsToExplore[concept_length <= ConceptsToExploreMaxConceptLength]

ConceptsToExploreValidConceptLength[, .N]
```

To confirm if that action make sense we can print the longer variables and confirm that aren't too specific.

```{r}
#| output: asis
#| echo: false

ConceptsToExploreValidConceptLength[concept_length == max(concept_length), 
                                    paste0("- `", concept, "`")] |>
  head() |>
  cat(sep = "\n")
```

Now we can calculate the **pairwise Jaccard string distances** of each of the variables to identify conceptually similar terms based on similar words to then identify variable clusters to select.

1. Create word-level hash tokens (not character-level) after case normalization.

```{r}
ConceptsToExploreHash <- 
  tolower(ConceptsToExploreValidConceptLength$concept) |>
  strsplit("\\s+") |> 
  hash()
```

2. Compute pairwise Jaccard distances between hashed word representations.

```{r}
ConceptsToExploreWordDistance <-
  seq_distmatrix(ConceptsToExploreHash, 
                 ConceptsToExploreHash, 
                 method = "jaccard", 
                 q = 1)
```

3. Assign concept names to matrix dimensions for interpretability.

```{r}
rownames(ConceptsToExploreWordDistance) <- ConceptsToExploreValidConceptLength$concept
colnames(ConceptsToExploreWordDistance) <- ConceptsToExploreValidConceptLength$concept
```

4. Transform symmetric matrix to long format, eliminating redundant pairwise comparisons.

```{r}
ConceptsToExploreWordDistance[upper.tri(ConceptsToExploreWordDistance, diag = TRUE)] <- NA

ConceptsToExploreWordDistanceDt <-
  as.data.table(ConceptsToExploreWordDistance,
                keep.rownames = "Variable Name 1") |>
  melt(id.vars = "Variable Name 1",
       variable.name = "Variable Name 2",
       variable.factor = FALSE,
       value.name = "Jaccard Distance",
       na.rm = TRUE)
```


With this inter-concept distance we can use the `edge_betweenness` argoritm to define the cluster to use. But as the result for this method will change depending the min Jaccard Distance selected to do the analysis, we need to explore several options before taking a final decision.

```{r}
#| echo: false
#| output: false

ConceptsToExploreByDistanceFilePath <-
  here("output/cache-data/08-expanding-transportation-socioeconomic/ConceptsToExploreByDistance.fst")

if(file.exists(ConceptsToExploreByDistanceFilePath)) {
  ConceptsToExplore <-
    fst::read_fst(ConceptsToExploreByDistanceFilePath, as.data.table = TRUE)
}
```


```{r}
define_clusters_by_distance <- function(min_jaccard_distance,
                                        dt){
  
  dt_filtered = dt[`Jaccard Distance` <= min_jaccard_distance]
  
  temp_graph =
    graph_from_data_frame(dt_filtered, directed = FALSE)
  
  temp_cluster = cluster_edge_betweenness(temp_graph)
  
  node_results = data.table(
    node = V(temp_graph)$name,
    cluster = temp_cluster$membership,
    `Jaccard Distance Threshold` = min_jaccard_distance,
    modularity = max(temp_cluster$modularity),
    n_clusters = max(temp_cluster$membership)
  )
  
  return(node_results)
  
}


ConceptsToExploreByDistance <-
  lapply(seq(0.1, 0.8, by = 0.10),
         FUN = define_clusters_by_distance,
         dt = ConceptsToExploreWordDistanceDt) |>
  rbindlist()
```

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(ConceptsToExploreByDistance, ConceptsToExploreByDistanceFilePath)
```


```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

ConceptsToExploreByDistance[, unique(.SD),
                            .SDcols = c("Jaccard Distance Threshold",
                                   "modularity",
                                   "n_clusters")] |>
  melt(id.vars = "Jaccard Distance Threshold",
       variable.name = "cluster_metric") |>
  ggplot(aes(`Jaccard Distance Threshold`,
             value,
             color = cluster_metric)) +
  geom_line()+
  geom_point()+
  scale_y_continuous(labels = \(x) fifelse(x <= 1 & x > 0, 
                                           label_percent()(x),
                                           label_comma(accuracy = 1)(x) ),
                     breaks = breaks_pretty(7))+
  scale_x_continuous(breaks = breaks_width(0.10)) +
  expand_limits(y = 0) +
  facet_wrap(vars(cluster_metric),
             scale = "free_y",
             ncol = 1L)+
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        legend.position = "none")
```

Based on the below chart, we can see that 95% of the variables present at least one relation with less than **0.60 distance**. Which sounds like the group we need to focus, as the longer become the variable the longer the distance of words it needs to match with another variable. By make this selection we expect to be able to explore the more important variables as:

- Present areas with many ways to measure
- Exclude too specific variables

We can also use these distance relations to create meaning clusters based on the **betweenness of the edges** in the network.

```{r}
ConceptsToExploreValidDistanceDt <- 
  ConceptsToExploreWordDistanceDt[`Jaccard Distance` <= 0.60]

ConceptsToExploreValidDistanceGraph <- 
  graph_from_data_frame(ConceptsToExploreValidDistanceDt,
                        directed = FALSE)

ConceptsToExploreValidDistanceCluster <-
  cluster_edge_betweenness(ConceptsToExploreValidDistanceGraph)

```

Now we can take the information back to data.table to more manipulation and exploration.

```{r}
ConceptsToExploreValidDistanceVariables <-
  V(ConceptsToExploreValidDistanceGraph) |>
  as.data.frame() |>
  as.data.table(keep.rownames = "Concept")

ConceptsToExploreValidDistanceVariables[
  j = `:=`(x = NULL,
           cluster = ConceptsToExploreValidDistanceCluster$membership)
]
```

To understand what kind of variables we have in each cluster lets use the **statistic tf-idf** to list the top 4 words of each cluster and use that reference to select the must meaningful ones for this context.

let's use the **statistic tf-idf** to list the top **30 more important words** of the variables present in the dataset by following the next steps:

1. Defining custom **stop words** for this data to **give space to more relevant words**.

```{r}
CustomStopWords <- c(
  stop_words[stop_words$lexicon == "snowball", c("word")][[1]],
  "estimate",
  "estimates",
  "percent",
  "months",
  "past",
  "detailed",
  "type",
  "types",
  "current",
  "adjusted",
  "united",
  "round",
  "selected",
  "median",
  "level",
   NA_character_
)
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

TopMeasiningWords <-
  ConceptsToExploreValidDistanceVariables[, .(cluster,
                                              concept_lower = tolower(Concept))
  ][, unnest_tokens(.SD, word, concept_lower)
  ][!word %chin% CustomStopWords
    & !word %like% "^\\d+$",
    .N,
    by = c("cluster", "word")
  ][, dataset_total := sum(N), 
    by = "cluster"
  ][, bind_tf_idf(.SD, word, cluster, N)
  ][order(-tf_idf)
  ][, .(`Top Words` =
          head(word, 5L) |> 
          stringr::str_to_sentence() |>
          paste0(collapse = " | ") |> 
          paste0(" - ", cluster)),
    keyby = "cluster"]

ConceptsToExploreValidDistanceVariables[!TopMeasiningWords,
                                        on = "cluster"]


TopMeasiningWords[ConceptsToExploreValidDistanceVariables,
                  on = "cluster",
                  j = .(`Node Count` = .N), 
                  by = `Top Words`] |>
  ggplot(aes(y = reorder(`Top Words`, `Node Count`), 
             x = `Node Count`)) +
  geom_col(fill = ColorGray,
           color = "black",
           linewidth = 0.2) +
  labs(title = "Cluster Node Distribution", 
       y = "Cluster ID",
       x = "Number of Variables") +
  geom_text(aes(label = `Node Count`),
            nudge_x = 0.5) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

After checking the results we are going to select the next clusters to continues with the analysis as are directly related to transportation behavior, income patterns, and population density.



| Cluster | Top Words                          | Reason for Inclusion                                                                 |
|---------|------------------------------------|--------------------------------------------------------------------------------------|
| 2       | Means, Transportation, Workplace, Geography | Directly captures commuting methods and workplace locations affecting taxi demand |
| 3       | Time, Travel, Departure, Workers   | Measures commute duration/patterns critical for taxi usage timing                 |
| 4       | Household, Workers, Size, Vehicles | Reflects vehicle ownership and workforce density influencing taxi reliance        |
| 19      | Households, Income, Retirement, Social | Income levels directly correlate with taxi affordability and usage frequency    |
| 13      | Ratio, Level, Income, Poverty      | Poverty/income ratios indicate socioeconomic segments likely to use taxis        |

```{r}
Asc1FinalVariables <-
  Asc1WordValidDistanceVariables[.(c(2, 3, 4, 19, 13)),
                                 on = "cluster"]

Asc1FinalVariables[]
```


```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Apply only background colors without borders
group_colors <- c(
  "ðŸšŒ Cluster 2: Transportation" = "#e3f2fd",
  "â±ï¸ Cluster 3: Travel Time" = "#fce4ec",
  "ðŸ˜ï¸ Cluster 4: Household Structure" = "#e8f5e9",
  "ðŸ“‰ Cluster 13: Poverty Level" = "#fff3e0",
  "ðŸ’° Cluster 19: Household Income" = "#f3e5f5"
)

# Group Labels
group_names = names(group_colors)

Asc1FinalVariables[, cluster_label := fcase(
  cluster == 2,  group_names[1L],
  cluster == 3,  group_names[2L],
  cluster == 4,  group_names[3L],
  cluster == 13, group_names[4L],
  cluster == 19, group_names[5L]
)]

# Sort clusters by size (descending order)
cluster_order <- Asc1FinalVariables[, .N, by = "cluster_label"][order(-N), cluster_label]
Asc1FinalVariables[, cluster_label := factor(cluster_label, levels = cluster_order)]

# VersiÃ³n minimalista sin lÃ­neas internas
gt_tbl_minimal <-
  Asc1FinalVariables[order(cluster_label), !c("cluster")] |>
  gt(groupname_col = "cluster_label") |>
  tab_header(
    title = md("**Variables Grouped by Cluster**"),
    subtitle = "Cluster labels include icons for easy recognition"
  ) |>
  cols_label(Concept = "Variable Description") |>
  tab_options(
    row_group.as_column = TRUE,
    table.width = pct(100),
    heading.background.color = "#f9f9f9",
    table.border.top.color = "gray90"
  ) |>
  # Remove all striping and borders
  opt_table_lines(extent = "none") |>
  tab_style(
    style = cell_borders(sides = "all", color = "transparent"),
    locations = cells_body()
  )

for (grp in cluster_order) {
  gt_tbl_minimal <- gt_tbl_minimal |>
    tab_style(
      style = list(
        cell_fill(color = group_colors[grp]),
        cell_text(weight = "bold")
      ),
      locations = cells_row_groups(groups = grp)
    ) |>
    tab_style(
      style = list(cell_fill(color = group_colors[grp])),
      locations = cells_body(rows = cluster_label == grp)
    )
}

gt_tbl_minimal
```


### Getting data from API

### Saving the data

```r
qs2::qs_save(OsmDataComplete, 
             here("output/cache-data/OsmDataComplete.qs"))
```


## Spatial joins

We perform two main types of spatial joins to associate the OpenStreetMap features with our taxi zones: first, to identify features entirely within each zone, and second, to identify features that intersect or overlap with the zone boundaries.

### Joining elements **contained** by each zone

Using the `st_within` function, this section identifies OpenStreetMap featuresâ€”such as points, lines, polygons, multilinestrings, and multipolygonsâ€”that are **fully contained** within each taxi zone's boundaries. `st_within` considers multilinestrings and multipolygons to be within a zone only if all their constituent parts are inside.

```r
PolygonWithinZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_within,
          left = FALSE)
```


### Joining lines that are **partially contained** for each zone. 

In this section, we identify OSM features that are not entirely within a taxi zone but do intersect or overlap with its boundary. For lines and multilinestrings, we use `st_crosses`, which identifies features that intersect the interior of a zone but are not completely contained within it. For polygons and multipolygons, we use `st_overlaps`, which identifies features that have some area in common with a zone but are not identical to or completely contained within it.

```r
PolygonInSeveralZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_overlaps,
          left = FALSE)
```

#### Cropping features overlapping multiple zones


## Validation


## Conclusion

