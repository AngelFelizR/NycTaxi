---
title: "Exploring Transportation and Socioeconomic Patterns"
editor_options: 
  chunk_output_type: console
execute:
  message: false
  warning: false
---

Expanding on the previous post's geospatial feature extraction, this article will delve into enriching our understanding of NYC taxi zones with socioeconomic and transportation-related data from the **US Census Bureau**. This new layer of information will provide crucial context for analyzing `take_current_trip` by revealing underlying community characteristics and commuting behaviors within each zone.

Here are the steps to follow:

1. **Importing training data with zone shapes**.

1.  **Import Census data**:
    * Identify relevant variables.
    * Download data for Manhattan, Queens and Brooklyn with geometry.

2.  **Spatially join ACS data to taxi zones**:
    * Perform spatial joins to aggregate census tract data to the taxi zone level.
    * Address overlapping and partially contained tracts.

3.  **Extract new features**:
    * Calculate proportions, averages, and densities of the ACS variables for each taxi zone.

4.  **Validation**:
    * Integrate new features with existing data and `take_current_trip`.
    * Define the correlation between new predictors and `take_current_trip`.
    
    
## Setting up the environment

### Define colors to use

```{r}
BoroughColors <- c(
  'Manhattan' = '#e41a1c',
  'Queens' = '#377eb8',
  'Brooklyn'= '#4daf4a'
)

BoroughSelected <- names(BoroughColors)

ColorHighlight <- "lightslateblue"
ColorGray <- "gray80"
```

### Loading packages

```{r}
## To manage relative paths
library(here)

## To transform data that fits in RAM
library(data.table)

## To work with spatial data
library(sf)

## To create interactive maps
library(tmap)
tmap_mode("view")

# To work and plot graphs
library(tidytext)
library(stringdist)
library(hashr)
library(igraph)

# Access to US Census Bureau datasets
library(tidycensus)

## To create general plots
library(ggplot2)
library(scales)

## To create table
library(gt)

## Custom functions
devtools::load_all()
```

:::{.callout-note title="Computational Performance"}
The geospatial processing operations in this document are computationally intensive. Although they could benefit from parallelization with packages like `future` and `future.apply`, we've opted for a sequential approach with caching via `qs2`  to maximize **reproducibility** and maintain **code simplicity**. Intermediate results are saved after each costly operation to avoid unnecessary recalculations during iterative development.
:::

## Importing training data with zone shapes

Details on how this data was obtained can be found in the **Data Collection Process**.

```{r}
ZonesShapes <-
  read_sf(here("raw-data/taxi_zones/taxi_zones.shp")) |>
  subset(borough %in% BoroughSelected) |>
  transform(Shape_Area = st_area(geometry))

TrainingSample <-
  here("output/take-trip-fst") |>
  list.files(full.names = TRUE) |>
  (\(x) data.table(full_path = x,
                   n_char = nchar(basename(x)),
                   name = basename(x)))() |>
  (\(dt) dt[order(n_char, name), full_path])() |>
  head(12L) |>
  lapply(FUN = fst::read_fst,
         columns = c("trip_id","PULocationID", "DOLocationID", "take_current_trip"),
         as.data.table = TRUE) |>
  rbindlist()
```

## Import Census data

### Identify relevant variables

Based on the `tidycensus` documentation we have 55 tables available for exploration. That's sounds great but a don't have any idea which variables I want use in this project.

To solve this initial problem we are going to scrap the official web site table to confirm:

- The corresponding survey for each dataset.
- Select the last year update of each dataset in 2022 or prior (our trainingset comes from 2022).

```{r}
#| echo: false
#| output: false

ValidTablesLastYearFilePath = here("output/cache-data/ValidTablesLastYear.fst")


if(file.exists(ValidTablesLastYearFilePath)) {
  ValidTablesLastYear <- 
    fst::read_fst(ValidTablesLastYearFilePath, as.data.table = TRUE)
}
```

```{r}
#| eval: false

CensusTables <-
  rvest::read_html("https://api.census.gov/data.html") |>
  rvest::html_table() |>
  (\(x) x[[1L]])() |>
  as.data.table() |>
  subset(!is.na(`API Base URL`)
         & Vintage != "N/A",
         select = c("Title",
                    "Description",
                    "Vintage",
                    "Dataset Type",
                    "Dataset Name",
                    "API Base URL"))

CensusTables[,`:=`(Vintage = as.integer(Vintage),
                   `Dataset Name` = gsub("â€º ", "/", `Dataset Name`))]

CensusTables[, `:=`(survey = fcase(`Dataset Name` %like% "^dec",
                                   "dec",
                                   `Dataset Name` %like% "^acs",
                                   "acs"),
                    `Dataset Name` = gsub("(dec|acs)/", "", `Dataset Name`))]

setorder(CensusTables, -Vintage)

ValidTables = c(
  "sf1", "sf2", "sf3", "sf4", "pl", "dhc", "dp", 
  "ddhca", "ddhcb", "sdhc", "as", "gu", "mp", 
  "vi", "acsse", "dpas", "dpgu", "dpmp", "dpvi",
  "dhcvi", "dhcgu", "dhcvi", "dhcas", "acs1", 
  "acs3", "acs5", "acs1/profile", "acs3/profile",
  "acs5/profile", "acs1/subject", "acs3/subject",
  "acs5/subject", "acs1/cprofile", "acs5/cprofile",
  "sf2profile", "sf3profile", "sf4profile", "aian",
  "aianprofile", "cd110h", "cd110s", "cd110hprofile",
  "cd110sprofile", "sldh", "slds", "sldhprofile",
  "sldsprofile", "cqr", "cd113", "cd113profile",
  "cd115", "cd115profile", "cd116", "plnat", "cd118"
)

ValidTablesYears <-
  CensusTables[
    Vintage <= 2022
  ][ValidTables,
    on = "Dataset Name",
    nomatch = NULL,
    unique(.SD, by = "Dataset Name"),
    .SDcols = c("Vintage", "Dataset Name", "survey")]

get_all_data = function(x, ref_df){
  
  imported_data =
    load_variables(ref_df$Vintage[x],
                   ref_df$`Dataset Name`[x])
  
  setDT(imported_data)
  
  imported_data[, `:=`(year = ref_df$Vintage[x],
                       dataset = ref_df$`Dataset Name`[x],
                       survey = ref_df$survey[x])]
  
  return(imported_data)
  
} 
  
ValidTablesLastYear <-
  lapply(seq_along(ValidTablesYears$`Dataset Name`), 
         get_all_data,
         ref_df = ValidTablesYears)|>
  rbindlist(use.names = TRUE, fill = TRUE)

nrow(ValidTablesLastYear) |> comma()
```

```{r}
#| echo: false

nrow(ValidTablesLastYear) |> comma()
```

After making that exploration we can see that we have more **340K variables** to explore.

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(ValidTablesLastYear, ValidTablesLastYearFilePath)
```

To understand have we have let's plot the number of variables by dataset and survey for each year.

Based on the below chart we can see that `acs1` was updated in 2022 and have more than 35k variables.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

YearBreaks <- unique(ValidTablesLastYear$year) |> sort(decreasing = TRUE)

VariablesPerDataset <-
  ValidTablesLastYear[, .N, 
                      .(year = factor(year), 
                        dataset,
                        survey)
  ][, dataset := reorder(dataset, N, FUN = sum)] 

ggplot(VariablesPerDataset) +
  geom_col(aes(N, dataset, fill = year)) +
  scale_fill_brewer(palette = "Purples",
                    breaks = YearBreaks) +
  scale_x_continuous(labels = scales::label_comma(scale = 1/1000, suffix = "k"),
                     breaks = scales::breaks_width(5e3))+
  labs(x = "Number of variables",
       fill = "Last update year")+
  facet_wrap(vars(survey), scale = "free") +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

To have a general idea of the variable available in that data set we can use the **statistic tf-idf** to list the top 30 more important words of the variables present in the dataset.

To have good results with this technique, it's important to complete a list of **stop words** for this data to give space to more relevant words.

```{r}
CustomStopWords <- c(
  stop_words$word,
  "estimate",
  "estimates",
  "percent",
  "months",
  "past",
  "detailed",
  "type",
  "types",
  "current",
  "adjusted",
  "united",
  "round",
  "selected",
   NA_character_
)
```

Based on the results we see some interesting words like:

-**Occupation**, **employed**, **employment** and **Industry**: I think work variable can affect daily patterns that could benefit taxi drivers.
-**Population**: The number of people requesting one service could affect the price.
-**Income**, **earnings**, **poverty**: The average income can affect if people prefer to take a taxi, have their on car or just take public transportation.
-**Transportation**: Any information about transportation will be useful.

So in general it seems this dataset have a lot of potential and we can concentrate the time to select the more important variables.

```{r}
#| echo: false
#| output: false

MostImportWordsPerDataset <-
  fst::read_fst(here("output/cache-data/MostImportWordsPerDataset.fst"),
                as.data.table = TRUE)
```


```{r}
#| eval: false

MostImportWordsPerDataset <-
  ValidTablesLastYear[, .(dataset,
                          name,
                          label = tolower(concept))
  ][, unnest_tokens(.SD, word, label)
  ][!word %chin% CustomStopWords
    & !word %like% "^\\d+$",
    .N,
    by = c("dataset", "word")
  ][, dataset_total := sum(N), 
    by = "dataset"
  ][, bind_tf_idf(.SD, word, dataset, N)]

setorder(MostImportWordsPerDataset, -tf_idf)
```

```{r}
#| echo: false
#| eval: false

fst::write_fst(MostImportWordsPerDataset,
               here("output/cache-data/MostImportWordsPerDataset.fst"))
```



```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

MostImportWordsPerDataset[dataset == "acs1", 
                          head(.SD, 30L),
                          by = "dataset"] |>
  ggplot(aes(tf_idf,
             reorder(word, tf_idf, FUN = sum))) +
  geom_col(fill = ColorGray,
           color = "black",
           linewidth = 0.2) +
  scale_x_continuous(labels = percent_format()) +
  labs(title = "acs1",
       x = "tf-idf",
       y = NULL) +
  theme_minimal()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

Just by looking to the raw data, I found that many variables measure the same thing for particular groups based on:

- Race
- Place of Birth
- Sex

As I will join the data based on geography I doesn't make sense to have predictors based on the situations of those particular groups as we won't have does details of passengers of the trips.

```{r}
VariablesToIgnore <- c(
  "Hispanic",
  "Latino",
  "American Indian",
  "Alaska Native",
  "White",
  "Black",
  "African American",
  "Asian",
  "Native Hawaiian",
  "Pacific Islander",
  "Races?",
  
  "Ancestry",
  "Nativity",
  "Citizenship Status",
  "Place of Birth",
  "Naturalization",
  "Current Residence",
  
  "Male",
  "Female",
  "Women",
  "Men"
)
```

Now we just need to apply the same filter for all variables in the dataset.

```{r}
#| eval: false

Asc1BasicConcept <-
  ValidTablesLastYear[dataset == "acs1"
                      & year == 2022
                      & !concept %like% paste(VariablesToIgnore, collapse = "|")
                      & !label %like% "Male|Female",
                      .(concept = unique(concept))]
```

```{r}
#| echo: false

Asc1BasicConcept <- fst::read_fst(here("output/cache-data/Asc1BasicConcept.fst"),
                                  as.data.table = TRUE)
```

```{r}
#| eval: false
#| echo: false
#| output: false

fst::write_fst(Asc1BasicConcept, here("output/cache-data/Asc1BasicConcept.fst"))
```


Another aspect we found out by exploring the data it's that some variables names are really long and the long the concept definition the more specific the variable and less useful for our model. For example, below we can see the variable with the longer number of characters:

```{r}
#| echo: false
#| output: asis

Asc1BasicConcept[nchar(concept) == max(nchar(concept)), paste0("> ", concept)] |> cat()
```

- It measure family income for families by responsibility for own grandchildren:
    - With Grandparent Householders
    - or With Spouses Living
    - With Own Grandchildren Under 18 Years

By exploring the cumulative distribution we can see that near 50% of the variable present less **70 characters** per variable which seems the better group to focus the exploration

```{r}
#| code-fold: true
#| code-summary: "Show the code"

Asc1BasicConcept[, concept_length := nchar(concept)]

ggplot(Asc1BasicConcept)+
  stat_ecdf(aes(concept_length)) +
  scale_x_continuous(breaks = breaks_width(10))+
  scale_y_continuous(labels = label_percent(accuracy = 1))+
  theme_minimal()+
  theme(panel.grid.minor = element_blank())
```

To confirm if that action make sense, we can see the 2 variables that have 70 characters. we can confirm that are general enough to be valid predictors:

```{r}
Asc1BasicConceptMaxConceptLength <- 70

Asc1BasicConceptValidConceptLength <- 
  Asc1BasicConcept[concept_length <= Asc1BasicConceptMaxConceptLength]

Asc1BasicConcept[concept_length == Asc1BasicConceptMaxConceptLength, concept]
```

After excluding those cases, we still have **`r nrow(Asc1BasicConceptValidConceptLength)` variables to explore**. But we notice that some of the variable measure similar aspects of the studied population and we can calculate the pairwise **Jaccard string distances** of each of the variables.

```{r}
# 1. Create a hash for each word 
Asc1BasicConceptHash <- 
  tolower(Asc1BasicConceptValidConceptLength$concept) |>
  strsplit("\\s+") |> 
  hash()

# 2. Compute pairwise string distances between hashed words.
Asc1BasicConceptWordDistance <-
  seq_distmatrix(Asc1BasicConceptHash, 
                 Asc1BasicConceptHash, 
                 method = "jaccard", 
                 q = 1)

# 3. Giving names to matrix
rownames(Asc1BasicConceptWordDistance) <- Asc1BasicConceptValidConceptLength$concept
colnames(Asc1BasicConceptWordDistance) <- Asc1BasicConceptValidConceptLength$concept

# 4. Transforming the matrix in to data.table
Asc1BasicConceptWordDistance[upper.tri(Asc1BasicConceptWordDistance, diag = TRUE)] <- NA

Asc1WordDistanceDt <-
  as.data.table(Asc1BasicConceptWordDistance,
                keep.rownames = "Variable Name 1") |>
  melt(id.vars = "Variable Name 1",
       variable.name = "Variable Name 2",
       variable.factor = FALSE,
       value.name = "Jaccard Distance",
       na.rm = TRUE)
```

To understand what this metric represent in this particular concept, we plotted the cumulative number of unique variables with defined `Jaccard Distance`.

Based on the below chart, we can see that 54% of the variables present at least one relation with less than **0.35 distance**. Which sounds like the group we need to focus, as the longer become the a variable the longer the distance of words it needs to match with another variable. By make this selection we expect to be able to explore the more important variables as:

- Present areas with many ways to measure
- Exclude too specifit variables


```{r}
#| code-fold: true
#| code-summary: "Show the code"


NumberOfVariablesByDistance <-
  seq(0, 1, by = 0.05) |>
  lapply(\(x) Asc1WordDistanceDt[`Jaccard Distance` <= x,
                                 .(`Jaccard Distance Threshold` = x,
                                   `Number of Variables` = uniqueN(c(`Variable Name 1`, `Variable Name 2`)))]) |>
  rbindlist()

NumberOfVariablesByDistance[, `Proportion of Variables` := `Number of Variables`/ nrow(Asc1BasicConceptValidConceptLength)]


ggplot(NumberOfVariablesByDistance,
       aes(`Jaccard Distance Threshold`, 
           `Number of Variables`))+
  geom_line() +
  geom_point() +
  geom_text(aes(label = percent(`Proportion of Variables`,
                                accuracy = 1)),
            nudge_y = 25,
            check_overlap = TRUE) + 
  scale_y_continuous(labels = label_comma(),
                     breaks = breaks_width(100)) +
  scale_x_continuous(labels = label_percent(),
                     breaks = breaks_width(0.1)) +
  expand_limits(x = 0, y = 400) +
  labs(title = "Cumulative Variable Linkage",
       subtitle = "by Jaccard Distance Cutoff", 
       x = "Jaccard Distance Threshold") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())
```

We can also use these distance relations to create meaning clusters based on the **betweenness of the edges** in the network.

```{r}
Asc1WordValidDistanceDt <- Asc1WordDistanceDt[`Jaccard Distance` <= 0.35]

Asc1WordValidDistanceGraph <- graph_from_data_frame(Asc1WordValidDistanceDt, directed = FALSE)
Asc1WordValidDistanceCluster <- cluster_edge_betweenness(Asc1WordValidDistanceGraph)

Asc1WordValidDistanceCluster
```

Now we can take the information back to data.table to more manipulation and exploration.

```{r}
Asc1WordValidDistanceVariables <-
  V(Asc1WordValidDistanceGraph) |>
  as.data.frame() |>
  as.data.table(keep.rownames = "Concept")

Asc1WordValidDistanceVariables[
  j = `:=`(x = NULL,
           cluster = Asc1WordValidDistanceCluster$membership)
]
```

To understand what kind of variables we have in each cluster lets use the **statistic tf-idf** to list the top 4 words of each cluster and use that reference to select the must meaningful ones for this context.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-height: 7.5

TopMeasiningWords <-
  Asc1WordValidDistanceVariables[, .(cluster,
                                     concept_lower = tolower(Concept))
  ][, unnest_tokens(.SD, word, concept_lower)
  ][!word %chin% CustomStopWords
    & !word %like% "^\\d+$",
    .N,
    by = c("cluster", "word")
  ][, dataset_total := sum(N), 
    by = "cluster"
  ][, bind_tf_idf(.SD, word, cluster, N)
  ][order(-tf_idf)
  ][, .(`Top Words` =
          head(word, 4L) |> 
          stringr::str_to_sentence() |>
          paste0(collapse = " | ") |> 
          paste0(" - ", cluster)),
    keyby = "cluster"]


TopMeasiningWords[Asc1WordValidDistanceVariables,
                  on = "cluster",
                  j = .(`Node Count` = .N), 
                  by = `Top Words`] |>
  ggplot(aes(y = reorder(`Top Words`, `Node Count`), 
             x = `Node Count`)) +
  geom_col(fill = ColorGray,
           color = "black",
           linewidth = 0.2) +
  labs(title = "Cluster Node Distribution", 
       y = "Cluster ID",
       x = "Number of Variables") +
  geom_text(aes(label = `Node Count`),
            nudge_x = 0.5) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

After checking the results we are going to select the next clusters to continues with the analysis as are directly related to transportation behavior, income patterns, and population density.



| Cluster | Top Words                          | Reason for Inclusion                                                                 |
|---------|------------------------------------|--------------------------------------------------------------------------------------|
| 2       | Means, Transportation, Workplace, Geography | Directly captures commuting methods and workplace locations affecting taxi demand |
| 3       | Time, Travel, Departure, Workers   | Measures commute duration/patterns critical for taxi usage timing                 |
| 4       | Household, Workers, Size, Vehicles | Reflects vehicle ownership and workforce density influencing taxi reliance        |
| 19      | Households, Income, Retirement, Social | Income levels directly correlate with taxi affordability and usage frequency    |
| 13      | Ratio, Level, Income, Poverty      | Poverty/income ratios indicate socioeconomic segments likely to use taxis        |


```{r}
#| code-fold: true
#| code-summary: "Show the code"


Asc1FinalVariables <-
  Asc1WordValidDistanceVariables[.(c(2, 3, 4, 19, 13)),
                                 on = "cluster"]

# Apply only background colors without borders
group_colors <- c(
  "ðŸšŒ Cluster 2: Transportation" = "#e3f2fd",
  "â±ï¸ Cluster 3: Travel Time" = "#fce4ec",
  "ðŸ˜ï¸ Cluster 4: Household Structure" = "#e8f5e9",
  "ðŸ“‰ Cluster 13: Poverty Level" = "#fff3e0",
  "ðŸ’° Cluster 19: Household Income" = "#f3e5f5"
)

# Group Labels
group_names = names(group_colors)

Asc1FinalVariables[, cluster_label := fcase(
  cluster == 2,  group_names[1L],
  cluster == 3,  group_names[2L],
  cluster == 4,  group_names[3L],
  cluster == 13, group_names[4L],
  cluster == 19, group_names[5L]
)]

# Sort clusters by size (descending order)
cluster_order <- Asc1FinalVariables[, .N, by = "cluster_label"][order(-N), cluster_label]
Asc1FinalVariables[, cluster_label := factor(cluster_label, levels = cluster_order)]

# VersiÃ³n minimalista sin lÃ­neas internas
gt_tbl_minimal <-
  Asc1FinalVariables[order(cluster_label), !c("cluster")] |>
  gt(groupname_col = "cluster_label") |>
  tab_header(
    title = md("**Variables Grouped by Cluster**"),
    subtitle = "Cluster labels include icons for easy recognition"
  ) |>
  cols_label(Concept = "Variable Description") |>
  tab_options(
    row_group.as_column = TRUE,
    table.width = pct(100),
    heading.background.color = "#f9f9f9",
    table.border.top.color = "gray90"
  ) |>
  # Remove all striping and borders
  opt_table_lines(extent = "none") |>
  tab_style(
    style = cell_borders(sides = "all", color = "transparent"),
    locations = cells_body()
  )

for (grp in cluster_order) {
  gt_tbl_minimal <- gt_tbl_minimal |>
    tab_style(
      style = list(
        cell_fill(color = group_colors[grp]),
        cell_text(weight = "bold")
      ),
      locations = cells_row_groups(groups = grp)
    ) |>
    tab_style(
      style = list(cell_fill(color = group_colors[grp])),
      locations = cells_body(rows = cluster_label == grp)
    )
}

gt_tbl_minimal
```


### Getting data from API

### Saving the data

```r
qs2::qs_save(OsmDataComplete, 
             here("output/cache-data/OsmDataComplete.qs"))
```


## Spatial joins

We perform two main types of spatial joins to associate the OpenStreetMap features with our taxi zones: first, to identify features entirely within each zone, and second, to identify features that intersect or overlap with the zone boundaries.

### Joining elements **contained** by each zone

Using the `st_within` function, this section identifies OpenStreetMap featuresâ€”such as points, lines, polygons, multilinestrings, and multipolygonsâ€”that are **fully contained** within each taxi zone's boundaries. `st_within` considers multilinestrings and multipolygons to be within a zone only if all their constituent parts are inside.

```r
PolygonWithinZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_within,
          left = FALSE)
```


### Joining lines that are **partially contained** for each zone. 

In this section, we identify OSM features that are not entirely within a taxi zone but do intersect or overlap with its boundary. For lines and multilinestrings, we use `st_crosses`, which identifies features that intersect the interior of a zone but are not completely contained within it. For polygons and multipolygons, we use `st_overlaps`, which identifies features that have some area in common with a zone but are not identical to or completely contained within it.

```r
PolygonInSeveralZones <-
  OsmDataComplete$osm_polygons |>
  st_join(y = ZonesShapes,
          join = st_overlaps,
          left = FALSE)
```

#### Cropping features overlapping multiple zones


## Validation


## Conclusion

